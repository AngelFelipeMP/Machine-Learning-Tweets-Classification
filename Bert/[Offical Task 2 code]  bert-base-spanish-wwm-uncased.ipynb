{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"[Offical Task 2 code]  bert-base-spanish-wwm-uncased.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U94-LXc92z44"},"source":["# Downloading Dependences"]},{"cell_type":"code","metadata":{"id":"ELwqFdCntJfH"},"source":["# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcnNlFEcvTw0"},"source":["# !apt-get install git-lfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HXbKnk1fO_b"},"source":["# !git lfs install\n","# !git clone https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEv-emIkgeLm"},"source":["# !git lfs install\n","# !git clone https://huggingface.co/bert-base-multilingual-uncased"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VK_nUhVyxjWy"},"source":["# !pip install transformers==3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjFmFhWd2hq1"},"source":["# Load Dependences"]},{"cell_type":"code","metadata":{"id":"8EGoAnTU2BNc"},"source":["### add NLP dependences\n","import pickle\n","import os\n","import torch\n","import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","from sklearn import metrics\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","\n","from tqdm import tqdm\n","from collections import OrderedDict, namedtuple\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","import joblib\n","\n","import logging\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","import sys\n","from sklearn import metrics, model_selection\n","\n","import warnings\n","import torch_xla\n","import torch_xla.debug.metrics as met\n","import torch_xla.distributed.data_parallel as dp\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.utils.utils as xu\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.test.test_utils as test_utils\n","import warnings\n","\n","from torch_xla.core.xla_model import mesh_reduce\n","\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOH2Op99s4XU","executionInfo":{"status":"ok","timestamp":1619773634541,"user_tz":180,"elapsed":20165,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"3a550a01-f2c7-4398-a3c8-1cb8618dbde1"},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jd3akA1t3l_R"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"pz-cjmTkHXGE"},"source":["class BERTBaseUncased(nn.Module):\n","    def __init__(self, bert_path, output_bert='pooler', NumberOfClasses=2):\n","        super(BERTBaseUncased, self).__init__()\n","        self.bert_path = bert_path\n","        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n","        self.bert_drop = nn.Dropout(0.3)\n","        self.output_bert = output_bert\n","        self.NumberOfClasses = NumberOfClasses\n","        self.OutPutHidden = nn.Linear(768 * 2, NumberOfClasses)\n","        self.OutPoller = nn.Linear(768, NumberOfClasses)\n","\n","    def forward(\n","            self,\n","            ids,\n","            mask,\n","            token_type_ids\n","    ):\n","        o1, o2 = self.bert(\n","            ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids)\n","          \n","        if self.output_bert=='hidden':\n","          apool = torch.mean(o1, 1)\n","          mpool, _ = torch.max(o1, 1)\n","          cat = torch.cat((apool, mpool), 1)\n","          bo = self.bert_drop(cat)\n","\n","          output = self.OutPutHidden(bo) \n","\n","        else:\n","          bo = self.bert_drop(o2)\n","          output = self.OutPoller(bo)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bu6iEzVwmGi"},"source":["class BERTDatasetTraining:\n","    def __init__(self, comment, targets, tokenizer, max_length):\n","        self.comment = comment\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.comment)\n","\n","    def __getitem__(self, item):\n","        comment = str(self.comment[item])\n","        comment = \" \".join(comment.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment,\n","            None,\n","            truncation=True,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_length = self.max_length - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0HJ8r3Uapyb"},"source":["#### Handcraft metric for multiclass evaluation\n","def cem_metric(target, output):\n","  conf_metrix = metrics.confusion_matrix(target, output)\n","  cem_metrix = np.zeros(conf_metrix.shape)\n","\n","  for column in range(conf_metrix.shape[1]):\n","    for row in range(conf_metrix.shape[0]):\n","    \n","      if row == column :\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2)/conf_metrix.sum()\n","                                          \n","      elif row < column:\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2 + conf_metrix.sum(axis=0)[row:column].sum())/conf_metrix.sum()\n","\n","      elif row > column:\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2 + conf_metrix.sum(axis=0)[column+1:row+1].sum())/conf_metrix.sum()\n","\n","  cem_metrix= - np.log2( np.where(cem_metrix !=0, cem_metrix, cem_metrix+0000000.1 ))\n","\n","  return np.sum(cem_metrix * conf_metrix.T) / np.sum( np.diag(cem_metrix) * conf_metrix.sum(axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLmqHi5aPf7b"},"source":["class TrainModel():\n","  def __init__(self, PathSaveFiles, BertVersion, BertPath,  OutputBert, LearningRate, BatchSize, Epochs, FileName, X_train, X_valid, y_train ,y_valid, MaxLen = 192, SaveModel=False):\n","    self.BertVersion = BertVersion\n","    self.BertPath = BertPath\n","    self.OutputBert = OutputBert\n","    self.LearningRate = LearningRate\n","    self.BatchSize = BatchSize\n","    self.Epochs = Epochs\n","    self.FileName = FileName\n","    self.X_train = X_train\n","    self.X_valid = X_valid\n","    self.y_train = y_train\n","    self.y_valid = y_valid\n","    self.NumberOfLabels = y_train.nunique()\n","    self.average_metrics =  'macro' if self.NumberOfLabels > 2 else 'binary'\n","    self.PathSaveFiles = PathSaveFiles\n","    self.MaxLen = MaxLen\n","    self.SaveModel = SaveModel\n","\n","\n","  def _run(self):\n","      def OpenEndSave(CurrentEpoch, module):\n","          if module == 'open'and CurrentEpoch == 1:\n","            with open(self.PathSaveFiles + self.FileName + \".pkl\", \"rb\") as f:\n","              self.Results = pickle.load(f)\n","\n","          elif module == 'save' and CurrentEpoch == self.Epochs:\n","            with open(self.PathSaveFiles + self.FileName + \".pkl\",'wb') as f:\n","              pickle.dump(self.Results, f)\n","\n","\n","      def loss_fn(outputs, targets):\n","        return nn.CrossEntropyLoss()(outputs, targets)\n","            \n","\n","      def train_loop_fn(data_loader, model, optimizer, device, scheduler=None, epoch=None):\n","          model.train()\n","          for bi, d in enumerate(data_loader):\n","              ids = d[\"ids\"]\n","              mask = d[\"mask\"]\n","              token_type_ids = d[\"token_type_ids\"]\n","              targets = d[\"targets\"]\n","\n","              ids = ids.to(device, dtype=torch.long)\n","              mask = mask.to(device, dtype=torch.long)\n","              token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","              targets = targets.to(device, dtype=torch.float)\n","              \n","\n","              optimizer.zero_grad()\n","              outputs = model(\n","                  ids=ids,\n","                  mask=mask,\n","                  token_type_ids=token_type_ids\n","              )\n","\n","              loss = loss_fn(outputs, targets)\n","              if bi % 10 == 0:\n","                  xm.master_print(f'bi={bi}, loss={loss}')\n","\n","                  ValueLoss = loss.cpu().detach().numpy().tolist()\n","                  ValueLoss = xm.mesh_reduce('test_loss',ValueLoss, np.mean)\n","                  self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['loss'].append(ValueLoss)\n","\n","              loss.backward()\n","              xm.optimizer_step(optimizer)\n","              if scheduler is not None:\n","                  scheduler.step()\n","\n","      def eval_loop_fn(data_loader, model, device):\n","          model.eval()\n","          fin_targets = []\n","          fin_outputs = []\n","          for bi, d in enumerate(data_loader):\n","              ids = d[\"ids\"]\n","              mask = d[\"mask\"]\n","              token_type_ids = d[\"token_type_ids\"]\n","              targets = d[\"targets\"]\n","\n","              ids = ids.to(device, dtype=torch.long)\n","              mask = mask.to(device, dtype=torch.long)\n","              token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","              targets = targets.to(device, dtype=torch.float)\n","\n","              outputs = model(\n","                  ids=ids,\n","                  mask=mask,\n","                  token_type_ids=token_type_ids\n","              )\n","\n","              targets_np = targets.cpu().detach().numpy().tolist()\n","              outputs = torch.argmax(outputs, dim=1)\n","              outputs_np = outputs.detach().cpu().numpy().tolist()\n","\n","              fin_targets.extend(targets_np)\n","              fin_outputs.extend(outputs_np)    \n","\n","          return fin_outputs, fin_targets\n","\n","      # tokenizer\n","      tokenizer = transformers.BertTokenizer.from_pretrained(self.BertPath, do_lower_case=True)\n","\n","      train_dataset = BERTDatasetTraining(\n","          comment=self.X_train.values,\n","          targets=self.y_train.values,\n","          tokenizer=tokenizer,\n","          max_length=self.MaxLen\n","      )\n","\n","      train_sampler = torch.utils.data.distributed.DistributedSampler(\n","            train_dataset,\n","            num_replicas=xm.xrt_world_size(),\n","            rank=xm.get_ordinal(),\n","            shuffle=True)\n","\n","      train_data_loader = torch.utils.data.DataLoader(\n","          train_dataset,\n","          batch_size=self.BatchSize,\n","          sampler=train_sampler,\n","          drop_last=True,\n","          num_workers=1\n","      )\n","\n","      valid_dataset = BERTDatasetTraining(\n","          comment=self.X_valid.values,\n","          targets=self.y_valid.values,\n","          tokenizer=tokenizer,\n","          max_length=self.MaxLen\n","      )\n","\n","      valid_sampler = torch.utils.data.distributed.DistributedSampler(\n","            valid_dataset,\n","            num_replicas=xm.xrt_world_size(),\n","            rank=xm.get_ordinal(),\n","            shuffle=False)\n","\n","      valid_data_loader = torch.utils.data.DataLoader(\n","          valid_dataset,\n","          batch_size=16,\n","          sampler=valid_sampler,\n","          drop_last=False,\n","          num_workers=1\n","      )\n","\n","      device = xm.xla_device()\n","      model = mx.to(device)\n","      \n","\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","      \n","      lr = 0.4 * self.LearningRate * xm.xrt_world_size()\n","      num_train_steps = int(len(train_dataset) / self.BatchSize / xm.xrt_world_size() * self.Epochs)\n","      xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n","\n","      optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","      scheduler = get_linear_schedule_with_warmup(\n","          optimizer,\n","          num_warmup_steps=0,\n","          num_training_steps=num_train_steps\n","      )\n","\n","      best_f1, f1, best_cem, cem = 0,0,0,0\n","\n","      for epoch in range(1, self.Epochs+1):\n","        ## print epoch\n","          xm.master_print(f'Epoch: {epoch} of {self.Epochs}')\n","        ## Open file to save results\n","          OpenEndSave(CurrentEpoch=epoch, module='open')\n","\n","          para_loader = pl.ParallelLoader(train_data_loader, [device])\n","          train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler, epoch=epoch)\n","\n","          para_loader = pl.ParallelLoader(valid_data_loader, [device])\n","          o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n","          \n","          if self.NumberOfLabels == 2:\n","            f1 = xm.mesh_reduce('validation_f1', metrics.f1_score(t, o), np.mean)\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1'].append(f1)\n","\n","          else:\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1_macro'].append(xm.mesh_reduce('validation_f1_macro', metrics.f1_score(t, o, average=self.average_metrics), np.mean))\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1_weighted'].append(cem)\n","            cem = xm.mesh_reduce('validation_cem', cem_metric(t, o), np.mean)\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['cem'].append(xm.mesh_reduce('validation_cem', cem_metric(t, o), np.mean))\n","\n","          accuracy = metrics.accuracy_score(t, o)\n","          accuracy = xm.mesh_reduce('test_accuracy', accuracy, np.mean)\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['accuracy'].append(accuracy)\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['recall'].append(xm.mesh_reduce('validation_recall', metrics.recall_score(t, o, average=self.average_metrics), np.mean))\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['precision'].append(xm.mesh_reduce('validation_precison', metrics.precision_score(t, o, average=self.average_metrics), np.mean))\n","              \n","        ## save file with save results\n","          OpenEndSave(CurrentEpoch=epoch, module='save')\n","\n","        ## Save model\n","          if self.SaveModel and epoch == self.Epochs:\n","            xm.save(model.state_dict(), self.PathSaveFiles + self.FileName + '.bin')\n","        \n","        ## print accuracy\n","          xm.master_print(f'Accuracy = {accuracy}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzvib7kT2Ku6"},"source":["#Load data"]},{"cell_type":"code","metadata":{"id":"z19LsZrQ2Q5N"},"source":["PathDataSet = \"../content/drive/MyDrive/Code/DETOXIS/Data/train.csv\"\n","## Task 1\n","# df_train = pd.read_csv(PathDataSet, usecols=[\"comment\", \"toxicity\"]).fillna(\"none\")\n","# NewColumnsNames = {\"comment\":\"Data\",\"toxicity\":\"Label\"}\n","\n","## Task 2\n","df_train = pd.read_csv(PathDataSet, usecols=[\"comment\", \"toxicity_level\"]).fillna(\"none\")\n","NewColumnsNames = {\"comment\":\"Data\",\"toxicity_level\":\"Label\"}\n","\n","df_train = df_train.rename(columns=NewColumnsNames)\n","df_train = df_train.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Va6l36Yc2Tth","executionInfo":{"status":"ok","timestamp":1619773762371,"user_tz":180,"elapsed":523,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"10d28a23-4b7d-4a5d-9311-f5f2adb3ea17"},"source":["df_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Haz que pase ya Pedro por Dios, porque a m√≠ se...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Precisamente por eso, va a acabar discutiendo ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Bueno.. as√≠ es la izquierda.. desviando la res...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>No todas tienen d√≥nde ir, ni pueden dejar el t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>La Polic√≠a Nacional ha arrestado ya a uno de l...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  Label\n","0  Haz que pase ya Pedro por Dios, porque a m√≠ se...      1\n","1  Precisamente por eso, va a acabar discutiendo ...      0\n","2  Bueno.. as√≠ es la izquierda.. desviando la res...      0\n","3  No todas tienen d√≥nde ir, ni pueden dejar el t...      0\n","4  La Polic√≠a Nacional ha arrestado ya a uno de l...      0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"3I3eUmvE3zPa"},"source":["#Load Weights"]},{"cell_type":"code","metadata":{"id":"OtRdnS-Rz3m0"},"source":["def CriateFileName(BertVersionDict, NumberOfClasses):\n","  \n","  NameFile = str()\n","  for BertModel in BertVersionDict.keys():\n","    NameFile += BertModel\n","\n","  if NumberOfClasses > 2:\n","    NameFile += 'Task2'\n","  else:\n","    NameFile += 'Task1'\n","\n","  return NameFile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"miaaavjhYUp-"},"source":["# BertVersion = {'SpanishBert':'../content/bert-base-spanish-wwm-uncased/', 'MultilingualBert':'../content/bert-base-multilingual-uncased/'}\n","# OutputBert = ['hidden', 'pooler']\n","# LearningRate = [1e-5, 3e-5, 5e-5]\n","# BatchSize = [8, 16, 32 , 64]\n","# Epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kHLReChbjh4"},"source":["## Train Parameters\n","BertVersion = {'SpanishBert':'../content/bert-base-spanish-wwm-uncased/'}\n","OutputBert = ['hidden', 'pooler']\n","LearningRate = [3e-5, 5e-5]\n","BatchSize = [8, 16, 32 , 64]\n","Epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exc5VakSaf7N"},"source":["## Evalute matrics\n","###### Task 1\n","MetricsTask1 = ['accuracy', 'f1', 'recall', 'precision']\n","###### Task 2\n","MetricsTask2 = ['accuracy', 'f1_macro', 'f1_weighted', 'recall', 'precision', 'cem']\n","\n","## Get for 'Binary' classification' task1 or 'Multilabel classifcation' task2\n","Metrics = MetricsTask2 if df_train['Label'].nunique() > 2 else MetricsTask1\n","\n","## Criate dictinaril results\n","ResultsTask = { bert:{ output:{ lr:{ bat:{ epoc:{ metric:[] for metric in Metrics + ['loss']} for epoc in range(1, Epochs+1) } for bat in BatchSize} for lr in LearningRate} for output in OutputBert } for bert in BertVersion.keys() }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjxtRdNL2Kus"},"source":["## Where to Save Files\n","Path = 'drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/'\n","\n","## Creating Main Parte Bert File Name\n","MainParteBertFileName = CriateFileName(BertVersion, NumberOfClasses=df_train['Label'].nunique())\n","\n","## Create file to save results\n","FileResults = MainParteBertFileName + 'Results'\n","# with open(Path + FileResults + \".pkl\",'wb') as f:\n","#   pickle.dump(ResultsTask, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RefkYdGrcgmy"},"source":["#Train"]},{"cell_type":"code","metadata":{"id":"Lb1R26k9lFXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619112623270,"user_tz":180,"elapsed":16465873,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"63a97554-ee85-4f30-98b6-f0ba159c54d3"},"source":["### Cross Validation\n","for BertV, BertPath in BertVersion.items():\n","  for OutputB in OutputBert:\n","\n","    ### Loading Bert trained weights\n","    mx = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique())\n","\n","    for lr in LearningRate:\n","      for Batch in BatchSize:\n","\n","        ## StratifiedKFold\n","        skf = StratifiedKFold(n_splits=10)\n","        fold = 1\n","        for train_index, valid_index in skf.split(df_train['Data'], df_train['Label']):\n","          X_train, X_valid = df_train.loc[train_index, 'Data'], df_train.loc[valid_index, 'Data']\n","          y_train, y_valid = df_train.loc[train_index, 'Label'], df_train.loc[valid_index, 'Label']\n","\n","          print(f'parameters: Bertmodel: {BertV}, Output: {OutputB}, lr: {lr}, Batch: {Batch}, Totsl Num. Epochs: {Epochs}, Fold: {fold}')\n","          fold += 1\n","          MoDeL = TrainModel(PathSaveFiles = Path,\n","                            BertVersion=BertV,\n","                            BertPath=BertPath,\n","                            OutputBert=OutputB,\n","                            LearningRate=lr,\n","                            BatchSize=Batch,\n","                            Epochs=Epochs,\n","                            FileName= FileResults,\n","                            X_train=X_train, \n","                            X_valid=X_valid,\n","                            y_train=y_train,\n","                            y_valid=y_valid)\n","        \n","\n","          def _mp_fn(rank, flags):\n","            torch.set_default_tensor_type('torch.FloatTensor')\n","            a = MoDeL._run()\n","\n","          FLAGS={}\n","          xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ were not used when initializing BertModel: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch: 19 of 20\n","bi=0, loss=0.00428386265411973\n","bi=10, loss=0.0068955314345657825\n","Accuracy = 0.7329545454545454\n","Epoch: 20 of 20\n","bi=0, loss=0.004021701868623495\n","bi=10, loss=0.005889202933758497\n","Accuracy = 0.7301136363636365\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3605506420135498\n","bi=10, loss=0.8074816465377808\n","Accuracy = 0.6761363636363636\n","Epoch: 2 of 20\n","bi=0, loss=0.9153426289558411\n","bi=10, loss=0.7944733500480652\n","Accuracy = 0.6903409090909092\n","Epoch: 3 of 20\n","bi=0, loss=0.743665337562561\n","bi=10, loss=0.571084201335907\n","Accuracy = 0.7244318181818181\n","Epoch: 4 of 20\n","bi=0, loss=0.5918833017349243\n","bi=10, loss=0.5661506056785583\n","Accuracy = 0.7130681818181819\n","Epoch: 5 of 20\n","bi=0, loss=0.463252991437912\n","bi=10, loss=0.5183402895927429\n","Accuracy = 0.6619318181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.49587884545326233\n","bi=10, loss=0.20992186665534973\n","Accuracy = 0.7045454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.22258040308952332\n","bi=10, loss=0.20770257711410522\n","Accuracy = 0.65625\n","Epoch: 8 of 20\n","bi=0, loss=0.15991996228694916\n","bi=10, loss=0.05816133692860603\n","Accuracy = 0.6931818181818181\n","Epoch: 9 of 20\n","bi=0, loss=0.19141490757465363\n","bi=10, loss=0.02341013215482235\n","Accuracy = 0.6903409090909092\n","Epoch: 10 of 20\n","bi=0, loss=0.03628600388765335\n","bi=10, loss=0.2935667335987091\n","Accuracy = 0.71875\n","Epoch: 11 of 20\n","bi=0, loss=0.4320639371871948\n","bi=10, loss=0.5768897533416748\n","Accuracy = 0.7159090909090908\n","Epoch: 12 of 20\n","bi=0, loss=0.6667839288711548\n","bi=10, loss=0.14301541447639465\n","Accuracy = 0.7215909090909092\n","Epoch: 13 of 20\n","bi=0, loss=0.23417305946350098\n","bi=10, loss=0.15111038088798523\n","Accuracy = 0.7130681818181818\n","Epoch: 14 of 20\n","bi=0, loss=0.14111919701099396\n","bi=10, loss=0.05539095774292946\n","Accuracy = 0.6789772727272727\n","Epoch: 15 of 20\n","bi=0, loss=0.0796484649181366\n","bi=10, loss=0.02780810557305813\n","Accuracy = 0.6818181818181819\n","Epoch: 16 of 20\n","bi=0, loss=0.06954406201839447\n","bi=10, loss=0.017020784318447113\n","Accuracy = 0.6931818181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.04477882385253906\n","bi=10, loss=0.017467517405748367\n","Accuracy = 0.7017045454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.0444144569337368\n","bi=10, loss=0.008183995261788368\n","Accuracy = 0.6931818181818181\n","Epoch: 19 of 20\n","bi=0, loss=0.02573837898671627\n","bi=10, loss=0.010374736972153187\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.019683945924043655\n","bi=10, loss=0.007899591699242592\n","Accuracy = 0.7045454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.320276141166687\n","bi=10, loss=0.8484792113304138\n","Accuracy = 0.7073863636363636\n","Epoch: 2 of 20\n","bi=0, loss=0.8666219711303711\n","bi=10, loss=0.7884587049484253\n","Accuracy = 0.7272727272727273\n","Epoch: 3 of 20\n","bi=0, loss=0.6778035163879395\n","bi=10, loss=0.5335268974304199\n","Accuracy = 0.7159090909090908\n","Epoch: 4 of 20\n","bi=0, loss=0.5383985638618469\n","bi=10, loss=0.3032873868942261\n","Accuracy = 0.7272727272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.2707641124725342\n","bi=10, loss=0.3206917345523834\n","Accuracy = 0.7329545454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.2833895981311798\n","bi=10, loss=0.23369790613651276\n","Accuracy = 0.5767045454545454\n","Epoch: 7 of 20\n","bi=0, loss=0.5792691707611084\n","bi=10, loss=0.21440035104751587\n","Accuracy = 0.7102272727272727\n","Epoch: 8 of 20\n","bi=0, loss=0.2341911345720291\n","bi=10, loss=0.12420971691608429\n","Accuracy = 0.6647727272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.1727207899093628\n","bi=10, loss=0.028698112815618515\n","Accuracy = 0.65625\n","Epoch: 10 of 20\n","bi=0, loss=0.10106498748064041\n","bi=10, loss=0.10061979293823242\n","Accuracy = 0.6960227272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.08752094954252243\n","bi=10, loss=0.16778935492038727\n","Accuracy = 0.6931818181818181\n","Epoch: 12 of 20\n","bi=0, loss=0.18755683302879333\n","bi=10, loss=0.15559013187885284\n","Accuracy = 0.6704545454545454\n","Epoch: 13 of 20\n","bi=0, loss=0.08640533685684204\n","bi=10, loss=0.18300215899944305\n","Accuracy = 0.7045454545454545\n","Epoch: 14 of 20\n","bi=0, loss=0.08956640213727951\n","bi=10, loss=0.06631121784448624\n","Accuracy = 0.7329545454545454\n","Epoch: 15 of 20\n","bi=0, loss=0.557652473449707\n","bi=10, loss=0.05634647607803345\n","Accuracy = 0.6363636363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.06960757076740265\n","bi=10, loss=0.017165180295705795\n","Accuracy = 0.71875\n","Epoch: 17 of 20\n","bi=0, loss=0.07005738466978073\n","bi=10, loss=0.011402448639273643\n","Accuracy = 0.7159090909090909\n","Epoch: 18 of 20\n","bi=0, loss=0.03551614284515381\n","bi=10, loss=0.009700662456452847\n","Accuracy = 0.7130681818181819\n","Epoch: 19 of 20\n","bi=0, loss=0.018051568418741226\n","bi=10, loss=0.0069478098303079605\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.014693032950162888\n","bi=10, loss=0.007407217286527157\n","Accuracy = 0.7045454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3376169204711914\n","bi=10, loss=0.9802337884902954\n","Accuracy = 0.6846590909090908\n","Epoch: 2 of 20\n","bi=0, loss=0.9769505262374878\n","bi=10, loss=0.9117172956466675\n","Accuracy = 0.6789772727272727\n","Epoch: 3 of 20\n","bi=0, loss=0.7910812497138977\n","bi=10, loss=0.689265787601471\n","Accuracy = 0.7301136363636364\n","Epoch: 4 of 20\n","bi=0, loss=0.5780842900276184\n","bi=10, loss=0.5013608336448669\n","Accuracy = 0.7272727272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.4565122425556183\n","bi=10, loss=0.4749810993671417\n","Accuracy = 0.7244318181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.3913039565086365\n","bi=10, loss=0.2629396915435791\n","Accuracy = 0.7102272727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.23581773042678833\n","bi=10, loss=0.16427107155323029\n","Accuracy = 0.7414772727272727\n","Epoch: 8 of 20\n","bi=0, loss=0.23642638325691223\n","bi=10, loss=0.13125751912593842\n","Accuracy = 0.6761363636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.1510774940252304\n","bi=10, loss=0.04482170566916466\n","Accuracy = 0.65625\n","Epoch: 10 of 20\n","bi=0, loss=0.08103269338607788\n","bi=10, loss=0.13710331916809082\n","Accuracy = 0.6988636363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.07135064899921417\n","bi=10, loss=0.19088749587535858\n","Accuracy = 0.7045454545454546\n","Epoch: 12 of 20\n","bi=0, loss=0.10450294613838196\n","bi=10, loss=0.5435805320739746\n","Accuracy = 0.71875\n","Epoch: 13 of 20\n","bi=0, loss=0.50248783826828\n","bi=10, loss=0.13273845613002777\n","Accuracy = 0.7215909090909092\n","Epoch: 14 of 20\n","bi=0, loss=0.617849588394165\n","bi=10, loss=0.14673058688640594\n","Accuracy = 0.75\n","Epoch: 15 of 20\n","bi=0, loss=0.2707984745502472\n","bi=10, loss=0.08380849659442902\n","Accuracy = 0.7443181818181819\n","Epoch: 16 of 20\n","bi=0, loss=0.11752084642648697\n","bi=10, loss=0.05085723102092743\n","Accuracy = 0.7471590909090909\n","Epoch: 17 of 20\n","bi=0, loss=0.11778752505779266\n","bi=10, loss=0.031536977738142014\n","Accuracy = 0.75\n","Epoch: 18 of 20\n","bi=0, loss=0.0864277109503746\n","bi=10, loss=0.03295382484793663\n","Accuracy = 0.7471590909090908\n","Epoch: 19 of 20\n","bi=0, loss=0.0592256598174572\n","bi=10, loss=0.01838100515305996\n","Accuracy = 0.7471590909090908\n","Epoch: 20 of 20\n","bi=0, loss=0.05218305438756943\n","bi=10, loss=0.02752632088959217\n","Accuracy = 0.7443181818181818\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3535865545272827\n","bi=10, loss=0.944503664970398\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.8742265701293945\n","bi=10, loss=0.7871581315994263\n","Accuracy = 0.7045454545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.784309446811676\n","bi=10, loss=0.4468570053577423\n","Accuracy = 0.7045454545454546\n","Epoch: 4 of 20\n","bi=0, loss=0.5995647311210632\n","bi=10, loss=0.3718196451663971\n","Accuracy = 0.7045454545454546\n","Epoch: 5 of 20\n","bi=0, loss=0.2694772183895111\n","bi=10, loss=0.254965603351593\n","Accuracy = 0.71875\n","Epoch: 6 of 20\n","bi=0, loss=0.42764824628829956\n","bi=10, loss=0.31239697337150574\n","Accuracy = 0.7130681818181819\n","Epoch: 7 of 20\n","bi=0, loss=0.24204158782958984\n","bi=10, loss=0.1260508894920349\n","Accuracy = 0.6619318181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.12443623691797256\n","bi=10, loss=0.22736681997776031\n","Accuracy = 0.6903409090909091\n","Epoch: 9 of 20\n","bi=0, loss=0.07144751399755478\n","bi=10, loss=0.03524603322148323\n","Accuracy = 0.6164772727272727\n","Epoch: 10 of 20\n","bi=0, loss=0.07255511730909348\n","bi=10, loss=0.04954706132411957\n","Accuracy = 0.6335227272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.33698445558547974\n","bi=10, loss=0.03415965288877487\n","Accuracy = 0.6051136363636364\n","Epoch: 12 of 20\n","bi=0, loss=0.12638631463050842\n","bi=10, loss=0.13198797404766083\n","Accuracy = 0.6761363636363636\n","Epoch: 13 of 20\n","bi=0, loss=0.16796237230300903\n","bi=10, loss=0.3700658977031708\n","Accuracy = 0.6960227272727273\n","Epoch: 14 of 20\n","bi=0, loss=0.23955188691616058\n","bi=10, loss=0.05263487994670868\n","Accuracy = 0.6903409090909092\n","Epoch: 15 of 20\n","bi=0, loss=0.15633156895637512\n","bi=10, loss=0.10338746011257172\n","Accuracy = 0.6903409090909092\n","Epoch: 16 of 20\n","bi=0, loss=0.050142738968133926\n","bi=10, loss=0.018734831362962723\n","Accuracy = 0.6704545454545454\n","Epoch: 17 of 20\n","bi=0, loss=0.031012536957859993\n","bi=10, loss=0.011214296333491802\n","Accuracy = 0.6704545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.010003097355365753\n","bi=10, loss=0.007287978660315275\n","Accuracy = 0.6704545454545454\n","Epoch: 19 of 20\n","bi=0, loss=0.02651878073811531\n","bi=10, loss=0.011918059550225735\n","Accuracy = 0.6647727272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.008566495962440968\n","bi=10, loss=0.007240036502480507\n","Accuracy = 0.6647727272727273\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3464583158493042\n","bi=10, loss=0.9703623056411743\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.8703324198722839\n","bi=10, loss=0.8873593211174011\n","Accuracy = 0.6875\n","Epoch: 3 of 20\n","bi=0, loss=0.8870205879211426\n","bi=10, loss=0.6107422113418579\n","Accuracy = 0.7102272727272727\n","Epoch: 4 of 20\n","bi=0, loss=0.5405417680740356\n","bi=10, loss=0.2952745854854584\n","Accuracy = 0.6960227272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.4747781753540039\n","bi=10, loss=0.4422927498817444\n","Accuracy = 0.7159090909090909\n","Epoch: 6 of 20\n","bi=0, loss=0.2061421275138855\n","bi=10, loss=0.7583302855491638\n","Accuracy = 0.6761363636363636\n","Epoch: 7 of 20\n","bi=0, loss=0.46077093482017517\n","bi=10, loss=0.1861114501953125\n","Accuracy = 0.6505681818181819\n","Epoch: 8 of 20\n","bi=0, loss=0.2857576012611389\n","bi=10, loss=0.11793294548988342\n","Accuracy = 0.6136363636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.37758684158325195\n","bi=10, loss=0.07364553958177567\n","Accuracy = 0.5795454545454546\n","Epoch: 10 of 20\n","bi=0, loss=0.2680182456970215\n","bi=10, loss=0.30840495228767395\n","Accuracy = 0.6051136363636365\n","Epoch: 11 of 20\n","bi=0, loss=0.4157443642616272\n","bi=10, loss=0.8067927360534668\n","Accuracy = 0.6903409090909092\n","Epoch: 12 of 20\n","bi=0, loss=0.3160471022129059\n","bi=10, loss=0.20344875752925873\n","Accuracy = 0.6676136363636362\n","Epoch: 13 of 20\n","bi=0, loss=0.2019776850938797\n","bi=10, loss=0.19967077672481537\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.14885711669921875\n","bi=10, loss=0.0738152265548706\n","Accuracy = 0.6676136363636364\n","Epoch: 15 of 20\n","bi=0, loss=0.07605945318937302\n","bi=10, loss=0.028588762506842613\n","Accuracy = 0.6590909090909092\n","Epoch: 16 of 20\n","bi=0, loss=0.10070373862981796\n","bi=10, loss=0.029063424095511436\n","Accuracy = 0.65625\n","Epoch: 17 of 20\n","bi=0, loss=0.051116954535245895\n","bi=10, loss=0.02196534164249897\n","Accuracy = 0.65625\n","Epoch: 18 of 20\n","bi=0, loss=0.034286148846149445\n","bi=10, loss=0.01446071919053793\n","Accuracy = 0.6590909090909092\n","Epoch: 19 of 20\n","bi=0, loss=0.015603121370077133\n","bi=10, loss=0.013477187603712082\n","Accuracy = 0.65625\n","Epoch: 20 of 20\n","bi=0, loss=0.011614800430834293\n","bi=10, loss=0.014379284344613552\n","Accuracy = 0.65625\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.352691411972046\n","bi=10, loss=0.8929387331008911\n","Accuracy = 0.6676136363636362\n","Epoch: 2 of 20\n","bi=0, loss=0.8735466003417969\n","bi=10, loss=0.8085183501243591\n","Accuracy = 0.6477272727272727\n","Epoch: 3 of 20\n","bi=0, loss=0.7885305881500244\n","bi=10, loss=0.6435263752937317\n","Accuracy = 0.6931818181818181\n","Epoch: 4 of 20\n","bi=0, loss=0.7315417528152466\n","bi=10, loss=0.6576179265975952\n","Accuracy = 0.6960227272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.5561408400535583\n","bi=10, loss=1.3950878381729126\n","Accuracy = 0.7130681818181818\n","Epoch: 6 of 20\n","bi=0, loss=0.4661767780780792\n","bi=10, loss=0.36831730604171753\n","Accuracy = 0.7159090909090908\n","Epoch: 7 of 20\n","bi=0, loss=0.2828209400177002\n","bi=10, loss=0.20225653052330017\n","Accuracy = 0.71875\n","Epoch: 8 of 20\n","bi=0, loss=0.2433856725692749\n","bi=10, loss=0.15087229013442993\n","Accuracy = 0.690340909090909\n","Epoch: 9 of 20\n","bi=0, loss=0.2512989640235901\n","bi=10, loss=0.046958282589912415\n","Accuracy = 0.6704545454545454\n","Epoch: 10 of 20\n","bi=0, loss=0.20395268499851227\n","bi=10, loss=0.0720345675945282\n","Accuracy = 0.7329545454545454\n","Epoch: 11 of 20\n","bi=0, loss=0.09448865801095963\n","bi=10, loss=0.044282007962465286\n","Accuracy = 0.6818181818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.07738445699214935\n","bi=10, loss=0.04173818230628967\n","Accuracy = 0.6306818181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.34253859519958496\n","bi=10, loss=0.18551896512508392\n","Accuracy = 0.6505681818181819\n","Epoch: 14 of 20\n","bi=0, loss=0.05935709550976753\n","bi=10, loss=0.1318681687116623\n","Accuracy = 0.7073863636363636\n","Epoch: 15 of 20\n","bi=0, loss=0.3330942690372467\n","bi=10, loss=0.03675323724746704\n","Accuracy = 0.6960227272727273\n","Epoch: 16 of 20\n","bi=0, loss=0.07143476605415344\n","bi=10, loss=0.05519820377230644\n","Accuracy = 0.7159090909090908\n","Epoch: 17 of 20\n","bi=0, loss=0.08005889505147934\n","bi=10, loss=0.014182568527758121\n","Accuracy = 0.6875\n","Epoch: 18 of 20\n","bi=0, loss=0.035042352974414825\n","bi=10, loss=0.009893958456814289\n","Accuracy = 0.6875\n","Epoch: 19 of 20\n","bi=0, loss=0.0191718190908432\n","bi=10, loss=0.009149243123829365\n","Accuracy = 0.6960227272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.016087962314486504\n","bi=10, loss=0.007740304805338383\n","Accuracy = 0.6988636363636364\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 1\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.348362922668457\n","Accuracy = 0.6676136363636364\n","Epoch: 2 of 20\n","bi=0, loss=0.8813350796699524\n","Accuracy = 0.6875\n","Epoch: 3 of 20\n","bi=0, loss=0.7571104764938354\n","Accuracy = 0.6505681818181819\n","Epoch: 4 of 20\n","bi=0, loss=0.7485039234161377\n","Accuracy = 0.6960227272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.5648009777069092\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.5115569829940796\n","Accuracy = 0.7017045454545455\n","Epoch: 7 of 20\n","bi=0, loss=0.6120327711105347\n","Accuracy = 0.6903409090909092\n","Epoch: 8 of 20\n","bi=0, loss=0.3630591928958893\n","Accuracy = 0.6051136363636364\n","Epoch: 9 of 20\n","bi=0, loss=0.3634701371192932\n","Accuracy = 0.6875\n","Epoch: 10 of 20\n","bi=0, loss=0.19829092919826508\n","Accuracy = 0.6335227272727272\n","Epoch: 11 of 20\n","bi=0, loss=0.14552778005599976\n","Accuracy = 0.6676136363636364\n","Epoch: 12 of 20\n","bi=0, loss=0.10323809087276459\n","Accuracy = 0.6534090909090908\n","Epoch: 13 of 20\n","bi=0, loss=0.06195714324712753\n","Accuracy = 0.6846590909090909\n","Epoch: 14 of 20\n","bi=0, loss=0.07601698487997055\n","Accuracy = 0.6789772727272727\n","Epoch: 15 of 20\n","bi=0, loss=0.027654044330120087\n","Accuracy = 0.6590909090909091\n","Epoch: 16 of 20\n","bi=0, loss=0.02333226054906845\n","Accuracy = 0.6761363636363636\n","Epoch: 17 of 20\n","bi=0, loss=0.018431473523378372\n","Accuracy = 0.6789772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.012846420519053936\n","Accuracy = 0.6704545454545454\n","Epoch: 19 of 20\n","bi=0, loss=0.036333005875349045\n","Accuracy = 0.6704545454545454\n","Epoch: 20 of 20\n","bi=0, loss=0.011767237447202206\n","Accuracy = 0.6704545454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 2\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3520973920822144\n","Accuracy = 0.6590909090909091\n","Epoch: 2 of 20\n","bi=0, loss=0.8374237418174744\n","Accuracy = 0.6676136363636364\n","Epoch: 3 of 20\n","bi=0, loss=0.7218881845474243\n","Accuracy = 0.6789772727272727\n","Epoch: 4 of 20\n","bi=0, loss=0.6602104306221008\n","Accuracy = 0.7130681818181819\n","Epoch: 5 of 20\n","bi=0, loss=0.5318114161491394\n","Accuracy = 0.7045454545454546\n","Epoch: 6 of 20\n","bi=0, loss=0.4336172640323639\n","Accuracy = 0.7244318181818181\n","Epoch: 7 of 20\n","bi=0, loss=0.3945009112358093\n","Accuracy = 0.7443181818181819\n","Epoch: 8 of 20\n","bi=0, loss=0.21643221378326416\n","Accuracy = 0.7215909090909092\n","Epoch: 9 of 20\n","bi=0, loss=0.08035626262426376\n","Accuracy = 0.7215909090909092\n","Epoch: 10 of 20\n","bi=0, loss=0.10412666201591492\n","Accuracy = 0.7215909090909092\n","Epoch: 11 of 20\n","bi=0, loss=0.07910097390413284\n","Accuracy = 0.7159090909090908\n","Epoch: 12 of 20\n","bi=0, loss=0.2610052824020386\n","Accuracy = 0.6335227272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.268003910779953\n","Accuracy = 0.7301136363636362\n","Epoch: 14 of 20\n","bi=0, loss=0.30814865231513977\n","Accuracy = 0.6903409090909091\n","Epoch: 15 of 20\n","bi=0, loss=0.15271255373954773\n","Accuracy = 0.71875\n","Epoch: 16 of 20\n","bi=0, loss=0.057275280356407166\n","Accuracy = 0.7386363636363638\n","Epoch: 17 of 20\n","bi=0, loss=0.08910851180553436\n","Accuracy = 0.7329545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.04034760966897011\n","Accuracy = 0.7386363636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.04388204962015152\n","Accuracy = 0.7386363636363636\n","Epoch: 20 of 20\n","bi=0, loss=0.036541834473609924\n","Accuracy = 0.7386363636363638\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 3\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3490761518478394\n","Accuracy = 0.6732954545454545\n","Epoch: 2 of 20\n","bi=0, loss=0.7766154408454895\n","Accuracy = 0.6704545454545454\n","Epoch: 3 of 20\n","bi=0, loss=0.632703423500061\n","Accuracy = 0.5880681818181819\n","Epoch: 4 of 20\n","bi=0, loss=0.719529926776886\n","Accuracy = 0.7045454545454546\n","Epoch: 5 of 20\n","bi=0, loss=0.48090168833732605\n","Accuracy = 0.7102272727272727\n","Epoch: 6 of 20\n","bi=0, loss=0.43258246779441833\n","Accuracy = 0.7073863636363635\n","Epoch: 7 of 20\n","bi=0, loss=0.41932639479637146\n","Accuracy = 0.6704545454545455\n","Epoch: 8 of 20\n","bi=0, loss=0.28234022855758667\n","Accuracy = 0.6590909090909092\n","Epoch: 9 of 20\n","bi=0, loss=0.1925467550754547\n","Accuracy = 0.7159090909090909\n","Epoch: 10 of 20\n","bi=0, loss=0.0804019570350647\n","Accuracy = 0.6960227272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.05641113594174385\n","Accuracy = 0.6732954545454545\n","Epoch: 12 of 20\n","bi=0, loss=0.11523348838090897\n","Accuracy = 0.7017045454545454\n","Epoch: 13 of 20\n","bi=0, loss=0.03540932387113571\n","Accuracy = 0.6619318181818182\n","Epoch: 14 of 20\n","bi=0, loss=0.06307268887758255\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.011721746064722538\n","Accuracy = 0.6931818181818181\n","Epoch: 16 of 20\n","bi=0, loss=0.021099818870425224\n","Accuracy = 0.6960227272727273\n","Epoch: 17 of 20\n","bi=0, loss=0.01033434085547924\n","Accuracy = 0.6903409090909091\n","Epoch: 18 of 20\n","bi=0, loss=0.006777250673621893\n","Accuracy = 0.6903409090909091\n","Epoch: 19 of 20\n","bi=0, loss=0.007821985520422459\n","Accuracy = 0.6903409090909091\n","Epoch: 20 of 20\n","bi=0, loss=0.010002726688981056\n","Accuracy = 0.6903409090909091\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 4\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3355389833450317\n","Accuracy = 0.6761363636363635\n","Epoch: 2 of 20\n","bi=0, loss=0.6744728684425354\n","Accuracy = 0.6761363636363635\n","Epoch: 3 of 20\n","bi=0, loss=0.653927743434906\n","Accuracy = 0.6818181818181819\n","Epoch: 4 of 20\n","bi=0, loss=0.557549238204956\n","Accuracy = 0.6875\n","Epoch: 5 of 20\n","bi=0, loss=0.456223726272583\n","Accuracy = 0.7073863636363636\n","Epoch: 6 of 20\n","bi=0, loss=0.29177242517471313\n","Accuracy = 0.6988636363636364\n","Epoch: 7 of 20\n","bi=0, loss=0.1963396668434143\n","Accuracy = 0.7130681818181819\n","Epoch: 8 of 20\n","bi=0, loss=0.17032408714294434\n","Accuracy = 0.7045454545454546\n","Epoch: 9 of 20\n","bi=0, loss=0.1685846596956253\n","Accuracy = 0.6988636363636364\n","Epoch: 10 of 20\n","bi=0, loss=0.1521458923816681\n","Accuracy = 0.6676136363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.2881048023700714\n","Accuracy = 0.6903409090909091\n","Epoch: 12 of 20\n","bi=0, loss=0.08083698153495789\n","Accuracy = 0.7045454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.34593117237091064\n","Accuracy = 0.6903409090909091\n","Epoch: 14 of 20\n","bi=0, loss=0.08579996228218079\n","Accuracy = 0.7159090909090908\n","Epoch: 15 of 20\n","bi=0, loss=0.15097051858901978\n","Accuracy = 0.7357954545454545\n","Epoch: 16 of 20\n","bi=0, loss=0.03407113626599312\n","Accuracy = 0.7301136363636362\n","Epoch: 17 of 20\n","bi=0, loss=0.018274664878845215\n","Accuracy = 0.7130681818181818\n","Epoch: 18 of 20\n","bi=0, loss=0.02928508073091507\n","Accuracy = 0.7159090909090908\n","Epoch: 19 of 20\n","bi=0, loss=0.02245292440056801\n","Accuracy = 0.71875\n","Epoch: 20 of 20\n","bi=0, loss=0.01455333549529314\n","Accuracy = 0.7159090909090908\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3423711061477661\n","Accuracy = 0.6619318181818181\n","Epoch: 2 of 20\n","bi=0, loss=0.9129085540771484\n","Accuracy = 0.6761363636363635\n","Epoch: 3 of 20\n","bi=0, loss=0.8793627619743347\n","Accuracy = 0.6903409090909091\n","Epoch: 4 of 20\n","bi=0, loss=0.7722039222717285\n","Accuracy = 0.7073863636363636\n","Epoch: 5 of 20\n","bi=0, loss=0.588186502456665\n","Accuracy = 0.71875\n","Epoch: 6 of 20\n","bi=0, loss=0.5056540369987488\n","Accuracy = 0.7272727272727273\n","Epoch: 7 of 20\n","bi=0, loss=0.35234469175338745\n","Accuracy = 0.7244318181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.2314988225698471\n","Accuracy = 0.6960227272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.16563744843006134\n","Accuracy = 0.7130681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.3372018337249756\n","Accuracy = 0.7073863636363636\n","Epoch: 11 of 20\n","bi=0, loss=0.14355657994747162\n","Accuracy = 0.6477272727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.2508236765861511\n","Accuracy = 0.6647727272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.1301417350769043\n","Accuracy = 0.7215909090909091\n","Epoch: 14 of 20\n","bi=0, loss=0.1512513905763626\n","Accuracy = 0.6903409090909092\n","Epoch: 15 of 20\n","bi=0, loss=0.129481241106987\n","Accuracy = 0.6988636363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.06697377562522888\n","Accuracy = 0.7244318181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.037842828780412674\n","Accuracy = 0.7244318181818181\n","Epoch: 18 of 20\n","bi=0, loss=0.02576550655066967\n","Accuracy = 0.7272727272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.016713133081793785\n","Accuracy = 0.7215909090909092\n","Epoch: 20 of 20\n","bi=0, loss=0.01696002669632435\n","Accuracy = 0.7130681818181819\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3215903043746948\n","Accuracy = 0.6676136363636362\n","Epoch: 2 of 20\n","bi=0, loss=0.8001662492752075\n","Accuracy = 0.6704545454545454\n","Epoch: 3 of 20\n","bi=0, loss=0.7444180846214294\n","Accuracy = 0.7329545454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.6997120976448059\n","Accuracy = 0.7244318181818181\n","Epoch: 5 of 20\n","bi=0, loss=0.5286640524864197\n","Accuracy = 0.6931818181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.4682241678237915\n","Accuracy = 0.7159090909090908\n","Epoch: 7 of 20\n","bi=0, loss=0.26554805040359497\n","Accuracy = 0.7414772727272727\n","Epoch: 8 of 20\n","bi=0, loss=0.28592151403427124\n","Accuracy = 0.6704545454545454\n","Epoch: 9 of 20\n","bi=0, loss=0.17407630383968353\n","Accuracy = 0.71875\n","Epoch: 10 of 20\n","bi=0, loss=0.07350042462348938\n","Accuracy = 0.7528409090909091\n","Epoch: 11 of 20\n","bi=0, loss=0.04186660423874855\n","Accuracy = 0.7102272727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.09976521134376526\n","Accuracy = 0.7386363636363636\n","Epoch: 13 of 20\n","bi=0, loss=0.041103191673755646\n","Accuracy = 0.6988636363636364\n","Epoch: 14 of 20\n","bi=0, loss=0.024676362052559853\n","Accuracy = 0.7045454545454546\n","Epoch: 15 of 20\n","bi=0, loss=0.012852194719016552\n","Accuracy = 0.7045454545454546\n","Epoch: 16 of 20\n","bi=0, loss=0.020188385620713234\n","Accuracy = 0.7556818181818182\n","Epoch: 17 of 20\n","bi=0, loss=0.01580188050866127\n","Accuracy = 0.7443181818181818\n","Epoch: 18 of 20\n","bi=0, loss=0.008385862223803997\n","Accuracy = 0.7272727272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.006851273588836193\n","Accuracy = 0.7244318181818181\n","Epoch: 20 of 20\n","bi=0, loss=0.00762257119640708\n","Accuracy = 0.7215909090909092\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3354398012161255\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.8399909138679504\n","Accuracy = 0.6789772727272727\n","Epoch: 3 of 20\n","bi=0, loss=0.81058269739151\n","Accuracy = 0.6619318181818181\n","Epoch: 4 of 20\n","bi=0, loss=0.7821683287620544\n","Accuracy = 0.7045454545454546\n","Epoch: 5 of 20\n","bi=0, loss=0.6387928128242493\n","Accuracy = 0.7130681818181819\n","Epoch: 6 of 20\n","bi=0, loss=0.5307880640029907\n","Accuracy = 0.71875\n","Epoch: 7 of 20\n","bi=0, loss=0.3812868893146515\n","Accuracy = 0.6761363636363636\n","Epoch: 8 of 20\n","bi=0, loss=0.19962584972381592\n","Accuracy = 0.6676136363636364\n","Epoch: 9 of 20\n","bi=0, loss=0.18381915986537933\n","Accuracy = 0.6931818181818181\n","Epoch: 10 of 20\n","bi=0, loss=0.28783756494522095\n","Accuracy = 0.7272727272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.08326316624879837\n","Accuracy = 0.6590909090909091\n","Epoch: 12 of 20\n","bi=0, loss=0.08546335995197296\n","Accuracy = 0.7215909090909092\n","Epoch: 13 of 20\n","bi=0, loss=0.04968036338686943\n","Accuracy = 0.7045454545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.5186261534690857\n","Accuracy = 0.6846590909090909\n","Epoch: 15 of 20\n","bi=0, loss=0.11127200722694397\n","Accuracy = 0.7045454545454546\n","Epoch: 16 of 20\n","bi=0, loss=0.07763666659593582\n","Accuracy = 0.7471590909090909\n","Epoch: 17 of 20\n","bi=0, loss=0.06348864734172821\n","Accuracy = 0.7329545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.02489870972931385\n","Accuracy = 0.7215909090909092\n","Epoch: 19 of 20\n","bi=0, loss=0.015985021367669106\n","Accuracy = 0.7102272727272727\n","Epoch: 20 of 20\n","bi=0, loss=0.0208115316927433\n","Accuracy = 0.7130681818181818\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3518736362457275\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.8102244138717651\n","Accuracy = 0.6875\n","Epoch: 3 of 20\n","bi=0, loss=0.7859217524528503\n","Accuracy = 0.65625\n","Epoch: 4 of 20\n","bi=0, loss=0.8341629505157471\n","Accuracy = 0.6903409090909091\n","Epoch: 5 of 20\n","bi=0, loss=0.6747868061065674\n","Accuracy = 0.71875\n","Epoch: 6 of 20\n","bi=0, loss=0.6107244491577148\n","Accuracy = 0.7329545454545454\n","Epoch: 7 of 20\n","bi=0, loss=0.5212903022766113\n","Accuracy = 0.7244318181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.4341087341308594\n","Accuracy = 0.7357954545454545\n","Epoch: 9 of 20\n","bi=0, loss=0.2882748544216156\n","Accuracy = 0.6335227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.40530723333358765\n","Accuracy = 0.7556818181818182\n","Epoch: 11 of 20\n","bi=0, loss=0.1466476321220398\n","Accuracy = 0.7017045454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.15327605605125427\n","Accuracy = 0.6846590909090909\n","Epoch: 13 of 20\n","bi=0, loss=0.1377076357603073\n","Accuracy = 0.7272727272727273\n","Epoch: 14 of 20\n","bi=0, loss=0.0509965755045414\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.03420382738113403\n","Accuracy = 0.6789772727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.05928090959787369\n","Accuracy = 0.6761363636363635\n","Epoch: 17 of 20\n","bi=0, loss=0.021958641707897186\n","Accuracy = 0.6931818181818182\n","Epoch: 18 of 20\n","bi=0, loss=0.015888366848230362\n","Accuracy = 0.6960227272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.016379157081246376\n","Accuracy = 0.6960227272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.009406874887645245\n","Accuracy = 0.6903409090909092\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.341120958328247\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.8349792957305908\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.8183882236480713\n","Accuracy = 0.6704545454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.8402504324913025\n","Accuracy = 0.7130681818181819\n","Epoch: 5 of 20\n","bi=0, loss=0.6989863514900208\n","Accuracy = 0.7102272727272727\n","Epoch: 6 of 20\n","bi=0, loss=0.6142164468765259\n","Accuracy = 0.6903409090909092\n","Epoch: 7 of 20\n","bi=0, loss=0.41831275820732117\n","Accuracy = 0.7045454545454546\n","Epoch: 8 of 20\n","bi=0, loss=0.31822413206100464\n","Accuracy = 0.6619318181818181\n","Epoch: 9 of 20\n","bi=0, loss=0.2849842607975006\n","Accuracy = 0.6960227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.27333343029022217\n","Accuracy = 0.6875\n","Epoch: 11 of 20\n","bi=0, loss=0.12721411883831024\n","Accuracy = 0.6477272727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.3617251515388489\n","Accuracy = 0.65625\n","Epoch: 13 of 20\n","bi=0, loss=0.10996023565530777\n","Accuracy = 0.6818181818181819\n","Epoch: 14 of 20\n","bi=0, loss=0.1187085509300232\n","Accuracy = 0.6676136363636365\n","Epoch: 15 of 20\n","bi=0, loss=0.05385929346084595\n","Accuracy = 0.6676136363636362\n","Epoch: 16 of 20\n","bi=0, loss=0.042167313396930695\n","Accuracy = 0.6732954545454545\n","Epoch: 17 of 20\n","bi=0, loss=0.01835266314446926\n","Accuracy = 0.6761363636363635\n","Epoch: 18 of 20\n","bi=0, loss=0.012669797986745834\n","Accuracy = 0.6789772727272727\n","Epoch: 19 of 20\n","bi=0, loss=0.012625536881387234\n","Accuracy = 0.6789772727272727\n","Epoch: 20 of 20\n","bi=0, loss=0.013710612431168556\n","Accuracy = 0.6789772727272727\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 3e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3385963439941406\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.8576055765151978\n","Accuracy = 0.6647727272727273\n","Epoch: 3 of 20\n","bi=0, loss=0.7826551795005798\n","Accuracy = 0.6335227272727273\n","Epoch: 4 of 20\n","bi=0, loss=0.8378082513809204\n","Accuracy = 0.6676136363636364\n","Epoch: 5 of 20\n","bi=0, loss=0.728082001209259\n","Accuracy = 0.6647727272727273\n","Epoch: 6 of 20\n","bi=0, loss=0.6657992005348206\n","Accuracy = 0.6988636363636364\n","Epoch: 7 of 20\n","bi=0, loss=0.500410795211792\n","Accuracy = 0.6988636363636364\n","Epoch: 8 of 20\n","bi=0, loss=0.3012104034423828\n","Accuracy = 0.7017045454545454\n","Epoch: 9 of 20\n","bi=0, loss=0.1657598316669464\n","Accuracy = 0.6875\n","Epoch: 10 of 20\n","bi=0, loss=0.5773540139198303\n","Accuracy = 0.6960227272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.8568240404129028\n","Accuracy = 0.7017045454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.3623700737953186\n","Accuracy = 0.6960227272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.3054719865322113\n","Accuracy = 0.7130681818181818\n","Epoch: 14 of 20\n","bi=0, loss=0.18400409817695618\n","Accuracy = 0.71875\n","Epoch: 15 of 20\n","bi=0, loss=0.0992908850312233\n","Accuracy = 0.6931818181818182\n","Epoch: 16 of 20\n","bi=0, loss=0.09411759674549103\n","Accuracy = 0.7159090909090908\n","Epoch: 17 of 20\n","bi=0, loss=0.059608716517686844\n","Accuracy = 0.7159090909090908\n","Epoch: 18 of 20\n","bi=0, loss=0.058376796543598175\n","Accuracy = 0.7073863636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.049954697489738464\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.05254324525594711\n","Accuracy = 0.7045454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 1\n","num_train_steps = 973, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3081258535385132\n","bi=10, loss=0.9137352705001831\n","bi=20, loss=0.9331231713294983\n","bi=30, loss=0.9360628128051758\n","bi=40, loss=0.8715589046478271\n","Accuracy = 0.6903409090909091\n","Epoch: 2 of 20\n","bi=0, loss=0.6298010349273682\n","bi=10, loss=0.56219881772995\n","bi=20, loss=0.7126684784889221\n","bi=30, loss=0.468340128660202\n","bi=40, loss=0.6425797343254089\n","Accuracy = 0.5880681818181819\n","Epoch: 3 of 20\n","bi=0, loss=0.276816189289093\n","bi=10, loss=0.47317802906036377\n","bi=20, loss=0.746266782283783\n","bi=30, loss=0.37557876110076904\n","bi=40, loss=0.5230891704559326\n","Accuracy = 0.59375\n","Epoch: 4 of 20\n","bi=0, loss=0.2888931334018707\n","bi=10, loss=0.3843260407447815\n","bi=20, loss=0.3100522458553314\n","bi=30, loss=0.2577161192893982\n","bi=40, loss=0.3544631004333496\n","Accuracy = 0.6676136363636364\n","Epoch: 5 of 20\n","bi=0, loss=0.20265105366706848\n","bi=10, loss=0.18755577504634857\n","bi=20, loss=0.22972407937049866\n","bi=30, loss=0.3680506944656372\n","bi=40, loss=0.3650047779083252\n","Accuracy = 0.6903409090909091\n","Epoch: 6 of 20\n","bi=0, loss=0.10962909460067749\n","bi=10, loss=0.2581334114074707\n","bi=20, loss=0.4463617205619812\n","bi=30, loss=0.2675281763076782\n","bi=40, loss=0.33101826906204224\n","Accuracy = 0.6789772727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.1808454692363739\n","bi=10, loss=0.16445381939411163\n","bi=20, loss=0.13425324857234955\n","bi=30, loss=0.04657052084803581\n","bi=40, loss=0.8605542778968811\n","Accuracy = 0.5909090909090909\n","Epoch: 8 of 20\n","bi=0, loss=0.21809202432632446\n","bi=10, loss=0.1255517303943634\n","bi=20, loss=0.4257708787918091\n","bi=30, loss=0.0418601855635643\n","bi=40, loss=0.24128630757331848\n","Accuracy = 0.5880681818181819\n","Epoch: 9 of 20\n","bi=0, loss=0.14558108150959015\n","bi=10, loss=0.21131858229637146\n","bi=20, loss=0.37704408168792725\n","bi=30, loss=0.5817942023277283\n","bi=40, loss=0.1880580484867096\n","Accuracy = 0.6335227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.06894741952419281\n","bi=10, loss=0.6584926247596741\n","bi=20, loss=0.05859088897705078\n","bi=30, loss=0.023099500685930252\n","bi=40, loss=0.17585405707359314\n","Accuracy = 0.5795454545454546\n","Epoch: 11 of 20\n","bi=0, loss=0.10237389802932739\n","bi=10, loss=0.10912195593118668\n","bi=20, loss=0.21046431362628937\n","bi=30, loss=0.06773154437541962\n","bi=40, loss=0.1480056792497635\n","Accuracy = 0.6392045454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.05929481238126755\n","bi=10, loss=0.051443759351968765\n","bi=20, loss=0.04003264009952545\n","bi=30, loss=0.021720778197050095\n","bi=40, loss=0.12713408470153809\n","Accuracy = 0.6619318181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.04857633635401726\n","bi=10, loss=0.045600492507219315\n","bi=20, loss=0.02679881639778614\n","bi=30, loss=0.009511920623481274\n","bi=40, loss=0.13190340995788574\n","Accuracy = 0.6278409090909092\n","Epoch: 14 of 20\n","bi=0, loss=0.03955107927322388\n","bi=10, loss=0.02793847769498825\n","bi=20, loss=0.03165985643863678\n","bi=30, loss=0.005175750702619553\n","bi=40, loss=0.12395375967025757\n","Accuracy = 0.6278409090909091\n","Epoch: 15 of 20\n","bi=0, loss=0.0457841120660305\n","bi=10, loss=0.030514735728502274\n","bi=20, loss=0.031987860798835754\n","bi=30, loss=0.005677932407706976\n","bi=40, loss=0.09477733075618744\n","Accuracy = 0.65625\n","Epoch: 16 of 20\n","bi=0, loss=0.0369316041469574\n","bi=10, loss=0.022879047319293022\n","bi=20, loss=0.02869468927383423\n","bi=30, loss=0.003858358832076192\n","bi=40, loss=0.06969830393791199\n","Accuracy = 0.6420454545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.027130762115120888\n","bi=10, loss=0.033215805888175964\n","bi=20, loss=0.02224467508494854\n","bi=30, loss=0.0037250984460115433\n","bi=40, loss=0.061820995062589645\n","Accuracy = 0.6448863636363636\n","Epoch: 18 of 20\n","bi=0, loss=0.04401513561606407\n","bi=10, loss=0.02573496475815773\n","bi=20, loss=0.032279789447784424\n","bi=30, loss=0.0028460295870900154\n","bi=40, loss=0.08284807205200195\n","Accuracy = 0.6392045454545454\n","Epoch: 19 of 20\n","bi=0, loss=0.05423445627093315\n","bi=10, loss=0.033158496022224426\n","bi=20, loss=0.04076511412858963\n","bi=30, loss=0.002627570880576968\n","bi=40, loss=0.06543473899364471\n","Accuracy = 0.6363636363636364\n","Epoch: 20 of 20\n","bi=0, loss=0.02415093407034874\n","bi=10, loss=0.03424568474292755\n","bi=20, loss=0.038778722286224365\n","bi=30, loss=0.003138253465294838\n","bi=40, loss=0.062484562397003174\n","Accuracy = 0.6335227272727273\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 2\n","num_train_steps = 973, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.2763621807098389\n","bi=10, loss=0.7656068801879883\n","bi=20, loss=1.1721993684768677\n","bi=30, loss=0.8007217645645142\n","bi=40, loss=1.1274678707122803\n","Accuracy = 0.7045454545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.6029313206672668\n","bi=10, loss=0.7058676481246948\n","bi=20, loss=1.313542366027832\n","bi=30, loss=0.7526910305023193\n","bi=40, loss=0.7328824996948242\n","Accuracy = 0.7073863636363636\n","Epoch: 3 of 20\n","bi=0, loss=0.43106988072395325\n","bi=10, loss=0.34696274995803833\n","bi=20, loss=0.7313891053199768\n","bi=30, loss=0.2669062316417694\n","bi=40, loss=0.7683234214782715\n","Accuracy = 0.6988636363636364\n","Epoch: 4 of 20\n","bi=0, loss=0.6445119380950928\n","bi=10, loss=0.17181050777435303\n","bi=20, loss=0.46847280859947205\n","bi=30, loss=0.3697875738143921\n","bi=40, loss=0.36443912982940674\n","Accuracy = 0.6590909090909091\n","Epoch: 5 of 20\n","bi=0, loss=0.11656694114208221\n","bi=10, loss=0.26041871309280396\n","bi=20, loss=0.3422238230705261\n","bi=30, loss=0.11749102175235748\n","bi=40, loss=0.3999132513999939\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.5384896397590637\n","bi=10, loss=0.7665005326271057\n","bi=20, loss=0.071853406727314\n","bi=30, loss=0.030645174905657768\n","bi=40, loss=0.1425734907388687\n","Accuracy = 0.6818181818181819\n","Epoch: 7 of 20\n","bi=0, loss=0.036771051585674286\n","bi=10, loss=0.0625641867518425\n","bi=20, loss=0.368632972240448\n","bi=30, loss=0.08840374648571014\n","bi=40, loss=0.11489475518465042\n","Accuracy = 0.5994318181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.48236197233200073\n","bi=10, loss=0.13961541652679443\n","bi=20, loss=0.36965447664260864\n","bi=30, loss=0.026446005329489708\n","bi=40, loss=0.09601923823356628\n","Accuracy = 0.6988636363636365\n","Epoch: 9 of 20\n","bi=0, loss=0.029575804248452187\n","bi=10, loss=0.2657988369464874\n","bi=20, loss=0.1526702642440796\n","bi=30, loss=0.012441747821867466\n","bi=40, loss=0.128530815243721\n","Accuracy = 0.7130681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.08509403467178345\n","bi=10, loss=0.21542853116989136\n","bi=20, loss=0.04464373365044594\n","bi=30, loss=0.00828121230006218\n","bi=40, loss=0.04390469193458557\n","Accuracy = 0.71875\n","Epoch: 11 of 20\n","bi=0, loss=0.012150059454143047\n","bi=10, loss=0.05798563361167908\n","bi=20, loss=0.008281311951577663\n","bi=30, loss=0.0024727466516196728\n","bi=40, loss=0.036324989050626755\n","Accuracy = 0.6875\n","Epoch: 12 of 20\n","bi=0, loss=0.005295773968100548\n","bi=10, loss=0.006810228805989027\n","bi=20, loss=0.004605498164892197\n","bi=30, loss=0.0014927982119843364\n","bi=40, loss=0.011484440416097641\n","Accuracy = 0.7102272727272727\n","Epoch: 13 of 20\n","bi=0, loss=0.002860739128664136\n","bi=10, loss=0.018603522330522537\n","bi=20, loss=0.0024992506951093674\n","bi=30, loss=0.0010199116077274084\n","bi=40, loss=0.08263226598501205\n","Accuracy = 0.7073863636363635\n","Epoch: 14 of 20\n","bi=0, loss=0.010145039297640324\n","bi=10, loss=0.005086760967969894\n","bi=20, loss=0.005796568468213081\n","bi=30, loss=0.0011364480014890432\n","bi=40, loss=0.008913293480873108\n","Accuracy = 0.7244318181818181\n","Epoch: 15 of 20\n","bi=0, loss=0.003086840733885765\n","bi=10, loss=0.004261142574250698\n","bi=20, loss=0.003759917104616761\n","bi=30, loss=0.006911313161253929\n","bi=40, loss=0.006296113599091768\n","Accuracy = 0.6931818181818181\n","Epoch: 16 of 20\n","bi=0, loss=0.0024353880435228348\n","bi=10, loss=0.0014969459734857082\n","bi=20, loss=0.001975738676264882\n","bi=30, loss=0.0008835705230012536\n","bi=40, loss=0.0033695255406200886\n","Accuracy = 0.6960227272727273\n","Epoch: 17 of 20\n","bi=0, loss=0.0014652055688202381\n","bi=10, loss=0.002653553383424878\n","bi=20, loss=0.0017502898117527366\n","bi=30, loss=0.0009208699921146035\n","bi=40, loss=0.0022274183575063944\n","Accuracy = 0.7045454545454546\n","Epoch: 18 of 20\n","bi=0, loss=0.0016306235920637846\n","bi=10, loss=0.002399844117462635\n","bi=20, loss=0.0021552080288529396\n","bi=30, loss=0.000717661518137902\n","bi=40, loss=0.002403372898697853\n","Accuracy = 0.7073863636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.0017939346143975854\n","bi=10, loss=0.001586572965607047\n","bi=20, loss=0.0018627960234880447\n","bi=30, loss=0.0006185739766806364\n","bi=40, loss=0.002683603437617421\n","Accuracy = 0.7073863636363636\n","Epoch: 20 of 20\n","bi=0, loss=0.001131004886701703\n","bi=10, loss=0.0014451779425144196\n","bi=20, loss=0.001428284333087504\n","bi=30, loss=0.0007215217920020223\n","bi=40, loss=0.0021109397057443857\n","Accuracy = 0.7073863636363636\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 3\n","num_train_steps = 973, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3214129209518433\n","bi=10, loss=0.7770062685012817\n","bi=20, loss=0.876868486404419\n","bi=30, loss=0.6909858584403992\n","bi=40, loss=1.0306637287139893\n","Accuracy = 0.6392045454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.6184741258621216\n","bi=10, loss=0.5254755616188049\n","bi=20, loss=0.7274500131607056\n","bi=30, loss=0.6798064708709717\n","bi=40, loss=0.8084801435470581\n","Accuracy = 0.6818181818181818\n","Epoch: 3 of 20\n","bi=0, loss=0.4551372528076172\n","bi=10, loss=0.43906015157699585\n","bi=20, loss=0.32685449719429016\n","bi=30, loss=0.4531398415565491\n","bi=40, loss=0.5680820941925049\n","Accuracy = 0.7159090909090908\n","Epoch: 4 of 20\n","bi=0, loss=0.0901632159948349\n","bi=10, loss=0.24883855879306793\n","bi=20, loss=0.6057716012001038\n","bi=30, loss=0.2956976592540741\n","bi=40, loss=0.3117745816707611\n","Accuracy = 0.7017045454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.03175469860434532\n","bi=10, loss=0.17056924104690552\n","bi=20, loss=0.70890212059021\n","bi=30, loss=0.3681281805038452\n","bi=40, loss=0.3106010854244232\n","Accuracy = 0.6931818181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.022353917360305786\n","bi=10, loss=0.36394503712654114\n","bi=20, loss=0.314193993806839\n","bi=30, loss=0.1292729526758194\n","bi=40, loss=0.21503384411334991\n","Accuracy = 0.7073863636363636\n","Epoch: 7 of 20\n","bi=0, loss=0.021180888637900352\n","bi=10, loss=0.05699728801846504\n","bi=20, loss=0.11394158750772476\n","bi=30, loss=0.2259797900915146\n","bi=40, loss=0.3082042634487152\n","Accuracy = 0.5454545454545454\n","Epoch: 8 of 20\n","bi=0, loss=0.03387586027383804\n","bi=10, loss=0.04645458981394768\n","bi=20, loss=0.07412883639335632\n","bi=30, loss=0.05150960013270378\n","bi=40, loss=0.20417581498622894\n","Accuracy = 0.6448863636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.006316810846328735\n","bi=10, loss=0.032074183225631714\n","bi=20, loss=0.09401285648345947\n","bi=30, loss=0.0758998692035675\n","bi=40, loss=0.30107131600379944\n","Accuracy = 0.6619318181818182\n","Epoch: 10 of 20\n","bi=0, loss=0.008165743201971054\n","bi=10, loss=0.06030993163585663\n","bi=20, loss=0.1447109580039978\n","bi=30, loss=0.012366488575935364\n","bi=40, loss=0.07446695864200592\n","Accuracy = 0.6761363636363635\n","Epoch: 11 of 20\n","bi=0, loss=0.0068082790821790695\n","bi=10, loss=0.015489853918552399\n","bi=20, loss=0.01079511921852827\n","bi=30, loss=0.027630815282464027\n","bi=40, loss=0.038888368755578995\n","Accuracy = 0.6818181818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.0021861675195395947\n","bi=10, loss=0.10512974858283997\n","bi=20, loss=0.008082129061222076\n","bi=30, loss=0.002655178774148226\n","bi=40, loss=0.41625937819480896\n","Accuracy = 0.6960227272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.001218888210132718\n","bi=10, loss=0.01011146605014801\n","bi=20, loss=0.0197821706533432\n","bi=30, loss=0.0015249731950461864\n","bi=40, loss=0.11540956795215607\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.001033311360515654\n","bi=10, loss=0.006596037186682224\n","bi=20, loss=0.008664547465741634\n","bi=30, loss=0.0020736465230584145\n","bi=40, loss=0.14074237644672394\n","Accuracy = 0.6761363636363636\n","Epoch: 15 of 20\n","bi=0, loss=0.0007465553353540599\n","bi=10, loss=0.00729373749345541\n","bi=20, loss=0.004605009686201811\n","bi=30, loss=0.001360211055725813\n","bi=40, loss=0.0907818078994751\n","Accuracy = 0.6732954545454546\n","Epoch: 16 of 20\n","bi=0, loss=0.0009900062577798963\n","bi=10, loss=0.0016152018215507269\n","bi=20, loss=0.002137809293344617\n","bi=30, loss=0.0012559478636831045\n","bi=40, loss=0.009673934429883957\n","Accuracy = 0.6732954545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.0006852558581158519\n","bi=10, loss=0.003260503290221095\n","bi=20, loss=0.002220112131908536\n","bi=30, loss=0.0012290687300264835\n","bi=40, loss=0.004440844524651766\n","Accuracy = 0.6761363636363636\n","Epoch: 18 of 20\n","bi=0, loss=0.0010058401385322213\n","bi=10, loss=0.002986567560583353\n","bi=20, loss=0.002145868493244052\n","bi=30, loss=0.0008652040851302445\n","bi=40, loss=0.0049611423164606094\n","Accuracy = 0.6676136363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.000758626963943243\n","bi=10, loss=0.001925282645970583\n","bi=20, loss=0.0015706798294559121\n","bi=30, loss=0.0008147187763825059\n","bi=40, loss=0.003818599507212639\n","Accuracy = 0.6676136363636364\n","Epoch: 20 of 20\n","bi=0, loss=0.0010284408926963806\n","bi=10, loss=0.002439316129311919\n","bi=20, loss=0.0014551249332726002\n","bi=30, loss=0.0008172458037734032\n","bi=40, loss=0.0034551892895251513\n","Accuracy = 0.6704545454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 4\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3839068412780762\n","bi=10, loss=0.5492793321609497\n","bi=20, loss=0.7116938829421997\n","bi=30, loss=1.189684510231018\n","bi=40, loss=0.7796013951301575\n","Accuracy = 0.6789772727272727\n","Epoch: 2 of 20\n","bi=0, loss=0.7230600118637085\n","bi=10, loss=0.32310277223587036\n","bi=20, loss=0.413898766040802\n","bi=30, loss=0.8040209412574768\n","bi=40, loss=0.747672975063324\n","Accuracy = 0.7045454545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.6731663942337036\n","bi=10, loss=0.6153541803359985\n","bi=20, loss=0.8644472360610962\n","bi=30, loss=0.9556015133857727\n","bi=40, loss=0.6157944798469543\n","Accuracy = 0.7102272727272727\n","Epoch: 4 of 20\n","bi=0, loss=0.5032867193222046\n","bi=10, loss=0.40661922097206116\n","bi=20, loss=0.8961246013641357\n","bi=30, loss=0.5612150430679321\n","bi=40, loss=0.6414028406143188\n","Accuracy = 0.6818181818181819\n","Epoch: 5 of 20\n","bi=0, loss=0.883211612701416\n","bi=10, loss=0.4492814242839813\n","bi=20, loss=0.28462690114974976\n","bi=30, loss=0.3476623296737671\n","bi=40, loss=0.35092273354530334\n","Accuracy = 0.7130681818181818\n","Epoch: 6 of 20\n","bi=0, loss=0.6295262575149536\n","bi=10, loss=0.14830809831619263\n","bi=20, loss=0.11308963596820831\n","bi=30, loss=0.3748004138469696\n","bi=40, loss=0.8759161233901978\n","Accuracy = 0.6960227272727273\n","Epoch: 7 of 20\n","bi=0, loss=0.33727189898490906\n","bi=10, loss=0.3112821877002716\n","bi=20, loss=0.15354357659816742\n","bi=30, loss=1.0704524517059326\n","bi=40, loss=0.9081995487213135\n","Accuracy = 0.6704545454545455\n","Epoch: 8 of 20\n","bi=0, loss=0.5633551478385925\n","bi=10, loss=0.19626858830451965\n","bi=20, loss=0.19234807789325714\n","bi=30, loss=0.27535828948020935\n","bi=40, loss=0.5503889918327332\n","Accuracy = 0.6988636363636365\n","Epoch: 9 of 20\n","bi=0, loss=0.3746781647205353\n","bi=10, loss=0.13296356797218323\n","bi=20, loss=0.13614100217819214\n","bi=30, loss=0.2906405031681061\n","bi=40, loss=0.5530333518981934\n","Accuracy = 0.71875\n","Epoch: 10 of 20\n","bi=0, loss=0.417680948972702\n","bi=10, loss=0.1332859992980957\n","bi=20, loss=0.05914470553398132\n","bi=30, loss=0.2767685353755951\n","bi=40, loss=0.2809842526912689\n","Accuracy = 0.7301136363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.24982593953609467\n","bi=10, loss=0.1326681524515152\n","bi=20, loss=0.042640164494514465\n","bi=30, loss=0.2477799952030182\n","bi=40, loss=0.6050310730934143\n","Accuracy = 0.6903409090909092\n","Epoch: 12 of 20\n","bi=0, loss=0.12738530337810516\n","bi=10, loss=0.07921605557203293\n","bi=20, loss=0.03697096183896065\n","bi=30, loss=0.2084113359451294\n","bi=40, loss=0.09090082347393036\n","Accuracy = 0.6818181818181819\n","Epoch: 13 of 20\n","bi=0, loss=0.057110197842121124\n","bi=10, loss=0.09851060807704926\n","bi=20, loss=0.020714620128273964\n","bi=30, loss=0.2970496416091919\n","bi=40, loss=0.05957155302166939\n","Accuracy = 0.6619318181818182\n","Epoch: 14 of 20\n","bi=0, loss=0.05775122717022896\n","bi=10, loss=0.08225934952497482\n","bi=20, loss=0.047227099537849426\n","bi=30, loss=0.28496068716049194\n","bi=40, loss=0.07264766842126846\n","Accuracy = 0.7045454545454546\n","Epoch: 15 of 20\n","bi=0, loss=0.04810168594121933\n","bi=10, loss=0.2989978790283203\n","bi=20, loss=0.05075716972351074\n","bi=30, loss=0.2087649703025818\n","bi=40, loss=0.32625189423561096\n","Accuracy = 0.6392045454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.048236772418022156\n","bi=10, loss=0.0504537932574749\n","bi=20, loss=0.018912870436906815\n","bi=30, loss=0.2212105244398117\n","bi=40, loss=0.04406864941120148\n","Accuracy = 0.7102272727272727\n","Epoch: 17 of 20\n","bi=0, loss=0.04106330871582031\n","bi=10, loss=0.06999623030424118\n","bi=20, loss=0.01931094378232956\n","bi=30, loss=0.26817750930786133\n","bi=40, loss=0.043610163033008575\n","Accuracy = 0.7272727272727273\n","Epoch: 18 of 20\n","bi=0, loss=0.05001305043697357\n","bi=10, loss=0.05220542475581169\n","bi=20, loss=0.018234174698591232\n","bi=30, loss=0.20542371273040771\n","bi=40, loss=0.05654887855052948\n","Accuracy = 0.7073863636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.04816604405641556\n","bi=10, loss=0.03829764947295189\n","bi=20, loss=0.015763059258461\n","bi=30, loss=0.18699787557125092\n","bi=40, loss=0.043436091393232346\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.049557317048311234\n","bi=10, loss=0.056818388402462006\n","bi=20, loss=0.014334477484226227\n","bi=30, loss=0.20387202501296997\n","bi=40, loss=0.047173433005809784\n","Accuracy = 0.7045454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3856152296066284\n","bi=10, loss=0.7134700417518616\n","bi=20, loss=0.6974613666534424\n","bi=30, loss=0.5114635825157166\n","bi=40, loss=0.7414804100990295\n","Accuracy = 0.6960227272727273\n","Epoch: 2 of 20\n","bi=0, loss=1.196567416191101\n","bi=10, loss=0.4770265817642212\n","bi=20, loss=0.3798523545265198\n","bi=30, loss=0.41596120595932007\n","bi=40, loss=0.5224776268005371\n","Accuracy = 0.7215909090909091\n","Epoch: 3 of 20\n","bi=0, loss=0.7456638813018799\n","bi=10, loss=0.5019207000732422\n","bi=20, loss=0.32074669003486633\n","bi=30, loss=1.4152417182922363\n","bi=40, loss=0.4441395401954651\n","Accuracy = 0.7357954545454546\n","Epoch: 4 of 20\n","bi=0, loss=0.6584019660949707\n","bi=10, loss=0.2049499899148941\n","bi=20, loss=0.36279362440109253\n","bi=30, loss=0.34694918990135193\n","bi=40, loss=0.1955229490995407\n","Accuracy = 0.6960227272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.9178428053855896\n","bi=10, loss=0.6462811231613159\n","bi=20, loss=0.3034295439720154\n","bi=30, loss=0.24028296768665314\n","bi=40, loss=0.6617762446403503\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=1.0822895765304565\n","bi=10, loss=0.5342254042625427\n","bi=20, loss=0.7528906464576721\n","bi=30, loss=0.8495664000511169\n","bi=40, loss=0.22053585946559906\n","Accuracy = 0.6789772727272727\n","Epoch: 7 of 20\n","bi=0, loss=1.1884541511535645\n","bi=10, loss=0.6532557010650635\n","bi=20, loss=0.3905261754989624\n","bi=30, loss=0.3798156976699829\n","bi=40, loss=0.1848936676979065\n","Accuracy = 0.6392045454545454\n","Epoch: 8 of 20\n","bi=0, loss=0.7759989500045776\n","bi=10, loss=0.2577327489852905\n","bi=20, loss=0.24716047942638397\n","bi=30, loss=0.21737951040267944\n","bi=40, loss=0.18804574012756348\n","Accuracy = 0.6875\n","Epoch: 9 of 20\n","bi=0, loss=0.6171928644180298\n","bi=10, loss=0.0899098813533783\n","bi=20, loss=0.09657114744186401\n","bi=30, loss=0.2337310016155243\n","bi=40, loss=0.16914556920528412\n","Accuracy = 0.6988636363636364\n","Epoch: 10 of 20\n","bi=0, loss=0.44678038358688354\n","bi=10, loss=0.0963670089840889\n","bi=20, loss=0.036852847784757614\n","bi=30, loss=0.22143691778182983\n","bi=40, loss=0.2720436155796051\n","Accuracy = 0.7272727272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.356087327003479\n","bi=10, loss=0.08389870077371597\n","bi=20, loss=0.019996358081698418\n","bi=30, loss=0.241185262799263\n","bi=40, loss=0.4558543860912323\n","Accuracy = 0.7017045454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.30649304389953613\n","bi=10, loss=0.07975910604000092\n","bi=20, loss=0.018350832164287567\n","bi=30, loss=0.22210125625133514\n","bi=40, loss=0.13822703063488007\n","Accuracy = 0.7045454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.2716345489025116\n","bi=10, loss=0.052512992173433304\n","bi=20, loss=0.10955639183521271\n","bi=30, loss=0.2188647985458374\n","bi=40, loss=0.034399110823869705\n","Accuracy = 0.7017045454545455\n","Epoch: 14 of 20\n","bi=0, loss=0.30533358454704285\n","bi=10, loss=0.06955067068338394\n","bi=20, loss=0.005704920738935471\n","bi=30, loss=0.2079186588525772\n","bi=40, loss=0.04189235717058182\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.3215789198875427\n","bi=10, loss=0.03513080254197121\n","bi=20, loss=0.06882940232753754\n","bi=30, loss=0.19555571675300598\n","bi=40, loss=0.03763500973582268\n","Accuracy = 0.7102272727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.2439197450876236\n","bi=10, loss=0.03902409225702286\n","bi=20, loss=0.006146386731415987\n","bi=30, loss=0.16532570123672485\n","bi=40, loss=0.028341230005025864\n","Accuracy = 0.6903409090909091\n","Epoch: 17 of 20\n","bi=0, loss=0.2850835919380188\n","bi=10, loss=0.03471752628684044\n","bi=20, loss=0.09170481562614441\n","bi=30, loss=0.21086673438549042\n","bi=40, loss=0.03950274735689163\n","Accuracy = 0.7045454545454546\n","Epoch: 18 of 20\n","bi=0, loss=0.2713046669960022\n","bi=10, loss=0.030794864520430565\n","bi=20, loss=0.004993426147848368\n","bi=30, loss=0.18813446164131165\n","bi=40, loss=0.048235807567834854\n","Accuracy = 0.7357954545454546\n","Epoch: 19 of 20\n","bi=0, loss=0.2955877184867859\n","bi=10, loss=0.02796960063278675\n","bi=20, loss=0.0035115517675876617\n","bi=30, loss=0.18915896117687225\n","bi=40, loss=0.027214251458644867\n","Accuracy = 0.7329545454545454\n","Epoch: 20 of 20\n","bi=0, loss=0.22208401560783386\n","bi=10, loss=0.16002047061920166\n","bi=20, loss=0.003205721266567707\n","bi=30, loss=0.20570549368858337\n","bi=40, loss=0.044289276003837585\n","Accuracy = 0.7215909090909092\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.327488660812378\n","bi=10, loss=0.5890249013900757\n","bi=20, loss=0.9725330471992493\n","bi=30, loss=1.0555768013000488\n","bi=40, loss=0.8952684998512268\n","Accuracy = 0.6676136363636362\n","Epoch: 2 of 20\n","bi=0, loss=1.6545016765594482\n","bi=10, loss=0.5437272787094116\n","bi=20, loss=0.7621600031852722\n","bi=30, loss=0.7104091644287109\n","bi=40, loss=0.6358188390731812\n","Accuracy = 0.6420454545454546\n","Epoch: 3 of 20\n","bi=0, loss=1.1887603998184204\n","bi=10, loss=0.20895759761333466\n","bi=20, loss=0.5360751152038574\n","bi=30, loss=1.1263269186019897\n","bi=40, loss=0.4366259276866913\n","Accuracy = 0.6988636363636364\n","Epoch: 4 of 20\n","bi=0, loss=1.0212712287902832\n","bi=10, loss=0.3675479292869568\n","bi=20, loss=1.175647258758545\n","bi=30, loss=0.5065178871154785\n","bi=40, loss=0.24572396278381348\n","Accuracy = 0.7130681818181819\n","Epoch: 5 of 20\n","bi=0, loss=1.1560888290405273\n","bi=10, loss=0.14200204610824585\n","bi=20, loss=0.3263033628463745\n","bi=30, loss=0.4752528667449951\n","bi=40, loss=0.40018051862716675\n","Accuracy = 0.6988636363636364\n","Epoch: 6 of 20\n","bi=0, loss=1.04844331741333\n","bi=10, loss=0.15038511157035828\n","bi=20, loss=0.24005413055419922\n","bi=30, loss=0.952893853187561\n","bi=40, loss=0.25423866510391235\n","Accuracy = 0.7017045454545454\n","Epoch: 7 of 20\n","bi=0, loss=1.1007471084594727\n","bi=10, loss=0.24867674708366394\n","bi=20, loss=0.09664134681224823\n","bi=30, loss=0.174482062458992\n","bi=40, loss=0.38530394434928894\n","Accuracy = 0.6988636363636364\n","Epoch: 8 of 20\n","bi=0, loss=0.8969731330871582\n","bi=10, loss=0.1921556144952774\n","bi=20, loss=0.19144849479198456\n","bi=30, loss=0.418551504611969\n","bi=40, loss=1.0136579275131226\n","Accuracy = 0.6590909090909091\n","Epoch: 9 of 20\n","bi=0, loss=0.6348483562469482\n","bi=10, loss=0.12820878624916077\n","bi=20, loss=0.13579651713371277\n","bi=30, loss=0.7481313347816467\n","bi=40, loss=0.3126559257507324\n","Accuracy = 0.5965909090909092\n","Epoch: 10 of 20\n","bi=0, loss=1.1980875730514526\n","bi=10, loss=0.14599096775054932\n","bi=20, loss=0.22024120390415192\n","bi=30, loss=0.6665102243423462\n","bi=40, loss=0.11534769833087921\n","Accuracy = 0.6732954545454546\n","Epoch: 11 of 20\n","bi=0, loss=0.928831934928894\n","bi=10, loss=0.0822434201836586\n","bi=20, loss=0.3050552010536194\n","bi=30, loss=0.8956516981124878\n","bi=40, loss=0.15126946568489075\n","Accuracy = 0.6619318181818181\n","Epoch: 12 of 20\n","bi=0, loss=0.587979257106781\n","bi=10, loss=0.44332385063171387\n","bi=20, loss=0.09299390017986298\n","bi=30, loss=0.2010401040315628\n","bi=40, loss=0.13941873610019684\n","Accuracy = 0.6420454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.3175312876701355\n","bi=10, loss=0.08077751100063324\n","bi=20, loss=0.19200573861598969\n","bi=30, loss=0.28951185941696167\n","bi=40, loss=0.0591789186000824\n","Accuracy = 0.6164772727272727\n","Epoch: 14 of 20\n","bi=0, loss=0.34845468401908875\n","bi=10, loss=0.07704061269760132\n","bi=20, loss=0.12167585641145706\n","bi=30, loss=0.2208511084318161\n","bi=40, loss=0.08441530168056488\n","Accuracy = 0.6392045454545454\n","Epoch: 15 of 20\n","bi=0, loss=0.34291940927505493\n","bi=10, loss=0.03679953143000603\n","bi=20, loss=0.01132446900010109\n","bi=30, loss=0.21043361723423004\n","bi=40, loss=0.061484720557928085\n","Accuracy = 0.6704545454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.37118491530418396\n","bi=10, loss=0.027527712285518646\n","bi=20, loss=0.006640268489718437\n","bi=30, loss=0.18557870388031006\n","bi=40, loss=0.029365716502070427\n","Accuracy = 0.6647727272727273\n","Epoch: 17 of 20\n","bi=0, loss=0.3012886047363281\n","bi=10, loss=0.061593130230903625\n","bi=20, loss=0.019390026107430458\n","bi=30, loss=0.18793925642967224\n","bi=40, loss=0.03611127287149429\n","Accuracy = 0.6676136363636364\n","Epoch: 18 of 20\n","bi=0, loss=0.29312023520469666\n","bi=10, loss=0.026393452659249306\n","bi=20, loss=0.006383872125297785\n","bi=30, loss=0.14802846312522888\n","bi=40, loss=0.04258650541305542\n","Accuracy = 0.6505681818181818\n","Epoch: 19 of 20\n","bi=0, loss=0.2994868755340576\n","bi=10, loss=0.020204834640026093\n","bi=20, loss=0.0035255614202469587\n","bi=30, loss=0.18305917084217072\n","bi=40, loss=0.025245558470487595\n","Accuracy = 0.6534090909090908\n","Epoch: 20 of 20\n","bi=0, loss=0.2756155729293823\n","bi=10, loss=0.04054882377386093\n","bi=20, loss=0.0033272081054747105\n","bi=30, loss=0.20352591574192047\n","bi=40, loss=0.035201266407966614\n","Accuracy = 0.65625\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3384478092193604\n","bi=10, loss=0.8148036003112793\n","bi=20, loss=1.391119122505188\n","bi=30, loss=1.0910727977752686\n","bi=40, loss=0.897806704044342\n","Accuracy = 0.6647727272727273\n","Epoch: 2 of 20\n","bi=0, loss=1.4390606880187988\n","bi=10, loss=0.5513482689857483\n","bi=20, loss=0.5036802291870117\n","bi=30, loss=0.8084093332290649\n","bi=40, loss=0.7952423095703125\n","Accuracy = 0.5880681818181819\n","Epoch: 3 of 20\n","bi=0, loss=1.4563015699386597\n","bi=10, loss=0.6535208225250244\n","bi=20, loss=0.5299287438392639\n","bi=30, loss=0.8105186820030212\n","bi=40, loss=0.32502713799476624\n","Accuracy = 0.7017045454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.9005342721939087\n","bi=10, loss=0.657035768032074\n","bi=20, loss=1.818403959274292\n","bi=30, loss=0.4576255679130554\n","bi=40, loss=0.39982202649116516\n","Accuracy = 0.6590909090909092\n","Epoch: 5 of 20\n","bi=0, loss=0.8721138834953308\n","bi=10, loss=0.18217089772224426\n","bi=20, loss=1.994885802268982\n","bi=30, loss=0.550883412361145\n","bi=40, loss=0.3321165144443512\n","Accuracy = 0.6875\n","Epoch: 6 of 20\n","bi=0, loss=0.7011741399765015\n","bi=10, loss=0.15578728914260864\n","bi=20, loss=1.221404790878296\n","bi=30, loss=0.6530712246894836\n","bi=40, loss=0.46282505989074707\n","Accuracy = 0.7045454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.7275263667106628\n","bi=10, loss=0.20725533366203308\n","bi=20, loss=0.7955660820007324\n","bi=30, loss=0.29631507396698\n","bi=40, loss=0.6070535778999329\n","Accuracy = 0.71875\n","Epoch: 8 of 20\n","bi=0, loss=1.5232442617416382\n","bi=10, loss=0.15852098166942596\n","bi=20, loss=0.2142907977104187\n","bi=30, loss=0.18862268328666687\n","bi=40, loss=0.47589829564094543\n","Accuracy = 0.65625\n","Epoch: 9 of 20\n","bi=0, loss=0.8049519062042236\n","bi=10, loss=0.2202400118112564\n","bi=20, loss=0.2673877477645874\n","bi=30, loss=0.2616582214832306\n","bi=40, loss=0.555574893951416\n","Accuracy = 0.7130681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.44502168893814087\n","bi=10, loss=0.08041848242282867\n","bi=20, loss=0.106709323823452\n","bi=30, loss=0.2104179561138153\n","bi=40, loss=0.6008102893829346\n","Accuracy = 0.6647727272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.3262507915496826\n","bi=10, loss=0.034059859812259674\n","bi=20, loss=0.12710344791412354\n","bi=30, loss=0.18829262256622314\n","bi=40, loss=0.3943786919116974\n","Accuracy = 0.7073863636363636\n","Epoch: 12 of 20\n","bi=0, loss=0.3129992187023163\n","bi=10, loss=0.03578612580895424\n","bi=20, loss=0.05692644789814949\n","bi=30, loss=0.0851171538233757\n","bi=40, loss=0.29206639528274536\n","Accuracy = 0.6590909090909091\n","Epoch: 13 of 20\n","bi=0, loss=0.24512669444084167\n","bi=10, loss=0.037579745054244995\n","bi=20, loss=0.058713123202323914\n","bi=30, loss=0.16822439432144165\n","bi=40, loss=0.04902607202529907\n","Accuracy = 0.7045454545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.824357807636261\n","bi=10, loss=0.04935721307992935\n","bi=20, loss=0.046641215682029724\n","bi=30, loss=0.17272460460662842\n","bi=40, loss=0.03902128338813782\n","Accuracy = 0.6420454545454546\n","Epoch: 15 of 20\n","bi=0, loss=0.5473361611366272\n","bi=10, loss=0.3224824368953705\n","bi=20, loss=0.04914388060569763\n","bi=30, loss=0.1745925098657608\n","bi=40, loss=0.041253022849559784\n","Accuracy = 0.6392045454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.3356294333934784\n","bi=10, loss=0.023397913202643394\n","bi=20, loss=0.03510757163167\n","bi=30, loss=0.14384524524211884\n","bi=40, loss=0.1855873465538025\n","Accuracy = 0.6988636363636364\n","Epoch: 17 of 20\n","bi=0, loss=0.22866258025169373\n","bi=10, loss=0.031223537400364876\n","bi=20, loss=0.047291260212659836\n","bi=30, loss=0.16385498642921448\n","bi=40, loss=0.03572802618145943\n","Accuracy = 0.6931818181818181\n","Epoch: 18 of 20\n","bi=0, loss=0.2323383092880249\n","bi=10, loss=0.028999067842960358\n","bi=20, loss=0.045903027057647705\n","bi=30, loss=0.19217894971370697\n","bi=40, loss=0.044936370104551315\n","Accuracy = 0.6761363636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.25508251786231995\n","bi=10, loss=0.020487509667873383\n","bi=20, loss=0.039902154356241226\n","bi=30, loss=0.1925327479839325\n","bi=40, loss=0.028438368812203407\n","Accuracy = 0.6761363636363638\n","Epoch: 20 of 20\n","bi=0, loss=0.24036885797977448\n","bi=10, loss=0.046717315912246704\n","bi=20, loss=0.027661070227622986\n","bi=30, loss=0.21979071199893951\n","bi=40, loss=0.027836501598358154\n","Accuracy = 0.6846590909090909\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3467457294464111\n","bi=10, loss=1.0546975135803223\n","bi=20, loss=1.2265822887420654\n","bi=30, loss=1.036460518836975\n","bi=40, loss=0.8519383668899536\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=1.4618241786956787\n","bi=10, loss=0.8825874328613281\n","bi=20, loss=1.0817055702209473\n","bi=30, loss=0.922034740447998\n","bi=40, loss=0.5348025560379028\n","Accuracy = 0.6022727272727273\n","Epoch: 3 of 20\n","bi=0, loss=1.3910883665084839\n","bi=10, loss=1.2094449996948242\n","bi=20, loss=0.8653438091278076\n","bi=30, loss=0.9030334949493408\n","bi=40, loss=0.3840477764606476\n","Accuracy = 0.6448863636363636\n","Epoch: 4 of 20\n","bi=0, loss=1.1917566061019897\n","bi=10, loss=0.9956045746803284\n","bi=20, loss=0.6047157049179077\n","bi=30, loss=0.5710330009460449\n","bi=40, loss=0.4048766493797302\n","Accuracy = 0.7272727272727273\n","Epoch: 5 of 20\n","bi=0, loss=1.768545389175415\n","bi=10, loss=0.856904149055481\n","bi=20, loss=0.8969367742538452\n","bi=30, loss=0.34061336517333984\n","bi=40, loss=0.24572034180164337\n","Accuracy = 0.7329545454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.8593666553497314\n","bi=10, loss=0.5157870650291443\n","bi=20, loss=0.7250555753707886\n","bi=30, loss=0.36975762248039246\n","bi=40, loss=0.19389554858207703\n","Accuracy = 0.6903409090909092\n","Epoch: 7 of 20\n","bi=0, loss=0.9016101360321045\n","bi=10, loss=0.7403320074081421\n","bi=20, loss=0.25378188490867615\n","bi=30, loss=0.2841230034828186\n","bi=40, loss=0.0771632194519043\n","Accuracy = 0.6931818181818182\n","Epoch: 8 of 20\n","bi=0, loss=0.4847869873046875\n","bi=10, loss=0.4117201268672943\n","bi=20, loss=0.31338030099868774\n","bi=30, loss=0.3249360918998718\n","bi=40, loss=0.04429122805595398\n","Accuracy = 0.6931818181818182\n","Epoch: 9 of 20\n","bi=0, loss=0.8053157925605774\n","bi=10, loss=0.4533829987049103\n","bi=20, loss=0.139781653881073\n","bi=30, loss=0.14165851473808289\n","bi=40, loss=0.045836206525564194\n","Accuracy = 0.6732954545454546\n","Epoch: 10 of 20\n","bi=0, loss=0.4195041060447693\n","bi=10, loss=0.2708868980407715\n","bi=20, loss=0.09377380460500717\n","bi=30, loss=0.15610350668430328\n","bi=40, loss=0.026878593489527702\n","Accuracy = 0.6363636363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.31771188974380493\n","bi=10, loss=0.24631303548812866\n","bi=20, loss=0.07779816538095474\n","bi=30, loss=0.2328065037727356\n","bi=40, loss=0.023128395900130272\n","Accuracy = 0.65625\n","Epoch: 12 of 20\n","bi=0, loss=0.2934325933456421\n","bi=10, loss=0.2722336947917938\n","bi=20, loss=0.15172159671783447\n","bi=30, loss=0.12202148139476776\n","bi=40, loss=0.043032851070165634\n","Accuracy = 0.6448863636363636\n","Epoch: 13 of 20\n","bi=0, loss=0.17491188645362854\n","bi=10, loss=0.37106749415397644\n","bi=20, loss=0.29160046577453613\n","bi=30, loss=0.23519600927829742\n","bi=40, loss=0.034040667116642\n","Accuracy = 0.6448863636363636\n","Epoch: 14 of 20\n","bi=0, loss=0.4379284977912903\n","bi=10, loss=0.28029558062553406\n","bi=20, loss=0.06444347649812698\n","bi=30, loss=0.1401161253452301\n","bi=40, loss=0.10873806476593018\n","Accuracy = 0.6392045454545454\n","Epoch: 15 of 20\n","bi=0, loss=0.31138724088668823\n","bi=10, loss=0.2485676109790802\n","bi=20, loss=0.04036632552742958\n","bi=30, loss=0.13234521448612213\n","bi=40, loss=0.06245448812842369\n","Accuracy = 0.6477272727272728\n","Epoch: 16 of 20\n","bi=0, loss=0.23297128081321716\n","bi=10, loss=0.24849526584148407\n","bi=20, loss=0.036015186458826065\n","bi=30, loss=0.09662053734064102\n","bi=40, loss=0.04256041347980499\n","Accuracy = 0.65625\n","Epoch: 17 of 20\n","bi=0, loss=0.1635328084230423\n","bi=10, loss=0.19523318111896515\n","bi=20, loss=0.07971818000078201\n","bi=30, loss=0.09292413294315338\n","bi=40, loss=0.10886367410421371\n","Accuracy = 0.6818181818181819\n","Epoch: 18 of 20\n","bi=0, loss=0.08911068737506866\n","bi=10, loss=0.16310714185237885\n","bi=20, loss=0.01892905868589878\n","bi=30, loss=0.0644603818655014\n","bi=40, loss=0.06416349858045578\n","Accuracy = 0.6761363636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.08849819004535675\n","bi=10, loss=0.10950203239917755\n","bi=20, loss=0.017881114035844803\n","bi=30, loss=0.07256874442100525\n","bi=40, loss=0.006745058577507734\n","Accuracy = 0.6761363636363636\n","Epoch: 20 of 20\n","bi=0, loss=0.09551224857568741\n","bi=10, loss=0.04911533743143082\n","bi=20, loss=0.010612936690449715\n","bi=30, loss=0.11262281984090805\n","bi=40, loss=0.015453209169209003\n","Accuracy = 0.6732954545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3467457294464111\n","bi=10, loss=1.0024638175964355\n","bi=20, loss=1.3342235088348389\n","bi=30, loss=1.1299726963043213\n","bi=40, loss=0.9611611366271973\n","Accuracy = 0.59375\n","Epoch: 2 of 20\n","bi=0, loss=1.4169261455535889\n","bi=10, loss=0.911285400390625\n","bi=20, loss=1.1969859600067139\n","bi=30, loss=0.6708912253379822\n","bi=40, loss=0.6381328701972961\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=1.2958146333694458\n","bi=10, loss=0.6948876976966858\n","bi=20, loss=1.0546889305114746\n","bi=30, loss=0.5065140724182129\n","bi=40, loss=0.9014627933502197\n","Accuracy = 0.6505681818181819\n","Epoch: 4 of 20\n","bi=0, loss=1.1566649675369263\n","bi=10, loss=0.8352550268173218\n","bi=20, loss=0.9135017991065979\n","bi=30, loss=0.3681510090827942\n","bi=40, loss=0.8340209126472473\n","Accuracy = 0.6846590909090909\n","Epoch: 5 of 20\n","bi=0, loss=1.1753681898117065\n","bi=10, loss=1.016847014427185\n","bi=20, loss=0.7990443706512451\n","bi=30, loss=0.44351378083229065\n","bi=40, loss=0.3786706030368805\n","Accuracy = 0.6448863636363635\n","Epoch: 6 of 20\n","bi=0, loss=1.1696271896362305\n","bi=10, loss=0.8047192096710205\n","bi=20, loss=0.7981271743774414\n","bi=30, loss=0.8186180591583252\n","bi=40, loss=0.33460313081741333\n","Accuracy = 0.6647727272727273\n","Epoch: 7 of 20\n","bi=0, loss=1.214981198310852\n","bi=10, loss=0.5152756571769714\n","bi=20, loss=0.9427741765975952\n","bi=30, loss=0.33663448691368103\n","bi=40, loss=0.17375169694423676\n","Accuracy = 0.6761363636363636\n","Epoch: 8 of 20\n","bi=0, loss=0.4496556520462036\n","bi=10, loss=0.4666079580783844\n","bi=20, loss=0.4661029577255249\n","bi=30, loss=0.3263075351715088\n","bi=40, loss=0.19781790673732758\n","Accuracy = 0.6647727272727273\n","Epoch: 9 of 20\n","bi=0, loss=1.0361158847808838\n","bi=10, loss=0.28037407994270325\n","bi=20, loss=0.8185795545578003\n","bi=30, loss=0.2708756625652313\n","bi=40, loss=0.3825586438179016\n","Accuracy = 0.6676136363636362\n","Epoch: 10 of 20\n","bi=0, loss=0.7770020365715027\n","bi=10, loss=0.4228440523147583\n","bi=20, loss=0.5729775428771973\n","bi=30, loss=0.18435508012771606\n","bi=40, loss=0.06264729052782059\n","Accuracy = 0.6335227272727273\n","Epoch: 11 of 20\n","bi=0, loss=1.0607246160507202\n","bi=10, loss=1.1544395685195923\n","bi=20, loss=0.967516303062439\n","bi=30, loss=0.1822909116744995\n","bi=40, loss=0.06859250366687775\n","Accuracy = 0.6789772727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.3458119034767151\n","bi=10, loss=0.8208245635032654\n","bi=20, loss=0.08202683925628662\n","bi=30, loss=0.16253270208835602\n","bi=40, loss=0.0342235341668129\n","Accuracy = 0.7045454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.2822943329811096\n","bi=10, loss=0.29670822620391846\n","bi=20, loss=0.07741370797157288\n","bi=30, loss=0.19592298567295074\n","bi=40, loss=0.05510035157203674\n","Accuracy = 0.65625\n","Epoch: 14 of 20\n","bi=0, loss=0.27911749482154846\n","bi=10, loss=0.32196271419525146\n","bi=20, loss=0.051311396062374115\n","bi=30, loss=0.16665495932102203\n","bi=40, loss=0.03658266365528107\n","Accuracy = 0.6590909090909092\n","Epoch: 15 of 20\n","bi=0, loss=0.2819797396659851\n","bi=10, loss=0.29069024324417114\n","bi=20, loss=0.03716285526752472\n","bi=30, loss=0.15424491465091705\n","bi=40, loss=0.1189330592751503\n","Accuracy = 0.6221590909090909\n","Epoch: 16 of 20\n","bi=0, loss=0.531003475189209\n","bi=10, loss=0.3600810170173645\n","bi=20, loss=0.032375939190387726\n","bi=30, loss=0.14439429342746735\n","bi=40, loss=0.052833616733551025\n","Accuracy = 0.6306818181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.26960206031799316\n","bi=10, loss=0.2832605838775635\n","bi=20, loss=0.03760597109794617\n","bi=30, loss=0.1717509925365448\n","bi=40, loss=0.04784989356994629\n","Accuracy = 0.6306818181818182\n","Epoch: 18 of 20\n","bi=0, loss=0.23674607276916504\n","bi=10, loss=0.2900118827819824\n","bi=20, loss=0.12001509964466095\n","bi=30, loss=0.150358647108078\n","bi=40, loss=0.04564990475773811\n","Accuracy = 0.6363636363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.230242058634758\n","bi=10, loss=0.2573845684528351\n","bi=20, loss=0.04085034132003784\n","bi=30, loss=0.23885689675807953\n","bi=40, loss=0.019329695031046867\n","Accuracy = 0.6676136363636364\n","Epoch: 20 of 20\n","bi=0, loss=0.24493545293807983\n","bi=10, loss=0.21812522411346436\n","bi=20, loss=0.0324566625058651\n","bi=30, loss=0.24665267765522003\n","bi=40, loss=0.03412311151623726\n","Accuracy = 0.6676136363636364\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 8, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 974, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3467457294464111\n","bi=10, loss=1.1499733924865723\n","bi=20, loss=1.2314256429672241\n","bi=30, loss=0.7726423740386963\n","bi=40, loss=0.5211082100868225\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=1.6032620668411255\n","bi=10, loss=0.994591474533081\n","bi=20, loss=0.9904618859291077\n","bi=30, loss=0.8144344687461853\n","bi=40, loss=0.986304521560669\n","Accuracy = 0.5880681818181819\n","Epoch: 3 of 20\n","bi=0, loss=1.3312944173812866\n","bi=10, loss=0.7082988023757935\n","bi=20, loss=0.9752400517463684\n","bi=30, loss=0.8728609681129456\n","bi=40, loss=0.1512971967458725\n","Accuracy = 0.6988636363636364\n","Epoch: 4 of 20\n","bi=0, loss=1.0610270500183105\n","bi=10, loss=0.8955506682395935\n","bi=20, loss=1.1311848163604736\n","bi=30, loss=0.7828003168106079\n","bi=40, loss=0.5163435935974121\n","Accuracy = 0.6392045454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.8158254623413086\n","bi=10, loss=0.7842723727226257\n","bi=20, loss=1.422971248626709\n","bi=30, loss=0.49612319469451904\n","bi=40, loss=0.5582051873207092\n","Accuracy = 0.7045454545454546\n","Epoch: 6 of 20\n","bi=0, loss=1.3157949447631836\n","bi=10, loss=0.6929066181182861\n","bi=20, loss=1.5025207996368408\n","bi=30, loss=0.7002687454223633\n","bi=40, loss=0.5368366241455078\n","Accuracy = 0.6619318181818181\n","Epoch: 7 of 20\n","bi=0, loss=0.8784921169281006\n","bi=10, loss=0.9833307266235352\n","bi=20, loss=1.2279692888259888\n","bi=30, loss=0.6290495991706848\n","bi=40, loss=0.5178110599517822\n","Accuracy = 0.6505681818181819\n","Epoch: 8 of 20\n","bi=0, loss=1.0129051208496094\n","bi=10, loss=1.088085412979126\n","bi=20, loss=1.1026490926742554\n","bi=30, loss=0.6657533645629883\n","bi=40, loss=0.6143062710762024\n","Accuracy = 0.6732954545454546\n","Epoch: 9 of 20\n","bi=0, loss=1.40459144115448\n","bi=10, loss=1.1360454559326172\n","bi=20, loss=1.2214828729629517\n","bi=30, loss=0.7117292881011963\n","bi=40, loss=0.6677533388137817\n","Accuracy = 0.6732954545454546\n","Epoch: 10 of 20\n","bi=0, loss=1.5184547901153564\n","bi=10, loss=1.1521037817001343\n","bi=20, loss=1.137714147567749\n","bi=30, loss=0.6546759605407715\n","bi=40, loss=0.6555291414260864\n","Accuracy = 0.6732954545454546\n","Epoch: 11 of 20\n","bi=0, loss=1.4364510774612427\n","bi=10, loss=1.1206682920455933\n","bi=20, loss=1.164056658744812\n","bi=30, loss=0.684378981590271\n","bi=40, loss=0.7507870197296143\n","Accuracy = 0.6732954545454546\n","Epoch: 12 of 20\n","bi=0, loss=1.4461207389831543\n","bi=10, loss=1.1856248378753662\n","bi=20, loss=1.2125065326690674\n","bi=30, loss=0.6963253617286682\n","bi=40, loss=0.6532747745513916\n","Accuracy = 0.6732954545454546\n","Epoch: 13 of 20\n","bi=0, loss=1.396422028541565\n","bi=10, loss=1.1374964714050293\n","bi=20, loss=1.2248890399932861\n","bi=30, loss=0.7006389498710632\n","bi=40, loss=0.7024606466293335\n","Accuracy = 0.6732954545454546\n","Epoch: 14 of 20\n","bi=0, loss=1.3567293882369995\n","bi=10, loss=1.1236681938171387\n","bi=20, loss=1.1311759948730469\n","bi=30, loss=0.651448130607605\n","bi=40, loss=0.6636806726455688\n","Accuracy = 0.6732954545454546\n","Epoch: 15 of 20\n","bi=0, loss=1.4266433715820312\n","bi=10, loss=1.1549369096755981\n","bi=20, loss=1.2291508913040161\n","bi=30, loss=0.7374991774559021\n","bi=40, loss=0.6816132664680481\n","Accuracy = 0.6732954545454546\n","Epoch: 16 of 20\n","bi=0, loss=1.352175235748291\n","bi=10, loss=1.1060985326766968\n","bi=20, loss=1.2143564224243164\n","bi=30, loss=0.7041826844215393\n","bi=40, loss=0.6882181763648987\n","Accuracy = 0.6732954545454546\n","Epoch: 17 of 20\n","bi=0, loss=1.3643832206726074\n","bi=10, loss=1.1599972248077393\n","bi=20, loss=1.2181880474090576\n","bi=30, loss=0.6228264570236206\n","bi=40, loss=0.7085565328598022\n","Accuracy = 0.6732954545454546\n","Epoch: 18 of 20\n","bi=0, loss=1.3830246925354004\n","bi=10, loss=1.1868704557418823\n","bi=20, loss=1.275373935699463\n","bi=30, loss=0.6548023223876953\n","bi=40, loss=0.6693468689918518\n","Accuracy = 0.6732954545454546\n","Epoch: 19 of 20\n","bi=0, loss=1.292431116104126\n","bi=10, loss=1.1098954677581787\n","bi=20, loss=1.2378352880477905\n","bi=30, loss=0.6724212169647217\n","bi=40, loss=0.6640067100524902\n","Accuracy = 0.6732954545454546\n","Epoch: 20 of 20\n","bi=0, loss=1.3999810218811035\n","bi=10, loss=1.1361560821533203\n","bi=20, loss=1.2372796535491943\n","bi=30, loss=0.6870557069778442\n","bi=40, loss=0.673836886882782\n","Accuracy = 0.6732954545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 1\n","num_train_steps = 486, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3388471603393555\n","bi=10, loss=1.0056443214416504\n","bi=20, loss=0.8934450149536133\n","Accuracy = 0.6420454545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.7330417037010193\n","bi=10, loss=0.7460638284683228\n","bi=20, loss=0.6240297555923462\n","Accuracy = 0.6619318181818181\n","Epoch: 3 of 20\n","bi=0, loss=0.5447216629981995\n","bi=10, loss=0.4569920301437378\n","bi=20, loss=0.4082518517971039\n","Accuracy = 0.6846590909090908\n","Epoch: 4 of 20\n","bi=0, loss=0.2903715968132019\n","bi=10, loss=0.4158429205417633\n","bi=20, loss=0.44724658131599426\n","Accuracy = 0.6193181818181819\n","Epoch: 5 of 20\n","bi=0, loss=0.33527466654777527\n","bi=10, loss=0.21587947010993958\n","bi=20, loss=0.1907418966293335\n","Accuracy = 0.5340909090909092\n","Epoch: 6 of 20\n","bi=0, loss=0.4535801112651825\n","bi=10, loss=0.3393562436103821\n","bi=20, loss=0.22462980449199677\n","Accuracy = 0.6676136363636365\n","Epoch: 7 of 20\n","bi=0, loss=0.2717542052268982\n","bi=10, loss=0.07337398827075958\n","bi=20, loss=0.10384252667427063\n","Accuracy = 0.6931818181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.2766175866127014\n","bi=10, loss=0.035922639071941376\n","bi=20, loss=0.06441429257392883\n","Accuracy = 0.5880681818181819\n","Epoch: 9 of 20\n","bi=0, loss=0.03329316899180412\n","bi=10, loss=0.2099035531282425\n","bi=20, loss=0.08139238506555557\n","Accuracy = 0.6420454545454546\n","Epoch: 10 of 20\n","bi=0, loss=0.33334773778915405\n","bi=10, loss=0.0454089492559433\n","bi=20, loss=0.08695656061172485\n","Accuracy = 0.6590909090909092\n","Epoch: 11 of 20\n","bi=0, loss=0.05422079935669899\n","bi=10, loss=0.04783046618103981\n","bi=20, loss=0.06725949794054031\n","Accuracy = 0.6761363636363636\n","Epoch: 12 of 20\n","bi=0, loss=0.02113501913845539\n","bi=10, loss=0.011596787720918655\n","bi=20, loss=0.13953596353530884\n","Accuracy = 0.65625\n","Epoch: 13 of 20\n","bi=0, loss=0.026227781549096107\n","bi=10, loss=0.01495374459773302\n","bi=20, loss=0.0498807430267334\n","Accuracy = 0.6477272727272727\n","Epoch: 14 of 20\n","bi=0, loss=0.018261773511767387\n","bi=10, loss=0.02620868757367134\n","bi=20, loss=0.04739237204194069\n","Accuracy = 0.65625\n","Epoch: 15 of 20\n","bi=0, loss=0.016260338947176933\n","bi=10, loss=0.016634691506624222\n","bi=20, loss=0.028347985818982124\n","Accuracy = 0.6136363636363635\n","Epoch: 16 of 20\n","bi=0, loss=0.014573698863387108\n","bi=10, loss=0.01668578013777733\n","bi=20, loss=0.038100045174360275\n","Accuracy = 0.6732954545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.031093457713723183\n","bi=10, loss=0.017322812229394913\n","bi=20, loss=0.04105482995510101\n","Accuracy = 0.6818181818181819\n","Epoch: 18 of 20\n","bi=0, loss=0.008540015667676926\n","bi=10, loss=0.007897820323705673\n","bi=20, loss=0.02770403027534485\n","Accuracy = 0.6761363636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.005841005593538284\n","bi=10, loss=0.005742174573242664\n","bi=20, loss=0.011572493240237236\n","Accuracy = 0.6732954545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.0063111744821071625\n","bi=10, loss=0.0060715158469974995\n","bi=20, loss=0.011243837885558605\n","Accuracy = 0.6704545454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 2\n","num_train_steps = 486, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.322965145111084\n","bi=10, loss=0.9870872497558594\n","bi=20, loss=0.8533228635787964\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.72542405128479\n","bi=10, loss=0.8975247144699097\n","bi=20, loss=0.578625500202179\n","Accuracy = 0.6818181818181819\n","Epoch: 3 of 20\n","bi=0, loss=0.5054355263710022\n","bi=10, loss=0.5446410179138184\n","bi=20, loss=0.43052613735198975\n","Accuracy = 0.7017045454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.3085276782512665\n","bi=10, loss=0.3906297981739044\n","bi=20, loss=0.18648384511470795\n","Accuracy = 0.7130681818181818\n","Epoch: 5 of 20\n","bi=0, loss=0.31132301688194275\n","bi=10, loss=0.3444654643535614\n","bi=20, loss=0.1904502511024475\n","Accuracy = 0.6590909090909091\n","Epoch: 6 of 20\n","bi=0, loss=0.2777605652809143\n","bi=10, loss=0.1196090504527092\n","bi=20, loss=0.23743633925914764\n","Accuracy = 0.65625\n","Epoch: 7 of 20\n","bi=0, loss=0.19261489808559418\n","bi=10, loss=0.17613565921783447\n","bi=20, loss=0.30651265382766724\n","Accuracy = 0.6903409090909091\n","Epoch: 8 of 20\n","bi=0, loss=0.2801162004470825\n","bi=10, loss=0.23123104870319366\n","bi=20, loss=0.11625640839338303\n","Accuracy = 0.6960227272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.3010968863964081\n","bi=10, loss=0.1354590505361557\n","bi=20, loss=0.07932549715042114\n","Accuracy = 0.6363636363636364\n","Epoch: 10 of 20\n","bi=0, loss=0.267184853553772\n","bi=10, loss=0.09779997915029526\n","bi=20, loss=0.05709356069564819\n","Accuracy = 0.6903409090909092\n","Epoch: 11 of 20\n","bi=0, loss=0.034076251089572906\n","bi=10, loss=0.02246183156967163\n","bi=20, loss=0.05573757737874985\n","Accuracy = 0.6846590909090908\n","Epoch: 12 of 20\n","bi=0, loss=0.02246752381324768\n","bi=10, loss=0.015958962962031364\n","bi=20, loss=0.045868463814258575\n","Accuracy = 0.7045454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.015245778486132622\n","bi=10, loss=0.018352223560214043\n","bi=20, loss=0.07014003396034241\n","Accuracy = 0.6903409090909091\n","Epoch: 14 of 20\n","bi=0, loss=0.010715785436332226\n","bi=10, loss=0.01610727049410343\n","bi=20, loss=0.1338396966457367\n","Accuracy = 0.7017045454545454\n","Epoch: 15 of 20\n","bi=0, loss=0.01361753698438406\n","bi=10, loss=0.020024912431836128\n","bi=20, loss=0.11831047385931015\n","Accuracy = 0.7017045454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.016048390418291092\n","bi=10, loss=0.009715097025036812\n","bi=20, loss=0.0400068536400795\n","Accuracy = 0.6761363636363635\n","Epoch: 17 of 20\n","bi=0, loss=0.02112574502825737\n","bi=10, loss=0.014933296479284763\n","bi=20, loss=0.04643034189939499\n","Accuracy = 0.6846590909090908\n","Epoch: 18 of 20\n","bi=0, loss=0.015010957606136799\n","bi=10, loss=0.016265852376818657\n","bi=20, loss=0.04550136253237724\n","Accuracy = 0.6789772727272727\n","Epoch: 19 of 20\n","bi=0, loss=0.009353959001600742\n","bi=10, loss=0.010151523165404797\n","bi=20, loss=0.05298227071762085\n","Accuracy = 0.6789772727272727\n","Epoch: 20 of 20\n","bi=0, loss=0.010340373031795025\n","bi=10, loss=0.010165927931666374\n","bi=20, loss=0.038638580590486526\n","Accuracy = 0.6846590909090908\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 3\n","num_train_steps = 486, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.369779109954834\n","bi=10, loss=0.9121863842010498\n","bi=20, loss=1.009812355041504\n","Accuracy = 0.6732954545454545\n","Epoch: 2 of 20\n","bi=0, loss=0.6941409707069397\n","bi=10, loss=0.6394616961479187\n","bi=20, loss=0.7438728213310242\n","Accuracy = 0.6392045454545454\n","Epoch: 3 of 20\n","bi=0, loss=0.41013672947883606\n","bi=10, loss=0.4600600302219391\n","bi=20, loss=0.5776227116584778\n","Accuracy = 0.7073863636363636\n","Epoch: 4 of 20\n","bi=0, loss=0.2973105013370514\n","bi=10, loss=0.37348026037216187\n","bi=20, loss=0.4240911602973938\n","Accuracy = 0.7244318181818181\n","Epoch: 5 of 20\n","bi=0, loss=0.09441109001636505\n","bi=10, loss=0.33143070340156555\n","bi=20, loss=0.37246957421302795\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.11537978053092957\n","bi=10, loss=0.23422004282474518\n","bi=20, loss=0.31715095043182373\n","Accuracy = 0.6278409090909092\n","Epoch: 7 of 20\n","bi=0, loss=0.12193602323532104\n","bi=10, loss=0.23432528972625732\n","bi=20, loss=0.3077896237373352\n","Accuracy = 0.6278409090909092\n","Epoch: 8 of 20\n","bi=0, loss=0.142787367105484\n","bi=10, loss=0.5829310417175293\n","bi=20, loss=0.20389774441719055\n","Accuracy = 0.6534090909090908\n","Epoch: 9 of 20\n","bi=0, loss=0.13470372557640076\n","bi=10, loss=0.15855088829994202\n","bi=20, loss=0.19910366833209991\n","Accuracy = 0.6079545454545454\n","Epoch: 10 of 20\n","bi=0, loss=0.0534246489405632\n","bi=10, loss=0.4889500141143799\n","bi=20, loss=0.1525510996580124\n","Accuracy = 0.7017045454545454\n","Epoch: 11 of 20\n","bi=0, loss=0.18552306294441223\n","bi=10, loss=0.12183640152215958\n","bi=20, loss=0.1593645215034485\n","Accuracy = 0.6448863636363636\n","Epoch: 12 of 20\n","bi=0, loss=0.018849633634090424\n","bi=10, loss=0.04247600585222244\n","bi=20, loss=0.07116137444972992\n","Accuracy = 0.6363636363636365\n","Epoch: 13 of 20\n","bi=0, loss=0.009566267021000385\n","bi=10, loss=0.021754641085863113\n","bi=20, loss=0.08314187824726105\n","Accuracy = 0.7073863636363636\n","Epoch: 14 of 20\n","bi=0, loss=0.012768270447850227\n","bi=10, loss=0.01835695654153824\n","bi=20, loss=0.018563007935881615\n","Accuracy = 0.6590909090909091\n","Epoch: 15 of 20\n","bi=0, loss=0.005101350136101246\n","bi=10, loss=0.01028014812618494\n","bi=20, loss=0.01733621209859848\n","Accuracy = 0.6505681818181818\n","Epoch: 16 of 20\n","bi=0, loss=0.0040210699662566185\n","bi=10, loss=0.013149417005479336\n","bi=20, loss=0.009825553745031357\n","Accuracy = 0.6789772727272727\n","Epoch: 17 of 20\n","bi=0, loss=0.005716463550925255\n","bi=10, loss=0.005730833858251572\n","bi=20, loss=0.0077330381609499454\n","Accuracy = 0.6789772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.0037478525191545486\n","bi=10, loss=0.004771731793880463\n","bi=20, loss=0.007805702276527882\n","Accuracy = 0.6761363636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.003790379036217928\n","bi=10, loss=0.003626166144385934\n","bi=20, loss=0.009901798330247402\n","Accuracy = 0.6761363636363636\n","Epoch: 20 of 20\n","bi=0, loss=0.003304816549643874\n","bi=10, loss=0.005466976668685675\n","bi=20, loss=0.005615127272903919\n","Accuracy = 0.6789772727272727\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 4\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.34818434715271\n","bi=10, loss=0.9895744919776917\n","bi=20, loss=0.9104464054107666\n","Accuracy = 0.6761363636363635\n","Epoch: 2 of 20\n","bi=0, loss=0.5548602342605591\n","bi=10, loss=0.7068485617637634\n","bi=20, loss=0.901405930519104\n","Accuracy = 0.6846590909090908\n","Epoch: 3 of 20\n","bi=0, loss=0.48738014698028564\n","bi=10, loss=0.4867247939109802\n","bi=20, loss=0.4779086709022522\n","Accuracy = 0.7130681818181818\n","Epoch: 4 of 20\n","bi=0, loss=0.4217349588871002\n","bi=10, loss=0.5369612574577332\n","bi=20, loss=0.3408295512199402\n","Accuracy = 0.6193181818181818\n","Epoch: 5 of 20\n","bi=0, loss=0.4108167290687561\n","bi=10, loss=0.47104412317276\n","bi=20, loss=0.26741155982017517\n","Accuracy = 0.7073863636363635\n","Epoch: 6 of 20\n","bi=0, loss=0.1316189020872116\n","bi=10, loss=0.298242449760437\n","bi=20, loss=0.21994253993034363\n","Accuracy = 0.7102272727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.08474522083997726\n","bi=10, loss=0.2760309875011444\n","bi=20, loss=0.5654150247573853\n","Accuracy = 0.5852272727272727\n","Epoch: 8 of 20\n","bi=0, loss=0.15251033008098602\n","bi=10, loss=0.25193801522254944\n","bi=20, loss=0.4177720248699188\n","Accuracy = 0.6704545454545455\n","Epoch: 9 of 20\n","bi=0, loss=0.10348047316074371\n","bi=10, loss=0.2369510978460312\n","bi=20, loss=0.3344942629337311\n","Accuracy = 0.7159090909090908\n","Epoch: 10 of 20\n","bi=0, loss=0.07600986212491989\n","bi=10, loss=0.3151920437812805\n","bi=20, loss=0.13357636332511902\n","Accuracy = 0.6818181818181819\n","Epoch: 11 of 20\n","bi=0, loss=0.07082681357860565\n","bi=10, loss=0.08959367871284485\n","bi=20, loss=0.271931529045105\n","Accuracy = 0.6818181818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.1537245661020279\n","bi=10, loss=0.05521102249622345\n","bi=20, loss=0.025378035381436348\n","Accuracy = 0.7244318181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.019192039966583252\n","bi=10, loss=0.04407551884651184\n","bi=20, loss=0.05834834277629852\n","Accuracy = 0.6988636363636364\n","Epoch: 14 of 20\n","bi=0, loss=0.017387451604008675\n","bi=10, loss=0.06678304821252823\n","bi=20, loss=0.024603331461548805\n","Accuracy = 0.7102272727272727\n","Epoch: 15 of 20\n","bi=0, loss=0.010808081366121769\n","bi=10, loss=0.053595200181007385\n","bi=20, loss=0.03975295275449753\n","Accuracy = 0.6875\n","Epoch: 16 of 20\n","bi=0, loss=0.01992296800017357\n","bi=10, loss=0.038847941905260086\n","bi=20, loss=0.06446491181850433\n","Accuracy = 0.7017045454545454\n","Epoch: 17 of 20\n","bi=0, loss=0.024813687428832054\n","bi=10, loss=0.03543798252940178\n","bi=20, loss=0.054266560822725296\n","Accuracy = 0.7045454545454545\n","Epoch: 18 of 20\n","bi=0, loss=0.01710103452205658\n","bi=10, loss=0.03332594782114029\n","bi=20, loss=0.026782337576150894\n","Accuracy = 0.6960227272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.024089861661195755\n","bi=10, loss=0.041406892240047455\n","bi=20, loss=0.04222770407795906\n","Accuracy = 0.6988636363636365\n","Epoch: 20 of 20\n","bi=0, loss=0.008198811672627926\n","bi=10, loss=0.06618129462003708\n","bi=20, loss=0.06802176684141159\n","Accuracy = 0.7045454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3490384817123413\n","bi=10, loss=0.8889375329017639\n","bi=20, loss=0.7969300150871277\n","Accuracy = 0.6619318181818181\n","Epoch: 2 of 20\n","bi=0, loss=0.788642406463623\n","bi=10, loss=0.8826988339424133\n","bi=20, loss=0.7699248790740967\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.7128924131393433\n","bi=10, loss=0.5895070433616638\n","bi=20, loss=1.0297046899795532\n","Accuracy = 0.6051136363636364\n","Epoch: 4 of 20\n","bi=0, loss=0.6310955286026001\n","bi=10, loss=0.513318657875061\n","bi=20, loss=0.36860308051109314\n","Accuracy = 0.6903409090909092\n","Epoch: 5 of 20\n","bi=0, loss=0.521050214767456\n","bi=10, loss=0.34775876998901367\n","bi=20, loss=0.4008675813674927\n","Accuracy = 0.6818181818181818\n","Epoch: 6 of 20\n","bi=0, loss=0.27158012986183167\n","bi=10, loss=0.7284806370735168\n","bi=20, loss=0.1806170493364334\n","Accuracy = 0.6846590909090908\n","Epoch: 7 of 20\n","bi=0, loss=0.3722877502441406\n","bi=10, loss=0.7878137230873108\n","bi=20, loss=0.4083178639411926\n","Accuracy = 0.6534090909090908\n","Epoch: 8 of 20\n","bi=0, loss=0.31140008568763733\n","bi=10, loss=0.5112089514732361\n","bi=20, loss=0.5980828404426575\n","Accuracy = 0.6534090909090909\n","Epoch: 9 of 20\n","bi=0, loss=0.3169479966163635\n","bi=10, loss=0.9306543469429016\n","bi=20, loss=0.3937705457210541\n","Accuracy = 0.6960227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.328008234500885\n","bi=10, loss=0.38059812784194946\n","bi=20, loss=0.21527855098247528\n","Accuracy = 0.6960227272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.22373990714550018\n","bi=10, loss=0.059680867940187454\n","bi=20, loss=0.13055066764354706\n","Accuracy = 0.6789772727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.1719098687171936\n","bi=10, loss=0.16223140060901642\n","bi=20, loss=0.09210073202848434\n","Accuracy = 0.6789772727272727\n","Epoch: 13 of 20\n","bi=0, loss=0.12681597471237183\n","bi=10, loss=0.09183194488286972\n","bi=20, loss=0.1589319407939911\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.15370208024978638\n","bi=10, loss=0.034499164670705795\n","bi=20, loss=0.060792434960603714\n","Accuracy = 0.6988636363636364\n","Epoch: 15 of 20\n","bi=0, loss=0.15742334723472595\n","bi=10, loss=0.05237370356917381\n","bi=20, loss=0.031143702566623688\n","Accuracy = 0.6732954545454546\n","Epoch: 16 of 20\n","bi=0, loss=0.1315503716468811\n","bi=10, loss=0.02640215866267681\n","bi=20, loss=0.045406900346279144\n","Accuracy = 0.6818181818181818\n","Epoch: 17 of 20\n","bi=0, loss=0.1581690013408661\n","bi=10, loss=0.029215514659881592\n","bi=20, loss=0.043162327259778976\n","Accuracy = 0.6846590909090908\n","Epoch: 18 of 20\n","bi=0, loss=0.12562432885169983\n","bi=10, loss=0.02620748244225979\n","bi=20, loss=0.04158138483762741\n","Accuracy = 0.6875\n","Epoch: 19 of 20\n","bi=0, loss=0.12480443716049194\n","bi=10, loss=0.018459653481841087\n","bi=20, loss=0.06270883977413177\n","Accuracy = 0.6846590909090909\n","Epoch: 20 of 20\n","bi=0, loss=0.1364077627658844\n","bi=10, loss=0.02493700012564659\n","bi=20, loss=0.041537798941135406\n","Accuracy = 0.6846590909090909\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3210259675979614\n","bi=10, loss=0.9132343530654907\n","bi=20, loss=0.9511255025863647\n","Accuracy = 0.7102272727272727\n","Epoch: 2 of 20\n","bi=0, loss=0.863499104976654\n","bi=10, loss=0.6794517040252686\n","bi=20, loss=0.8179873824119568\n","Accuracy = 0.6988636363636364\n","Epoch: 3 of 20\n","bi=0, loss=0.7357174754142761\n","bi=10, loss=0.49382472038269043\n","bi=20, loss=0.6197110414505005\n","Accuracy = 0.7159090909090909\n","Epoch: 4 of 20\n","bi=0, loss=0.4212726354598999\n","bi=10, loss=0.28728049993515015\n","bi=20, loss=0.9415839910507202\n","Accuracy = 0.6761363636363635\n","Epoch: 5 of 20\n","bi=0, loss=0.4864526093006134\n","bi=10, loss=0.2483157515525818\n","bi=20, loss=0.38130849599838257\n","Accuracy = 0.7244318181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.3960336148738861\n","bi=10, loss=0.1000969186425209\n","bi=20, loss=0.10691405832767487\n","Accuracy = 0.6761363636363636\n","Epoch: 7 of 20\n","bi=0, loss=0.35860785841941833\n","bi=10, loss=0.5412402749061584\n","bi=20, loss=0.09529334306716919\n","Accuracy = 0.7102272727272727\n","Epoch: 8 of 20\n","bi=0, loss=0.1611298769712448\n","bi=10, loss=0.13565705716609955\n","bi=20, loss=0.2537219226360321\n","Accuracy = 0.6846590909090909\n","Epoch: 9 of 20\n","bi=0, loss=0.1439606100320816\n","bi=10, loss=0.08835870027542114\n","bi=20, loss=0.16411498188972473\n","Accuracy = 0.5369318181818182\n","Epoch: 10 of 20\n","bi=0, loss=0.30649539828300476\n","bi=10, loss=0.16652826964855194\n","bi=20, loss=0.2616366744041443\n","Accuracy = 0.6676136363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.15309078991413116\n","bi=10, loss=0.07825936377048492\n","bi=20, loss=0.05148006230592728\n","Accuracy = 0.6704545454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.15522389113903046\n","bi=10, loss=0.03527868166565895\n","bi=20, loss=0.09664913266897202\n","Accuracy = 0.6732954545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.12910893559455872\n","bi=10, loss=0.019262997433543205\n","bi=20, loss=0.05890610069036484\n","Accuracy = 0.6477272727272727\n","Epoch: 14 of 20\n","bi=0, loss=0.14069068431854248\n","bi=10, loss=0.019008805975317955\n","bi=20, loss=0.054584600031375885\n","Accuracy = 0.6505681818181818\n","Epoch: 15 of 20\n","bi=0, loss=0.12692862749099731\n","bi=10, loss=0.02052583172917366\n","bi=20, loss=0.0503457710146904\n","Accuracy = 0.6676136363636365\n","Epoch: 16 of 20\n","bi=0, loss=0.12955516576766968\n","bi=10, loss=0.01721893809735775\n","bi=20, loss=0.07882949709892273\n","Accuracy = 0.6676136363636365\n","Epoch: 17 of 20\n","bi=0, loss=0.1360750049352646\n","bi=10, loss=0.024267273023724556\n","bi=20, loss=0.03136366233229637\n","Accuracy = 0.6647727272727273\n","Epoch: 18 of 20\n","bi=0, loss=0.12778620421886444\n","bi=10, loss=0.016522517427802086\n","bi=20, loss=0.05201326683163643\n","Accuracy = 0.6647727272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.11217696964740753\n","bi=10, loss=0.01297960989177227\n","bi=20, loss=0.05018436536192894\n","Accuracy = 0.6647727272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.10573501884937286\n","bi=10, loss=0.021816782653331757\n","bi=20, loss=0.04554013907909393\n","Accuracy = 0.6647727272727273\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.329298496246338\n","bi=10, loss=1.0843418836593628\n","bi=20, loss=1.0923138856887817\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.8907253742218018\n","bi=10, loss=1.0999765396118164\n","bi=20, loss=0.9056966304779053\n","Accuracy = 0.6590909090909092\n","Epoch: 3 of 20\n","bi=0, loss=1.0047836303710938\n","bi=10, loss=0.6869260668754578\n","bi=20, loss=0.8408230543136597\n","Accuracy = 0.6988636363636365\n","Epoch: 4 of 20\n","bi=0, loss=0.7524340748786926\n","bi=10, loss=0.6952641010284424\n","bi=20, loss=0.8110067844390869\n","Accuracy = 0.7017045454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.6247466802597046\n","bi=10, loss=0.5873315334320068\n","bi=20, loss=0.4756993055343628\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.1713685244321823\n","bi=10, loss=0.46544820070266724\n","bi=20, loss=0.3505252003669739\n","Accuracy = 0.6676136363636364\n","Epoch: 7 of 20\n","bi=0, loss=0.21333520114421844\n","bi=10, loss=0.47050416469573975\n","bi=20, loss=0.21366176009178162\n","Accuracy = 0.6676136363636364\n","Epoch: 8 of 20\n","bi=0, loss=0.28530123829841614\n","bi=10, loss=0.2386433631181717\n","bi=20, loss=0.10308961570262909\n","Accuracy = 0.7045454545454546\n","Epoch: 9 of 20\n","bi=0, loss=0.5665692687034607\n","bi=10, loss=0.21923252940177917\n","bi=20, loss=0.20371590554714203\n","Accuracy = 0.7357954545454546\n","Epoch: 10 of 20\n","bi=0, loss=0.4442792236804962\n","bi=10, loss=0.27999556064605713\n","bi=20, loss=0.1622493416070938\n","Accuracy = 0.7414772727272727\n","Epoch: 11 of 20\n","bi=0, loss=0.4387839734554291\n","bi=10, loss=0.1484808325767517\n","bi=20, loss=0.3934265077114105\n","Accuracy = 0.6704545454545455\n","Epoch: 12 of 20\n","bi=0, loss=0.13357096910476685\n","bi=10, loss=0.10891613364219666\n","bi=20, loss=0.04141946882009506\n","Accuracy = 0.6931818181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.1404150277376175\n","bi=10, loss=0.12947429716587067\n","bi=20, loss=0.045504938811063766\n","Accuracy = 0.7073863636363636\n","Epoch: 14 of 20\n","bi=0, loss=0.14878009259700775\n","bi=10, loss=0.1474272608757019\n","bi=20, loss=0.05427326261997223\n","Accuracy = 0.7471590909090909\n","Epoch: 15 of 20\n","bi=0, loss=0.14288832247257233\n","bi=10, loss=0.1100715696811676\n","bi=20, loss=0.09718628972768784\n","Accuracy = 0.7102272727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.13880673050880432\n","bi=10, loss=0.09975523501634598\n","bi=20, loss=0.05483885109424591\n","Accuracy = 0.7045454545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.14875459671020508\n","bi=10, loss=0.11918400228023529\n","bi=20, loss=0.07099952548742294\n","Accuracy = 0.6931818181818181\n","Epoch: 18 of 20\n","bi=0, loss=0.1151932030916214\n","bi=10, loss=0.10969992727041245\n","bi=20, loss=0.05382262170314789\n","Accuracy = 0.6903409090909092\n","Epoch: 19 of 20\n","bi=0, loss=0.11944232881069183\n","bi=10, loss=0.15777255594730377\n","bi=20, loss=0.04717708379030228\n","Accuracy = 0.7130681818181819\n","Epoch: 20 of 20\n","bi=0, loss=0.1308833360671997\n","bi=10, loss=0.10669731348752975\n","bi=20, loss=0.05088992789387703\n","Accuracy = 0.71875\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3283307552337646\n","bi=10, loss=0.9628311991691589\n","bi=20, loss=1.039006233215332\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.9013260006904602\n","bi=10, loss=0.820401668548584\n","bi=20, loss=0.7604700922966003\n","Accuracy = 0.71875\n","Epoch: 3 of 20\n","bi=0, loss=0.776731550693512\n","bi=10, loss=0.7324600219726562\n","bi=20, loss=0.6298893690109253\n","Accuracy = 0.7244318181818181\n","Epoch: 4 of 20\n","bi=0, loss=0.8602542281150818\n","bi=10, loss=0.308330774307251\n","bi=20, loss=1.3520972728729248\n","Accuracy = 0.6392045454545454\n","Epoch: 5 of 20\n","bi=0, loss=1.3661508560180664\n","bi=10, loss=0.4705578088760376\n","bi=20, loss=0.22587604820728302\n","Accuracy = 0.7244318181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.35049763321876526\n","bi=10, loss=0.2775914669036865\n","bi=20, loss=0.32324522733688354\n","Accuracy = 0.6931818181818181\n","Epoch: 7 of 20\n","bi=0, loss=0.14668050408363342\n","bi=10, loss=0.2397039383649826\n","bi=20, loss=0.3866557776927948\n","Accuracy = 0.5426136363636364\n","Epoch: 8 of 20\n","bi=0, loss=0.655297577381134\n","bi=10, loss=0.2011861950159073\n","bi=20, loss=0.13924413919448853\n","Accuracy = 0.6676136363636364\n","Epoch: 9 of 20\n","bi=0, loss=0.5179209113121033\n","bi=10, loss=0.11684450507164001\n","bi=20, loss=0.11294431984424591\n","Accuracy = 0.6988636363636365\n","Epoch: 10 of 20\n","bi=0, loss=0.151250422000885\n","bi=10, loss=0.07729838788509369\n","bi=20, loss=0.0557074099779129\n","Accuracy = 0.6761363636363636\n","Epoch: 11 of 20\n","bi=0, loss=0.153935045003891\n","bi=10, loss=0.04708089679479599\n","bi=20, loss=0.02905607968568802\n","Accuracy = 0.6818181818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.1432100534439087\n","bi=10, loss=0.03095770999789238\n","bi=20, loss=0.11932118982076645\n","Accuracy = 0.6789772727272727\n","Epoch: 13 of 20\n","bi=0, loss=0.10247202962636948\n","bi=10, loss=0.12175361067056656\n","bi=20, loss=0.057399682700634\n","Accuracy = 0.6818181818181819\n","Epoch: 14 of 20\n","bi=0, loss=0.14492468535900116\n","bi=10, loss=0.16421233117580414\n","bi=20, loss=0.030000723898410797\n","Accuracy = 0.6903409090909091\n","Epoch: 15 of 20\n","bi=0, loss=0.13446883857250214\n","bi=10, loss=0.1327892243862152\n","bi=20, loss=0.014428447932004929\n","Accuracy = 0.6789772727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.03327642008662224\n","bi=10, loss=0.05669189989566803\n","bi=20, loss=0.007354128174483776\n","Accuracy = 0.6846590909090909\n","Epoch: 17 of 20\n","bi=0, loss=0.015238679945468903\n","bi=10, loss=0.09448346495628357\n","bi=20, loss=0.009127618744969368\n","Accuracy = 0.6903409090909091\n","Epoch: 18 of 20\n","bi=0, loss=0.024377452209591866\n","bi=10, loss=0.03866194561123848\n","bi=20, loss=0.006348375231027603\n","Accuracy = 0.6903409090909091\n","Epoch: 19 of 20\n","bi=0, loss=0.08067198097705841\n","bi=10, loss=0.1606772243976593\n","bi=20, loss=0.005209553521126509\n","Accuracy = 0.6875\n","Epoch: 20 of 20\n","bi=0, loss=0.01145077496767044\n","bi=10, loss=0.02405397966504097\n","bi=20, loss=0.02042250894010067\n","Accuracy = 0.6903409090909092\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3114113807678223\n","bi=10, loss=1.1590780019760132\n","bi=20, loss=1.1513828039169312\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.8746293187141418\n","bi=10, loss=0.8729990720748901\n","bi=20, loss=0.9048407077789307\n","Accuracy = 0.6903409090909091\n","Epoch: 3 of 20\n","bi=0, loss=0.8056883811950684\n","bi=10, loss=0.45825955271720886\n","bi=20, loss=1.1236692667007446\n","Accuracy = 0.71875\n","Epoch: 4 of 20\n","bi=0, loss=0.4127487242221832\n","bi=10, loss=0.1202278658747673\n","bi=20, loss=0.392626017332077\n","Accuracy = 0.7045454545454546\n","Epoch: 5 of 20\n","bi=0, loss=0.41603562235832214\n","bi=10, loss=0.20520685613155365\n","bi=20, loss=0.2262921929359436\n","Accuracy = 0.6931818181818181\n","Epoch: 6 of 20\n","bi=0, loss=0.18773117661476135\n","bi=10, loss=0.19160355627536774\n","bi=20, loss=0.10147988051176071\n","Accuracy = 0.6761363636363635\n","Epoch: 7 of 20\n","bi=0, loss=0.3509611487388611\n","bi=10, loss=0.12418968230485916\n","bi=20, loss=0.24862118065357208\n","Accuracy = 0.6988636363636364\n","Epoch: 8 of 20\n","bi=0, loss=0.16332201659679413\n","bi=10, loss=0.052022721618413925\n","bi=20, loss=0.05551512911915779\n","Accuracy = 0.6846590909090908\n","Epoch: 9 of 20\n","bi=0, loss=0.28418096899986267\n","bi=10, loss=0.052332550287246704\n","bi=20, loss=0.6260971426963806\n","Accuracy = 0.6477272727272727\n","Epoch: 10 of 20\n","bi=0, loss=0.2722703516483307\n","bi=10, loss=0.16124333441257477\n","bi=20, loss=0.05976439267396927\n","Accuracy = 0.6875\n","Epoch: 11 of 20\n","bi=0, loss=0.14215165376663208\n","bi=10, loss=0.026230379939079285\n","bi=20, loss=0.05891899764537811\n","Accuracy = 0.6704545454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.15067727863788605\n","bi=10, loss=0.028158001601696014\n","bi=20, loss=0.04547114670276642\n","Accuracy = 0.6931818181818182\n","Epoch: 13 of 20\n","bi=0, loss=0.09794635325670242\n","bi=10, loss=0.012163011357188225\n","bi=20, loss=0.048732880502939224\n","Accuracy = 0.7159090909090909\n","Epoch: 14 of 20\n","bi=0, loss=0.04087785631418228\n","bi=10, loss=0.014392520301043987\n","bi=20, loss=0.030797552317380905\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.0811590775847435\n","bi=10, loss=0.017490332946181297\n","bi=20, loss=0.01936064101755619\n","Accuracy = 0.6960227272727273\n","Epoch: 16 of 20\n","bi=0, loss=0.22010129690170288\n","bi=10, loss=0.03139244019985199\n","bi=20, loss=0.012574054300785065\n","Accuracy = 0.6931818181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.17485353350639343\n","bi=10, loss=0.02988198585808277\n","bi=20, loss=0.010596074163913727\n","Accuracy = 0.6931818181818182\n","Epoch: 18 of 20\n","bi=0, loss=0.0821661502122879\n","bi=10, loss=0.0046545350924134254\n","bi=20, loss=0.008893193677067757\n","Accuracy = 0.6903409090909092\n","Epoch: 19 of 20\n","bi=0, loss=0.0307854525744915\n","bi=10, loss=0.002338078571483493\n","bi=20, loss=0.006765591446310282\n","Accuracy = 0.6875\n","Epoch: 20 of 20\n","bi=0, loss=0.008561581373214722\n","bi=10, loss=0.0026283063925802708\n","bi=20, loss=0.007062436081469059\n","Accuracy = 0.6931818181818181\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 16, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 487, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.297513723373413\n","bi=10, loss=1.0448540449142456\n","bi=20, loss=0.9544956088066101\n","Accuracy = 0.6789772727272727\n","Epoch: 2 of 20\n","bi=0, loss=0.8452123403549194\n","bi=10, loss=0.8453118801116943\n","bi=20, loss=0.8225834369659424\n","Accuracy = 0.6761363636363636\n","Epoch: 3 of 20\n","bi=0, loss=0.7629557251930237\n","bi=10, loss=0.8194681406021118\n","bi=20, loss=1.1482727527618408\n","Accuracy = 0.6704545454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.46603429317474365\n","bi=10, loss=0.6258717179298401\n","bi=20, loss=0.4138612151145935\n","Accuracy = 0.6846590909090908\n","Epoch: 5 of 20\n","bi=0, loss=0.6731801629066467\n","bi=10, loss=0.3035867214202881\n","bi=20, loss=0.36680030822753906\n","Accuracy = 0.6420454545454545\n","Epoch: 6 of 20\n","bi=0, loss=0.42832863330841064\n","bi=10, loss=0.21056801080703735\n","bi=20, loss=0.493156373500824\n","Accuracy = 0.5823863636363636\n","Epoch: 7 of 20\n","bi=0, loss=0.5392687916755676\n","bi=10, loss=0.24967293441295624\n","bi=20, loss=0.14384254813194275\n","Accuracy = 0.5795454545454546\n","Epoch: 8 of 20\n","bi=0, loss=0.6306841373443604\n","bi=10, loss=0.5411736369132996\n","bi=20, loss=0.3847675919532776\n","Accuracy = 0.6789772727272727\n","Epoch: 9 of 20\n","bi=0, loss=0.40784451365470886\n","bi=10, loss=0.307439386844635\n","bi=20, loss=0.07296623289585114\n","Accuracy = 0.6846590909090908\n","Epoch: 10 of 20\n","bi=0, loss=0.44160231947898865\n","bi=10, loss=0.03672651946544647\n","bi=20, loss=0.08060891926288605\n","Accuracy = 0.6988636363636365\n","Epoch: 11 of 20\n","bi=0, loss=0.14494085311889648\n","bi=10, loss=0.023276887834072113\n","bi=20, loss=0.042370423674583435\n","Accuracy = 0.6960227272727273\n","Epoch: 12 of 20\n","bi=0, loss=0.12977392971515656\n","bi=10, loss=0.019268661737442017\n","bi=20, loss=0.04328300803899765\n","Accuracy = 0.6647727272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.10898394137620926\n","bi=10, loss=0.020624203607439995\n","bi=20, loss=0.04353151470422745\n","Accuracy = 0.6789772727272727\n","Epoch: 14 of 20\n","bi=0, loss=0.12556886672973633\n","bi=10, loss=0.052351728081703186\n","bi=20, loss=0.026282761245965958\n","Accuracy = 0.6988636363636365\n","Epoch: 15 of 20\n","bi=0, loss=0.12899722158908844\n","bi=10, loss=0.0251710694283247\n","bi=20, loss=0.027349501848220825\n","Accuracy = 0.6761363636363636\n","Epoch: 16 of 20\n","bi=0, loss=0.14117930829524994\n","bi=10, loss=0.1411469727754593\n","bi=20, loss=0.02486470714211464\n","Accuracy = 0.6761363636363636\n","Epoch: 17 of 20\n","bi=0, loss=0.14874430000782013\n","bi=10, loss=0.026178741827607155\n","bi=20, loss=0.022942177951335907\n","Accuracy = 0.6818181818181819\n","Epoch: 18 of 20\n","bi=0, loss=0.1058197170495987\n","bi=10, loss=0.06838971376419067\n","bi=20, loss=0.016179759055376053\n","Accuracy = 0.6903409090909091\n","Epoch: 19 of 20\n","bi=0, loss=0.1133980080485344\n","bi=10, loss=0.012020695954561234\n","bi=20, loss=0.012330979108810425\n","Accuracy = 0.6846590909090908\n","Epoch: 20 of 20\n","bi=0, loss=0.11340174823999405\n","bi=10, loss=0.04254281520843506\n","bi=20, loss=0.012622361071407795\n","Accuracy = 0.6846590909090908\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 1\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3596243858337402\n","bi=10, loss=0.9420981407165527\n","Accuracy = 0.6676136363636364\n","Epoch: 2 of 20\n","bi=0, loss=0.8575310707092285\n","bi=10, loss=0.7545220255851746\n","Accuracy = 0.6676136363636364\n","Epoch: 3 of 20\n","bi=0, loss=0.6928046941757202\n","bi=10, loss=0.5280341506004333\n","Accuracy = 0.6846590909090908\n","Epoch: 4 of 20\n","bi=0, loss=0.5283532738685608\n","bi=10, loss=0.3590255081653595\n","Accuracy = 0.7073863636363636\n","Epoch: 5 of 20\n","bi=0, loss=0.8000708818435669\n","bi=10, loss=0.2145872712135315\n","Accuracy = 0.6221590909090908\n","Epoch: 6 of 20\n","bi=0, loss=0.38550055027008057\n","bi=10, loss=0.7923241257667542\n","Accuracy = 0.6420454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.529439389705658\n","bi=10, loss=0.14875191450119019\n","Accuracy = 0.6392045454545455\n","Epoch: 8 of 20\n","bi=0, loss=0.252968966960907\n","bi=10, loss=0.07927346229553223\n","Accuracy = 0.6448863636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.14349746704101562\n","bi=10, loss=0.4542882442474365\n","Accuracy = 0.6818181818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.1940893679857254\n","bi=10, loss=0.08072896301746368\n","Accuracy = 0.6988636363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.25163814425468445\n","bi=10, loss=0.04122272878885269\n","Accuracy = 0.6619318181818181\n","Epoch: 12 of 20\n","bi=0, loss=0.09711944311857224\n","bi=10, loss=0.46248000860214233\n","Accuracy = 0.6022727272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.12687352299690247\n","bi=10, loss=0.21274995803833008\n","Accuracy = 0.6363636363636364\n","Epoch: 14 of 20\n","bi=0, loss=0.2451469898223877\n","bi=10, loss=0.06512872874736786\n","Accuracy = 0.6590909090909092\n","Epoch: 15 of 20\n","bi=0, loss=0.05776435136795044\n","bi=10, loss=0.022578172385692596\n","Accuracy = 0.6448863636363636\n","Epoch: 16 of 20\n","bi=0, loss=0.06357764452695847\n","bi=10, loss=0.02578340470790863\n","Accuracy = 0.6590909090909092\n","Epoch: 17 of 20\n","bi=0, loss=0.04878876358270645\n","bi=10, loss=0.016708962619304657\n","Accuracy = 0.65625\n","Epoch: 18 of 20\n","bi=0, loss=0.05758379399776459\n","bi=10, loss=0.018525611609220505\n","Accuracy = 0.6420454545454546\n","Epoch: 19 of 20\n","bi=0, loss=0.05160362645983696\n","bi=10, loss=0.018083523958921432\n","Accuracy = 0.6420454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.061138831079006195\n","bi=10, loss=0.019948238506913185\n","Accuracy = 0.6420454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 2\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3532062768936157\n","bi=10, loss=0.8911584615707397\n","Accuracy = 0.6590909090909091\n","Epoch: 2 of 20\n","bi=0, loss=0.9040899872779846\n","bi=10, loss=1.1124417781829834\n","Accuracy = 0.6590909090909091\n","Epoch: 3 of 20\n","bi=0, loss=0.9159730076789856\n","bi=10, loss=0.8832036852836609\n","Accuracy = 0.6590909090909091\n","Epoch: 4 of 20\n","bi=0, loss=0.8815909624099731\n","bi=10, loss=0.8541469573974609\n","Accuracy = 0.6363636363636364\n","Epoch: 5 of 20\n","bi=0, loss=0.7732457518577576\n","bi=10, loss=0.5822061896324158\n","Accuracy = 0.6875\n","Epoch: 6 of 20\n","bi=0, loss=0.6382324695587158\n","bi=10, loss=0.456839919090271\n","Accuracy = 0.7045454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.5548918843269348\n","bi=10, loss=0.21654075384140015\n","Accuracy = 0.6931818181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.4086940288543701\n","bi=10, loss=0.4865349233150482\n","Accuracy = 0.7130681818181819\n","Epoch: 9 of 20\n","bi=0, loss=0.3472307324409485\n","bi=10, loss=0.27877277135849\n","Accuracy = 0.6505681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.2040516436100006\n","bi=10, loss=0.1072118878364563\n","Accuracy = 0.6704545454545454\n","Epoch: 11 of 20\n","bi=0, loss=0.16496209800243378\n","bi=10, loss=0.18363307416439056\n","Accuracy = 0.7102272727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.19783884286880493\n","bi=10, loss=0.1161024421453476\n","Accuracy = 0.7045454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.13561230897903442\n","bi=10, loss=0.03644507750868797\n","Accuracy = 0.7045454545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.10014516115188599\n","bi=10, loss=0.24838650226593018\n","Accuracy = 0.6164772727272727\n","Epoch: 15 of 20\n","bi=0, loss=0.2120797038078308\n","bi=10, loss=0.20199359953403473\n","Accuracy = 0.6676136363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.30291956663131714\n","bi=10, loss=0.11298818141222\n","Accuracy = 0.6732954545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.06841187179088593\n","bi=10, loss=0.04028531536459923\n","Accuracy = 0.6704545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.05648666247725487\n","bi=10, loss=0.03764652460813522\n","Accuracy = 0.6789772727272727\n","Epoch: 19 of 20\n","bi=0, loss=0.06390522420406342\n","bi=10, loss=0.029621977359056473\n","Accuracy = 0.6732954545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.07013961672782898\n","bi=10, loss=0.021891839802265167\n","Accuracy = 0.6761363636363636\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 3\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.363140344619751\n","bi=10, loss=0.8618713021278381\n","Accuracy = 0.6732954545454545\n","Epoch: 2 of 20\n","bi=0, loss=0.8236075639724731\n","bi=10, loss=0.9318360090255737\n","Accuracy = 0.6789772727272727\n","Epoch: 3 of 20\n","bi=0, loss=0.7790856957435608\n","bi=10, loss=0.6997729539871216\n","Accuracy = 0.6448863636363636\n","Epoch: 4 of 20\n","bi=0, loss=0.5838459134101868\n","bi=10, loss=0.6067227721214294\n","Accuracy = 0.7159090909090908\n","Epoch: 5 of 20\n","bi=0, loss=0.4556700587272644\n","bi=10, loss=0.41916534304618835\n","Accuracy = 0.6704545454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.3710227310657501\n","bi=10, loss=0.44110605120658875\n","Accuracy = 0.6136363636363636\n","Epoch: 7 of 20\n","bi=0, loss=0.32680198550224304\n","bi=10, loss=1.0256035327911377\n","Accuracy = 0.7215909090909091\n","Epoch: 8 of 20\n","bi=0, loss=0.5186413526535034\n","bi=10, loss=0.2621096968650818\n","Accuracy = 0.6846590909090909\n","Epoch: 9 of 20\n","bi=0, loss=0.130367249250412\n","bi=10, loss=0.105861596763134\n","Accuracy = 0.6761363636363636\n","Epoch: 10 of 20\n","bi=0, loss=0.16921547055244446\n","bi=10, loss=0.08599293231964111\n","Accuracy = 0.6846590909090908\n","Epoch: 11 of 20\n","bi=0, loss=0.06880056858062744\n","bi=10, loss=0.06605572253465652\n","Accuracy = 0.6960227272727273\n","Epoch: 12 of 20\n","bi=0, loss=0.0388372503221035\n","bi=10, loss=0.04081225022673607\n","Accuracy = 0.7017045454545454\n","Epoch: 13 of 20\n","bi=0, loss=0.03213874623179436\n","bi=10, loss=0.013840829953551292\n","Accuracy = 0.6619318181818181\n","Epoch: 14 of 20\n","bi=0, loss=0.05299580097198486\n","bi=10, loss=0.1726250946521759\n","Accuracy = 0.6761363636363636\n","Epoch: 15 of 20\n","bi=0, loss=0.191078320145607\n","bi=10, loss=0.08987163752317429\n","Accuracy = 0.6107954545454546\n","Epoch: 16 of 20\n","bi=0, loss=0.3786258101463318\n","bi=10, loss=0.09363862872123718\n","Accuracy = 0.6392045454545454\n","Epoch: 17 of 20\n","bi=0, loss=0.026445018127560616\n","bi=10, loss=0.043262191116809845\n","Accuracy = 0.6704545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.06014055758714676\n","bi=10, loss=0.03358456492424011\n","Accuracy = 0.6732954545454546\n","Epoch: 19 of 20\n","bi=0, loss=0.052768588066101074\n","bi=10, loss=0.018547283485531807\n","Accuracy = 0.65625\n","Epoch: 20 of 20\n","bi=0, loss=0.051490820944309235\n","bi=10, loss=0.018294459208846092\n","Accuracy = 0.6590909090909092\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 4\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3503704071044922\n","bi=10, loss=0.8796641826629639\n","Accuracy = 0.6761363636363635\n","Epoch: 2 of 20\n","bi=0, loss=0.6581674218177795\n","bi=10, loss=0.8036162257194519\n","Accuracy = 0.6789772727272727\n","Epoch: 3 of 20\n","bi=0, loss=0.751000165939331\n","bi=10, loss=0.527424156665802\n","Accuracy = 0.7073863636363636\n","Epoch: 4 of 20\n","bi=0, loss=0.43028461933135986\n","bi=10, loss=0.29504141211509705\n","Accuracy = 0.6448863636363636\n","Epoch: 5 of 20\n","bi=0, loss=0.4004398584365845\n","bi=10, loss=0.2951052188873291\n","Accuracy = 0.6818181818181819\n","Epoch: 6 of 20\n","bi=0, loss=0.1474214494228363\n","bi=10, loss=0.26115623116493225\n","Accuracy = 0.7215909090909091\n","Epoch: 7 of 20\n","bi=0, loss=0.27879318594932556\n","bi=10, loss=0.09171237051486969\n","Accuracy = 0.6931818181818182\n","Epoch: 8 of 20\n","bi=0, loss=0.11248259246349335\n","bi=10, loss=0.139431893825531\n","Accuracy = 0.6903409090909092\n","Epoch: 9 of 20\n","bi=0, loss=0.19615833461284637\n","bi=10, loss=0.08834177255630493\n","Accuracy = 0.6846590909090908\n","Epoch: 10 of 20\n","bi=0, loss=0.0843212828040123\n","bi=10, loss=0.22710345685482025\n","Accuracy = 0.7159090909090908\n","Epoch: 11 of 20\n","bi=0, loss=0.02835400588810444\n","bi=10, loss=0.10848626494407654\n","Accuracy = 0.6732954545454546\n","Epoch: 12 of 20\n","bi=0, loss=0.10912875086069107\n","bi=10, loss=0.12167565524578094\n","Accuracy = 0.6335227272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.10544107854366302\n","bi=10, loss=0.3068622648715973\n","Accuracy = 0.6619318181818181\n","Epoch: 14 of 20\n","bi=0, loss=0.043380334973335266\n","bi=10, loss=0.5456737279891968\n","Accuracy = 0.7272727272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.06005510315299034\n","bi=10, loss=0.0982382595539093\n","Accuracy = 0.71875\n","Epoch: 16 of 20\n","bi=0, loss=0.09336818754673004\n","bi=10, loss=0.024884870275855064\n","Accuracy = 0.71875\n","Epoch: 17 of 20\n","bi=0, loss=0.02862592041492462\n","bi=10, loss=0.018631359562277794\n","Accuracy = 0.6931818181818181\n","Epoch: 18 of 20\n","bi=0, loss=0.08497664332389832\n","bi=10, loss=0.030130693688988686\n","Accuracy = 0.6960227272727273\n","Epoch: 19 of 20\n","bi=0, loss=0.061179786920547485\n","bi=10, loss=0.010611497797071934\n","Accuracy = 0.6960227272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.06899239122867584\n","bi=10, loss=0.009190457873046398\n","Accuracy = 0.6903409090909091\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3605506420135498\n","bi=10, loss=0.7899090051651001\n","Accuracy = 0.6846590909090908\n","Epoch: 2 of 20\n","bi=0, loss=0.8739094138145447\n","bi=10, loss=0.8265914916992188\n","Accuracy = 0.6619318181818181\n","Epoch: 3 of 20\n","bi=0, loss=0.8709388971328735\n","bi=10, loss=0.7725119590759277\n","Accuracy = 0.7130681818181818\n","Epoch: 4 of 20\n","bi=0, loss=0.738282322883606\n","bi=10, loss=0.49117282032966614\n","Accuracy = 0.5880681818181819\n","Epoch: 5 of 20\n","bi=0, loss=1.0425909757614136\n","bi=10, loss=0.3234102427959442\n","Accuracy = 0.7301136363636362\n","Epoch: 6 of 20\n","bi=0, loss=0.5663187503814697\n","bi=10, loss=0.22013147175312042\n","Accuracy = 0.6789772727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.3575248718261719\n","bi=10, loss=0.20167014002799988\n","Accuracy = 0.6590909090909091\n","Epoch: 8 of 20\n","bi=0, loss=0.22778812050819397\n","bi=10, loss=0.14538492262363434\n","Accuracy = 0.6875\n","Epoch: 9 of 20\n","bi=0, loss=0.3678007423877716\n","bi=10, loss=0.23060104250907898\n","Accuracy = 0.7045454545454545\n","Epoch: 10 of 20\n","bi=0, loss=0.21056205034255981\n","bi=10, loss=0.06391765922307968\n","Accuracy = 0.6761363636363636\n","Epoch: 11 of 20\n","bi=0, loss=0.18798166513442993\n","bi=10, loss=0.15651512145996094\n","Accuracy = 0.6903409090909091\n","Epoch: 12 of 20\n","bi=0, loss=0.12069881707429886\n","bi=10, loss=0.1492556929588318\n","Accuracy = 0.6903409090909092\n","Epoch: 13 of 20\n","bi=0, loss=0.15994718670845032\n","bi=10, loss=0.2210678905248642\n","Accuracy = 0.7130681818181819\n","Epoch: 14 of 20\n","bi=0, loss=0.4697921574115753\n","bi=10, loss=0.22116044163703918\n","Accuracy = 0.7102272727272727\n","Epoch: 15 of 20\n","bi=0, loss=0.42685776948928833\n","bi=10, loss=0.06507845222949982\n","Accuracy = 0.7017045454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.12284527719020844\n","bi=10, loss=0.2630063593387604\n","Accuracy = 0.65625\n","Epoch: 17 of 20\n","bi=0, loss=0.1114756241440773\n","bi=10, loss=0.15529343485832214\n","Accuracy = 0.6619318181818181\n","Epoch: 18 of 20\n","bi=0, loss=0.09213545173406601\n","bi=10, loss=0.031505461782217026\n","Accuracy = 0.6761363636363635\n","Epoch: 19 of 20\n","bi=0, loss=0.08191188424825668\n","bi=10, loss=0.026492660865187645\n","Accuracy = 0.6732954545454545\n","Epoch: 20 of 20\n","bi=0, loss=0.0796118900179863\n","bi=10, loss=0.028791116550564766\n","Accuracy = 0.6761363636363635\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.320276141166687\n","bi=10, loss=0.8405401706695557\n","Accuracy = 0.6676136363636362\n","Epoch: 2 of 20\n","bi=0, loss=0.8715879321098328\n","bi=10, loss=0.8424667119979858\n","Accuracy = 0.6846590909090908\n","Epoch: 3 of 20\n","bi=0, loss=0.7151275277137756\n","bi=10, loss=0.550177276134491\n","Accuracy = 0.6960227272727273\n","Epoch: 4 of 20\n","bi=0, loss=0.40886929631233215\n","bi=10, loss=0.3421420454978943\n","Accuracy = 0.71875\n","Epoch: 5 of 20\n","bi=0, loss=0.27833086252212524\n","bi=10, loss=0.3451556861400604\n","Accuracy = 0.7045454545454546\n","Epoch: 6 of 20\n","bi=0, loss=0.19768787920475006\n","bi=10, loss=0.42484578490257263\n","Accuracy = 0.5852272727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.6405954360961914\n","bi=10, loss=0.23253671824932098\n","Accuracy = 0.6392045454545454\n","Epoch: 8 of 20\n","bi=0, loss=0.22277909517288208\n","bi=10, loss=0.21064846217632294\n","Accuracy = 0.65625\n","Epoch: 9 of 20\n","bi=0, loss=0.13910415768623352\n","bi=10, loss=0.1853092461824417\n","Accuracy = 0.6704545454545454\n","Epoch: 10 of 20\n","bi=0, loss=0.0856626033782959\n","bi=10, loss=0.18435560166835785\n","Accuracy = 0.6619318181818181\n","Epoch: 11 of 20\n","bi=0, loss=0.07437928766012192\n","bi=10, loss=0.2537880539894104\n","Accuracy = 0.6846590909090909\n","Epoch: 12 of 20\n","bi=0, loss=0.06074725463986397\n","bi=10, loss=0.18071232736110687\n","Accuracy = 0.7073863636363635\n","Epoch: 13 of 20\n","bi=0, loss=0.3131742775440216\n","bi=10, loss=0.04081879183650017\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.22633537650108337\n","bi=10, loss=0.08710095286369324\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.14426974952220917\n","bi=10, loss=0.27103692293167114\n","Accuracy = 0.6448863636363635\n","Epoch: 16 of 20\n","bi=0, loss=0.0964389443397522\n","bi=10, loss=0.03092258982360363\n","Accuracy = 0.65625\n","Epoch: 17 of 20\n","bi=0, loss=0.07989896088838577\n","bi=10, loss=0.028817567974328995\n","Accuracy = 0.6505681818181818\n","Epoch: 18 of 20\n","bi=0, loss=0.07947883009910583\n","bi=10, loss=0.023741280660033226\n","Accuracy = 0.6619318181818181\n","Epoch: 19 of 20\n","bi=0, loss=0.06889072060585022\n","bi=10, loss=0.02984217368066311\n","Accuracy = 0.6647727272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.06988674402236938\n","bi=10, loss=0.02267845720052719\n","Accuracy = 0.6647727272727273\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3376169204711914\n","bi=10, loss=0.9553654193878174\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.9568967223167419\n","bi=10, loss=1.0266270637512207\n","Accuracy = 0.7017045454545455\n","Epoch: 3 of 20\n","bi=0, loss=0.7884399890899658\n","bi=10, loss=0.6781651973724365\n","Accuracy = 0.7159090909090909\n","Epoch: 4 of 20\n","bi=0, loss=0.583716869354248\n","bi=10, loss=0.3468817472457886\n","Accuracy = 0.7329545454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.33929795026779175\n","bi=10, loss=0.4351590573787689\n","Accuracy = 0.71875\n","Epoch: 6 of 20\n","bi=0, loss=0.3106651306152344\n","bi=10, loss=0.6939857602119446\n","Accuracy = 0.7357954545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.18331588804721832\n","bi=10, loss=0.06731016933917999\n","Accuracy = 0.71875\n","Epoch: 8 of 20\n","bi=0, loss=0.10974949598312378\n","bi=10, loss=0.04965014383196831\n","Accuracy = 0.6875\n","Epoch: 9 of 20\n","bi=0, loss=0.07962897419929504\n","bi=10, loss=0.10132614523172379\n","Accuracy = 0.6335227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.08455737680196762\n","bi=10, loss=0.02418959140777588\n","Accuracy = 0.7073863636363635\n","Epoch: 11 of 20\n","bi=0, loss=0.11598704010248184\n","bi=10, loss=0.08878208696842194\n","Accuracy = 0.7130681818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.02223709598183632\n","bi=10, loss=0.17133581638336182\n","Accuracy = 0.7301136363636365\n","Epoch: 13 of 20\n","bi=0, loss=0.026883594691753387\n","bi=10, loss=0.009265705943107605\n","Accuracy = 0.7357954545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.2406132072210312\n","bi=10, loss=0.0246607456356287\n","Accuracy = 0.6960227272727273\n","Epoch: 15 of 20\n","bi=0, loss=0.30884435772895813\n","bi=10, loss=0.02947041019797325\n","Accuracy = 0.6931818181818181\n","Epoch: 16 of 20\n","bi=0, loss=0.0877862423658371\n","bi=10, loss=0.08916988223791122\n","Accuracy = 0.7102272727272727\n","Epoch: 17 of 20\n","bi=0, loss=0.07072503864765167\n","bi=10, loss=0.01739557459950447\n","Accuracy = 0.7329545454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.05136683210730553\n","bi=10, loss=0.020418794825673103\n","Accuracy = 0.7215909090909091\n","Epoch: 19 of 20\n","bi=0, loss=0.044483330100774765\n","bi=10, loss=0.00832919217646122\n","Accuracy = 0.7215909090909092\n","Epoch: 20 of 20\n","bi=0, loss=0.020754344761371613\n","bi=10, loss=0.008389070630073547\n","Accuracy = 0.7215909090909091\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3535865545272827\n","bi=10, loss=1.0230787992477417\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.8674293756484985\n","bi=10, loss=0.881089985370636\n","Accuracy = 0.6761363636363638\n","Epoch: 3 of 20\n","bi=0, loss=0.7906171679496765\n","bi=10, loss=0.5962468981742859\n","Accuracy = 0.7357954545454546\n","Epoch: 4 of 20\n","bi=0, loss=0.5630077123641968\n","bi=10, loss=0.6653274893760681\n","Accuracy = 0.6960227272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.46511808037757874\n","bi=10, loss=0.46787673234939575\n","Accuracy = 0.7045454545454546\n","Epoch: 6 of 20\n","bi=0, loss=0.16512003540992737\n","bi=10, loss=0.4810563325881958\n","Accuracy = 0.6306818181818181\n","Epoch: 7 of 20\n","bi=0, loss=0.479509174823761\n","bi=10, loss=0.2409071922302246\n","Accuracy = 0.6647727272727273\n","Epoch: 8 of 20\n","bi=0, loss=0.36676058173179626\n","bi=10, loss=0.21761126816272736\n","Accuracy = 0.6960227272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.1706262230873108\n","bi=10, loss=0.07007825374603271\n","Accuracy = 0.71875\n","Epoch: 10 of 20\n","bi=0, loss=0.14769166707992554\n","bi=10, loss=0.05832016095519066\n","Accuracy = 0.6846590909090908\n","Epoch: 11 of 20\n","bi=0, loss=0.07284171134233475\n","bi=10, loss=0.03364007547497749\n","Accuracy = 0.6505681818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.06488784402608871\n","bi=10, loss=0.014084307476878166\n","Accuracy = 0.7017045454545454\n","Epoch: 13 of 20\n","bi=0, loss=0.06073618307709694\n","bi=10, loss=0.015543234534561634\n","Accuracy = 0.684659090909091\n","Epoch: 14 of 20\n","bi=0, loss=0.04550205543637276\n","bi=10, loss=0.010680005885660648\n","Accuracy = 0.6846590909090909\n","Epoch: 15 of 20\n","bi=0, loss=0.14798776805400848\n","bi=10, loss=0.007530570030212402\n","Accuracy = 0.7301136363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.2684963345527649\n","bi=10, loss=0.013895107433199883\n","Accuracy = 0.7073863636363636\n","Epoch: 17 of 20\n","bi=0, loss=0.03631655499339104\n","bi=10, loss=0.0956830158829689\n","Accuracy = 0.6164772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.15503807365894318\n","bi=10, loss=0.023315774276852608\n","Accuracy = 0.7130681818181819\n","Epoch: 19 of 20\n","bi=0, loss=0.06530116498470306\n","bi=10, loss=0.018885396420955658\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.04768971726298332\n","bi=10, loss=0.015488575212657452\n","Accuracy = 0.7017045454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3464583158493042\n","bi=10, loss=1.0577727556228638\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.8643375635147095\n","bi=10, loss=1.0119998455047607\n","Accuracy = 0.6676136363636365\n","Epoch: 3 of 20\n","bi=0, loss=0.8332562446594238\n","bi=10, loss=0.9441143274307251\n","Accuracy = 0.7017045454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.7628297805786133\n","bi=10, loss=0.6581981182098389\n","Accuracy = 0.6875\n","Epoch: 5 of 20\n","bi=0, loss=0.4377090036869049\n","bi=10, loss=0.4848257601261139\n","Accuracy = 0.6988636363636364\n","Epoch: 6 of 20\n","bi=0, loss=0.5123306512832642\n","bi=10, loss=0.34250596165657043\n","Accuracy = 0.7045454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.22071383893489838\n","bi=10, loss=0.8547396659851074\n","Accuracy = 0.6392045454545454\n","Epoch: 8 of 20\n","bi=0, loss=0.380781888961792\n","bi=10, loss=0.29748427867889404\n","Accuracy = 0.6818181818181819\n","Epoch: 9 of 20\n","bi=0, loss=0.20173577964305878\n","bi=10, loss=0.06867372989654541\n","Accuracy = 0.6931818181818181\n","Epoch: 10 of 20\n","bi=0, loss=0.15262725949287415\n","bi=10, loss=0.25893980264663696\n","Accuracy = 0.7017045454545454\n","Epoch: 11 of 20\n","bi=0, loss=0.23463518917560577\n","bi=10, loss=0.07980556041002274\n","Accuracy = 0.6732954545454546\n","Epoch: 12 of 20\n","bi=0, loss=0.09940744191408157\n","bi=10, loss=0.04013202711939812\n","Accuracy = 0.6960227272727273\n","Epoch: 13 of 20\n","bi=0, loss=0.05855840444564819\n","bi=10, loss=0.012932267040014267\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.011071055196225643\n","bi=10, loss=0.008334433659911156\n","Accuracy = 0.7017045454545455\n","Epoch: 15 of 20\n","bi=0, loss=0.05470149219036102\n","bi=10, loss=0.005237629171460867\n","Accuracy = 0.6818181818181819\n","Epoch: 16 of 20\n","bi=0, loss=0.02774455025792122\n","bi=10, loss=0.016428891569375992\n","Accuracy = 0.6931818181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.005860771518200636\n","bi=10, loss=0.025002045556902885\n","Accuracy = 0.6789772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.014718310907483101\n","bi=10, loss=0.006429486442357302\n","Accuracy = 0.6988636363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.008712423965334892\n","bi=10, loss=0.005602327175438404\n","Accuracy = 0.7017045454545454\n","Epoch: 20 of 20\n","bi=0, loss=0.007933207787573338\n","bi=10, loss=0.006579460110515356\n","Accuracy = 0.7017045454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 32, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 243, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.352691411972046\n","bi=10, loss=0.954913318157196\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.9482623934745789\n","bi=10, loss=1.0009523630142212\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.9264779686927795\n","bi=10, loss=0.9007887840270996\n","Accuracy = 0.6590909090909092\n","Epoch: 4 of 20\n","bi=0, loss=0.8229498863220215\n","bi=10, loss=0.6595919132232666\n","Accuracy = 0.6676136363636364\n","Epoch: 5 of 20\n","bi=0, loss=0.8115348219871521\n","bi=10, loss=0.746372640132904\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.6808421611785889\n","bi=10, loss=0.7154814600944519\n","Accuracy = 0.6960227272727273\n","Epoch: 7 of 20\n","bi=0, loss=0.6154633164405823\n","bi=10, loss=0.4080660343170166\n","Accuracy = 0.6732954545454546\n","Epoch: 8 of 20\n","bi=0, loss=0.34064680337905884\n","bi=10, loss=0.21299909055233002\n","Accuracy = 0.6988636363636364\n","Epoch: 9 of 20\n","bi=0, loss=0.2877452075481415\n","bi=10, loss=0.11977194994688034\n","Accuracy = 0.6960227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.18155008554458618\n","bi=10, loss=0.13913686573505402\n","Accuracy = 0.6818181818181818\n","Epoch: 11 of 20\n","bi=0, loss=0.15284675359725952\n","bi=10, loss=0.1910596638917923\n","Accuracy = 0.6761363636363636\n","Epoch: 12 of 20\n","bi=0, loss=0.20840899646282196\n","bi=10, loss=0.11090176552534103\n","Accuracy = 0.6107954545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.1349678933620453\n","bi=10, loss=0.05048713460564613\n","Accuracy = 0.6732954545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.12173610925674438\n","bi=10, loss=0.37583062052726746\n","Accuracy = 0.6903409090909091\n","Epoch: 15 of 20\n","bi=0, loss=0.2756088972091675\n","bi=10, loss=0.11917199194431305\n","Accuracy = 0.6960227272727273\n","Epoch: 16 of 20\n","bi=0, loss=0.20325729250907898\n","bi=10, loss=0.04822313413023949\n","Accuracy = 0.6647727272727272\n","Epoch: 17 of 20\n","bi=0, loss=0.11090689152479172\n","bi=10, loss=0.032767780125141144\n","Accuracy = 0.6818181818181819\n","Epoch: 18 of 20\n","bi=0, loss=0.08726301044225693\n","bi=10, loss=0.027221554890275\n","Accuracy = 0.6988636363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.08773139119148254\n","bi=10, loss=0.023701930418610573\n","Accuracy = 0.6903409090909092\n","Epoch: 20 of 20\n","bi=0, loss=0.05640771985054016\n","bi=10, loss=0.027227552607655525\n","Accuracy = 0.6875\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 1\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.348362922668457\n","Accuracy = 0.6676136363636364\n","Epoch: 2 of 20\n","bi=0, loss=0.8774291276931763\n","Accuracy = 0.6676136363636364\n","Epoch: 3 of 20\n","bi=0, loss=0.8846620917320251\n","Accuracy = 0.6676136363636364\n","Epoch: 4 of 20\n","bi=0, loss=0.8730897903442383\n","Accuracy = 0.6704545454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.7936338782310486\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.6142417788505554\n","Accuracy = 0.5596590909090909\n","Epoch: 7 of 20\n","bi=0, loss=0.6909701228141785\n","Accuracy = 0.6647727272727273\n","Epoch: 8 of 20\n","bi=0, loss=0.4595004916191101\n","Accuracy = 0.6448863636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.3485240638256073\n","Accuracy = 0.6676136363636364\n","Epoch: 10 of 20\n","bi=0, loss=0.3210904598236084\n","Accuracy = 0.6619318181818182\n","Epoch: 11 of 20\n","bi=0, loss=0.24678277969360352\n","Accuracy = 0.6619318181818181\n","Epoch: 12 of 20\n","bi=0, loss=0.47172465920448303\n","Accuracy = 0.6931818181818182\n","Epoch: 13 of 20\n","bi=0, loss=0.5214855074882507\n","Accuracy = 0.6420454545454546\n","Epoch: 14 of 20\n","bi=0, loss=0.28171563148498535\n","Accuracy = 0.6732954545454545\n","Epoch: 15 of 20\n","bi=0, loss=0.2433496117591858\n","Accuracy = 0.6619318181818181\n","Epoch: 16 of 20\n","bi=0, loss=0.15127894282341003\n","Accuracy = 0.6505681818181819\n","Epoch: 17 of 20\n","bi=0, loss=0.09917493909597397\n","Accuracy = 0.6392045454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.09439675509929657\n","Accuracy = 0.6448863636363636\n","Epoch: 19 of 20\n","bi=0, loss=0.0760258138179779\n","Accuracy = 0.6420454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.08121040463447571\n","Accuracy = 0.6420454545454546\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 2\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3520973920822144\n","Accuracy = 0.6590909090909091\n","Epoch: 2 of 20\n","bi=0, loss=0.8473979830741882\n","Accuracy = 0.6590909090909091\n","Epoch: 3 of 20\n","bi=0, loss=0.7982594966888428\n","Accuracy = 0.6590909090909091\n","Epoch: 4 of 20\n","bi=0, loss=0.756206750869751\n","Accuracy = 0.6647727272727273\n","Epoch: 5 of 20\n","bi=0, loss=0.6902034282684326\n","Accuracy = 0.6704545454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.6271460652351379\n","Accuracy = 0.71875\n","Epoch: 7 of 20\n","bi=0, loss=0.508564829826355\n","Accuracy = 0.7244318181818181\n","Epoch: 8 of 20\n","bi=0, loss=0.4188399016857147\n","Accuracy = 0.7329545454545454\n","Epoch: 9 of 20\n","bi=0, loss=0.25275951623916626\n","Accuracy = 0.7301136363636365\n","Epoch: 10 of 20\n","bi=0, loss=0.25058579444885254\n","Accuracy = 0.6988636363636364\n","Epoch: 11 of 20\n","bi=0, loss=0.19666147232055664\n","Accuracy = 0.7159090909090908\n","Epoch: 12 of 20\n","bi=0, loss=0.18572354316711426\n","Accuracy = 0.5539772727272727\n","Epoch: 13 of 20\n","bi=0, loss=0.4246825873851776\n","Accuracy = 0.6931818181818182\n","Epoch: 14 of 20\n","bi=0, loss=0.333812415599823\n","Accuracy = 0.6988636363636365\n","Epoch: 15 of 20\n","bi=0, loss=0.19746412336826324\n","Accuracy = 0.7244318181818181\n","Epoch: 16 of 20\n","bi=0, loss=0.114076167345047\n","Accuracy = 0.6846590909090909\n","Epoch: 17 of 20\n","bi=0, loss=0.10498181730508804\n","Accuracy = 0.7017045454545454\n","Epoch: 18 of 20\n","bi=0, loss=0.06144220381975174\n","Accuracy = 0.6988636363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.05648135766386986\n","Accuracy = 0.6960227272727273\n","Epoch: 20 of 20\n","bi=0, loss=0.04328900948166847\n","Accuracy = 0.6988636363636365\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 3\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3490761518478394\n","Accuracy = 0.6732954545454545\n","Epoch: 2 of 20\n","bi=0, loss=0.7955262064933777\n","Accuracy = 0.6732954545454545\n","Epoch: 3 of 20\n","bi=0, loss=0.7854678630828857\n","Accuracy = 0.6732954545454545\n","Epoch: 4 of 20\n","bi=0, loss=0.7264031171798706\n","Accuracy = 0.5625\n","Epoch: 5 of 20\n","bi=0, loss=0.7627772092819214\n","Accuracy = 0.6732954545454546\n","Epoch: 6 of 20\n","bi=0, loss=0.5162132978439331\n","Accuracy = 0.6875\n","Epoch: 7 of 20\n","bi=0, loss=0.41783979535102844\n","Accuracy = 0.6846590909090908\n","Epoch: 8 of 20\n","bi=0, loss=0.3309226632118225\n","Accuracy = 0.6590909090909092\n","Epoch: 9 of 20\n","bi=0, loss=0.24193069338798523\n","Accuracy = 0.6051136363636364\n","Epoch: 10 of 20\n","bi=0, loss=0.20784662663936615\n","Accuracy = 0.5965909090909092\n","Epoch: 11 of 20\n","bi=0, loss=0.172724187374115\n","Accuracy = 0.5909090909090908\n","Epoch: 12 of 20\n","bi=0, loss=0.26121312379837036\n","Accuracy = 0.7386363636363635\n","Epoch: 13 of 20\n","bi=0, loss=0.4453687369823456\n","Accuracy = 0.6590909090909092\n","Epoch: 14 of 20\n","bi=0, loss=0.18117161095142365\n","Accuracy = 0.7159090909090909\n","Epoch: 15 of 20\n","bi=0, loss=0.1596742868423462\n","Accuracy = 0.6363636363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.05816885456442833\n","Accuracy = 0.6732954545454546\n","Epoch: 17 of 20\n","bi=0, loss=0.045078929513692856\n","Accuracy = 0.6732954545454546\n","Epoch: 18 of 20\n","bi=0, loss=0.05238082632422447\n","Accuracy = 0.6732954545454546\n","Epoch: 19 of 20\n","bi=0, loss=0.039137061685323715\n","Accuracy = 0.6704545454545454\n","Epoch: 20 of 20\n","bi=0, loss=0.0374179445207119\n","Accuracy = 0.6676136363636364\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 4\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3355389833450317\n","Accuracy = 0.6761363636363635\n","Epoch: 2 of 20\n","bi=0, loss=0.7060998678207397\n","Accuracy = 0.6761363636363635\n","Epoch: 3 of 20\n","bi=0, loss=0.738398551940918\n","Accuracy = 0.6789772727272727\n","Epoch: 4 of 20\n","bi=0, loss=0.6838325262069702\n","Accuracy = 0.6761363636363635\n","Epoch: 5 of 20\n","bi=0, loss=0.6437198519706726\n","Accuracy = 0.7017045454545454\n","Epoch: 6 of 20\n","bi=0, loss=0.563563346862793\n","Accuracy = 0.7045454545454546\n","Epoch: 7 of 20\n","bi=0, loss=0.4934689700603485\n","Accuracy = 0.7045454545454546\n","Epoch: 8 of 20\n","bi=0, loss=0.4014965891838074\n","Accuracy = 0.6988636363636365\n","Epoch: 9 of 20\n","bi=0, loss=0.34575846791267395\n","Accuracy = 0.7130681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.1890905797481537\n","Accuracy = 0.6278409090909092\n","Epoch: 11 of 20\n","bi=0, loss=0.41705870628356934\n","Accuracy = 0.6079545454545454\n","Epoch: 12 of 20\n","bi=0, loss=0.4377104640007019\n","Accuracy = 0.7215909090909092\n","Epoch: 13 of 20\n","bi=0, loss=0.2967820167541504\n","Accuracy = 0.7017045454545454\n","Epoch: 14 of 20\n","bi=0, loss=0.1972164809703827\n","Accuracy = 0.7045454545454546\n","Epoch: 15 of 20\n","bi=0, loss=0.15507589280605316\n","Accuracy = 0.7017045454545454\n","Epoch: 16 of 20\n","bi=0, loss=0.14349617063999176\n","Accuracy = 0.7017045454545454\n","Epoch: 17 of 20\n","bi=0, loss=0.09722732752561569\n","Accuracy = 0.7045454545454546\n","Epoch: 18 of 20\n","bi=0, loss=0.07419876754283905\n","Accuracy = 0.7017045454545455\n","Epoch: 19 of 20\n","bi=0, loss=0.05519101768732071\n","Accuracy = 0.7017045454545455\n","Epoch: 20 of 20\n","bi=0, loss=0.058010704815387726\n","Accuracy = 0.7017045454545455\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 5\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3423711061477661\n","Accuracy = 0.6619318181818181\n","Epoch: 2 of 20\n","bi=0, loss=0.9553441405296326\n","Accuracy = 0.6619318181818181\n","Epoch: 3 of 20\n","bi=0, loss=0.9233128428459167\n","Accuracy = 0.6619318181818181\n","Epoch: 4 of 20\n","bi=0, loss=0.9023366570472717\n","Accuracy = 0.6846590909090908\n","Epoch: 5 of 20\n","bi=0, loss=0.6933004856109619\n","Accuracy = 0.7130681818181819\n","Epoch: 6 of 20\n","bi=0, loss=0.7202938795089722\n","Accuracy = 0.65625\n","Epoch: 7 of 20\n","bi=0, loss=0.5423120856285095\n","Accuracy = 0.6846590909090908\n","Epoch: 8 of 20\n","bi=0, loss=0.3175535500049591\n","Accuracy = 0.6761363636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.202922523021698\n","Accuracy = 0.6960227272727273\n","Epoch: 10 of 20\n","bi=0, loss=0.2132420539855957\n","Accuracy = 0.6647727272727273\n","Epoch: 11 of 20\n","bi=0, loss=0.18083827197551727\n","Accuracy = 0.6789772727272727\n","Epoch: 12 of 20\n","bi=0, loss=0.073448047041893\n","Accuracy = 0.7244318181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.09160222858190536\n","Accuracy = 0.7045454545454545\n","Epoch: 14 of 20\n","bi=0, loss=0.3484686613082886\n","Accuracy = 0.7130681818181819\n","Epoch: 15 of 20\n","bi=0, loss=0.21472743153572083\n","Accuracy = 0.6988636363636362\n","Epoch: 16 of 20\n","bi=0, loss=0.09066805988550186\n","Accuracy = 0.6931818181818181\n","Epoch: 17 of 20\n","bi=0, loss=0.07128188014030457\n","Accuracy = 0.6789772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.07912895828485489\n","Accuracy = 0.6988636363636365\n","Epoch: 19 of 20\n","bi=0, loss=0.044851191341876984\n","Accuracy = 0.7045454545454546\n","Epoch: 20 of 20\n","bi=0, loss=0.040301598608493805\n","Accuracy = 0.7073863636363635\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 6\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3215903043746948\n","Accuracy = 0.6676136363636362\n","Epoch: 2 of 20\n","bi=0, loss=0.818279504776001\n","Accuracy = 0.6676136363636362\n","Epoch: 3 of 20\n","bi=0, loss=0.8142299652099609\n","Accuracy = 0.6676136363636362\n","Epoch: 4 of 20\n","bi=0, loss=0.8218763470649719\n","Accuracy = 0.6306818181818181\n","Epoch: 5 of 20\n","bi=0, loss=0.7089253067970276\n","Accuracy = 0.6903409090909092\n","Epoch: 6 of 20\n","bi=0, loss=0.6364259719848633\n","Accuracy = 0.71875\n","Epoch: 7 of 20\n","bi=0, loss=0.5695197582244873\n","Accuracy = 0.7215909090909091\n","Epoch: 8 of 20\n","bi=0, loss=0.4057806134223938\n","Accuracy = 0.6960227272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.1530003547668457\n","Accuracy = 0.6619318181818182\n","Epoch: 10 of 20\n","bi=0, loss=0.21274253726005554\n","Accuracy = 0.6789772727272727\n","Epoch: 11 of 20\n","bi=0, loss=0.22577941417694092\n","Accuracy = 0.5568181818181819\n","Epoch: 12 of 20\n","bi=0, loss=0.4388507902622223\n","Accuracy = 0.7073863636363635\n","Epoch: 13 of 20\n","bi=0, loss=0.18916790187358856\n","Accuracy = 0.6477272727272727\n","Epoch: 14 of 20\n","bi=0, loss=0.16329821944236755\n","Accuracy = 0.6846590909090909\n","Epoch: 15 of 20\n","bi=0, loss=0.09087815135717392\n","Accuracy = 0.6676136363636364\n","Epoch: 16 of 20\n","bi=0, loss=0.07533998787403107\n","Accuracy = 0.6676136363636364\n","Epoch: 17 of 20\n","bi=0, loss=0.0564701110124588\n","Accuracy = 0.6789772727272727\n","Epoch: 18 of 20\n","bi=0, loss=0.04646235331892967\n","Accuracy = 0.6732954545454546\n","Epoch: 19 of 20\n","bi=0, loss=0.03809452801942825\n","Accuracy = 0.6676136363636364\n","Epoch: 20 of 20\n","bi=0, loss=0.038667213171720505\n","Accuracy = 0.6789772727272728\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 7\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3354398012161255\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.856982946395874\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.8933531641960144\n","Accuracy = 0.6732954545454546\n","Epoch: 4 of 20\n","bi=0, loss=0.9248775839805603\n","Accuracy = 0.6704545454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.977572500705719\n","Accuracy = 0.6732954545454546\n","Epoch: 6 of 20\n","bi=0, loss=0.8489130139350891\n","Accuracy = 0.6789772727272727\n","Epoch: 7 of 20\n","bi=0, loss=0.8065657019615173\n","Accuracy = 0.6534090909090908\n","Epoch: 8 of 20\n","bi=0, loss=0.7850168347358704\n","Accuracy = 0.6647727272727273\n","Epoch: 9 of 20\n","bi=0, loss=0.6890904903411865\n","Accuracy = 0.6619318181818181\n","Epoch: 10 of 20\n","bi=0, loss=0.6002987623214722\n","Accuracy = 0.7215909090909091\n","Epoch: 11 of 20\n","bi=0, loss=0.5141615271568298\n","Accuracy = 0.6988636363636364\n","Epoch: 12 of 20\n","bi=0, loss=0.5913940072059631\n","Accuracy = 0.7017045454545455\n","Epoch: 13 of 20\n","bi=0, loss=0.38131725788116455\n","Accuracy = 0.7215909090909092\n","Epoch: 14 of 20\n","bi=0, loss=0.3229716718196869\n","Accuracy = 0.7130681818181819\n","Epoch: 15 of 20\n","bi=0, loss=0.36398738622665405\n","Accuracy = 0.7130681818181819\n","Epoch: 16 of 20\n","bi=0, loss=0.24669650197029114\n","Accuracy = 0.7386363636363635\n","Epoch: 17 of 20\n","bi=0, loss=0.163808211684227\n","Accuracy = 0.7357954545454546\n","Epoch: 18 of 20\n","bi=0, loss=0.10812334716320038\n","Accuracy = 0.7443181818181819\n","Epoch: 19 of 20\n","bi=0, loss=0.11070410907268524\n","Accuracy = 0.7301136363636364\n","Epoch: 20 of 20\n","bi=0, loss=0.08838889747858047\n","Accuracy = 0.7329545454545454\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 8\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3518736362457275\n","Accuracy = 0.6704545454545454\n","Epoch: 2 of 20\n","bi=0, loss=0.8331697583198547\n","Accuracy = 0.6704545454545454\n","Epoch: 3 of 20\n","bi=0, loss=0.8475677371025085\n","Accuracy = 0.6704545454545454\n","Epoch: 4 of 20\n","bi=0, loss=0.809145450592041\n","Accuracy = 0.6704545454545454\n","Epoch: 5 of 20\n","bi=0, loss=0.8897575736045837\n","Accuracy = 0.6761363636363636\n","Epoch: 6 of 20\n","bi=0, loss=0.9086191654205322\n","Accuracy = 0.6931818181818181\n","Epoch: 7 of 20\n","bi=0, loss=0.9146397113800049\n","Accuracy = 0.6647727272727273\n","Epoch: 8 of 20\n","bi=0, loss=0.8250628113746643\n","Accuracy = 0.7130681818181818\n","Epoch: 9 of 20\n","bi=0, loss=0.7709894180297852\n","Accuracy = 0.6818181818181818\n","Epoch: 10 of 20\n","bi=0, loss=0.7295873761177063\n","Accuracy = 0.6534090909090908\n","Epoch: 11 of 20\n","bi=0, loss=0.8120535016059875\n","Accuracy = 0.6534090909090909\n","Epoch: 12 of 20\n","bi=0, loss=0.6483182311058044\n","Accuracy = 0.7244318181818181\n","Epoch: 13 of 20\n","bi=0, loss=0.5690436363220215\n","Accuracy = 0.6704545454545454\n","Epoch: 14 of 20\n","bi=0, loss=0.4715120196342468\n","Accuracy = 0.6875\n","Epoch: 15 of 20\n","bi=0, loss=0.3529188334941864\n","Accuracy = 0.7102272727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.31547799706459045\n","Accuracy = 0.6903409090909091\n","Epoch: 17 of 20\n","bi=0, loss=0.26906618475914\n","Accuracy = 0.6960227272727273\n","Epoch: 18 of 20\n","bi=0, loss=0.19028958678245544\n","Accuracy = 0.6875\n","Epoch: 19 of 20\n","bi=0, loss=0.12259231507778168\n","Accuracy = 0.6875\n","Epoch: 20 of 20\n","bi=0, loss=0.09735137969255447\n","Accuracy = 0.6846590909090909\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 9\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.341120958328247\n","Accuracy = 0.6676136363636365\n","Epoch: 2 of 20\n","bi=0, loss=0.8387012481689453\n","Accuracy = 0.6676136363636365\n","Epoch: 3 of 20\n","bi=0, loss=0.8460911512374878\n","Accuracy = 0.6676136363636365\n","Epoch: 4 of 20\n","bi=0, loss=0.8814800977706909\n","Accuracy = 0.6676136363636365\n","Epoch: 5 of 20\n","bi=0, loss=0.8522565364837646\n","Accuracy = 0.6676136363636365\n","Epoch: 6 of 20\n","bi=0, loss=0.8836492896080017\n","Accuracy = 0.6676136363636365\n","Epoch: 7 of 20\n","bi=0, loss=0.8914896845817566\n","Accuracy = 0.6676136363636365\n","Epoch: 8 of 20\n","bi=0, loss=0.8403760194778442\n","Accuracy = 0.6761363636363636\n","Epoch: 9 of 20\n","bi=0, loss=0.7429518699645996\n","Accuracy = 0.7102272727272727\n","Epoch: 10 of 20\n","bi=0, loss=0.7008416056632996\n","Accuracy = 0.6818181818181819\n","Epoch: 11 of 20\n","bi=0, loss=0.5679498314857483\n","Accuracy = 0.6590909090909091\n","Epoch: 12 of 20\n","bi=0, loss=0.5050090551376343\n","Accuracy = 0.7102272727272727\n","Epoch: 13 of 20\n","bi=0, loss=0.5374152064323425\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.2402109056711197\n","Accuracy = 0.6420454545454546\n","Epoch: 15 of 20\n","bi=0, loss=0.32218635082244873\n","Accuracy = 0.5852272727272727\n","Epoch: 16 of 20\n","bi=0, loss=0.9518687725067139\n","Accuracy = 0.6960227272727273\n","Epoch: 17 of 20\n","bi=0, loss=0.3553869426250458\n","Accuracy = 0.6818181818181818\n","Epoch: 18 of 20\n","bi=0, loss=0.25274819135665894\n","Accuracy = 0.6676136363636364\n","Epoch: 19 of 20\n","bi=0, loss=0.26162683963775635\n","Accuracy = 0.6761363636363636\n","Epoch: 20 of 20\n","bi=0, loss=0.2572508156299591\n","Accuracy = 0.6818181818181818\n","parameters: Bertmodel: SpanishBert, Output: pooler, lr: 5e-05, Batch: 64, Totsl Num. Epochs: 20, Fold: 10\n","num_train_steps = 121, world_size=8\n","Epoch: 1 of 20\n","bi=0, loss=1.3385963439941406\n","Accuracy = 0.6732954545454546\n","Epoch: 2 of 20\n","bi=0, loss=0.8832628130912781\n","Accuracy = 0.6732954545454546\n","Epoch: 3 of 20\n","bi=0, loss=0.8664416670799255\n","Accuracy = 0.6732954545454546\n","Epoch: 4 of 20\n","bi=0, loss=0.8345115780830383\n","Accuracy = 0.6732954545454546\n","Epoch: 5 of 20\n","bi=0, loss=0.840160608291626\n","Accuracy = 0.6732954545454545\n","Epoch: 6 of 20\n","bi=0, loss=0.8515728712081909\n","Accuracy = 0.6647727272727273\n","Epoch: 7 of 20\n","bi=0, loss=0.7907693386077881\n","Accuracy = 0.7017045454545454\n","Epoch: 8 of 20\n","bi=0, loss=0.6183133125305176\n","Accuracy = 0.7017045454545454\n","Epoch: 9 of 20\n","bi=0, loss=0.38226762413978577\n","Accuracy = 0.7130681818181819\n","Epoch: 10 of 20\n","bi=0, loss=0.2973906993865967\n","Accuracy = 0.6846590909090908\n","Epoch: 11 of 20\n","bi=0, loss=0.20330531895160675\n","Accuracy = 0.5056818181818182\n","Epoch: 12 of 20\n","bi=0, loss=0.6475247740745544\n","Accuracy = 0.6420454545454546\n","Epoch: 13 of 20\n","bi=0, loss=0.4808468222618103\n","Accuracy = 0.6875\n","Epoch: 14 of 20\n","bi=0, loss=0.42558613419532776\n","Accuracy = 0.6931818181818181\n","Epoch: 15 of 20\n","bi=0, loss=0.4066028594970703\n","Accuracy = 0.7073863636363636\n","Epoch: 16 of 20\n","bi=0, loss=0.26100465655326843\n","Accuracy = 0.7073863636363636\n","Epoch: 17 of 20\n","bi=0, loss=0.12954050302505493\n","Accuracy = 0.7159090909090909\n","Epoch: 18 of 20\n","bi=0, loss=0.1391216367483139\n","Accuracy = 0.7130681818181819\n","Epoch: 19 of 20\n","bi=0, loss=0.10194981843233109\n","Accuracy = 0.7159090909090909\n","Epoch: 20 of 20\n","bi=0, loss=0.09727035462856293\n","Accuracy = 0.71875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hUbui2-D_FP3"},"source":["def AveragResults(FileName, Path):\n","  with open(Path + FileName + \".pkl\", \"rb\") as f:\n","              Results = pickle.load(f)\n","\n","  for BT, ModelBertType,  in Results.items():\n","    for OP, OutPut in ModelBertType.items():\n","      for LR, LearningRate in OutPut.items():\n","        for BS, BatchSize in LearningRate.items():\n","          for EP, Epoch in BatchSize.items():\n","            for Metrics, ValuesCrossValidation in  Epoch.items():\n"," \n","              # Metrics = np.mean(ValuesCrossValidation)\n","              Results[BT][OP][LR][BS][EP][Metrics] = np.mean(ValuesCrossValidation)\n","            \n","  with open('Average' + FileName + '.pkl','wb') as f:\n","    pickle.dump(Results, f)\n","\n","  with open(Path + 'Average' + FileName + '.pkl','wb') as f:\n","    pickle.dump(Results, f)\n","  \n","  return Results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNx5HL3gE1G9"},"source":["## Average and Save Results\n","AverageResultsTask = AveragResults(FileName=FileResults, Path=Path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Wm1dH5GgFDP"},"source":["### create dataframe for our results\n","def create_Data_Frame(all_resultas):\n","\n","  \n","\n","  ### Criate a pandas da Frame with all results\n","  df_results = pd.DataFrame.from_dict({(BertType, OutpuType, LearningRate, BactSize, Epochs): all_resultas[BertType][OutpuType][LearningRate][BactSize][Epochs]\n","                            for BertType in all_resultas.keys()\n","                            for OutpuType in all_resultas[BertType].keys()\n","                            for LearningRate in all_resultas[BertType][OutpuType].keys()\n","                            for BactSize in all_resultas[BertType][OutpuType][LearningRate].keys()\n","                            for Epochs in all_resultas[BertType][OutpuType][LearningRate][BactSize].keys()},\n","                        orient='index')\n","  return df_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_e1yqO24I3w4","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1619773827476,"user_tz":180,"elapsed":544,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"e2dca32d-e406-4e86-f79c-9971b666cd21"},"source":["## Create a Data Frame\n","DfResultsTask = create_Data_Frame(all_resultas=AverageResultsTask)\n","\n","### save results to a CSV file\n","DfResultsTask.to_csv(Path + 'Average' + FileResults + '_CSV_' + '.csv')\n","\n","### See the Avarage results in the Pandas data Frame\n","DfResultsTask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1_macro</th>\n","      <th>f1_weighted</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>cem</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"11\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"5\" valign=\"top\">hidden</th>\n","      <th rowspan=\"5\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"5\" valign=\"top\">8</th>\n","      <th>1</th>\n","      <td>0.685795</td>\n","      <td>0.288260</td>\n","      <td>0.000000</td>\n","      <td>0.314073</td>\n","      <td>0.331216</td>\n","      <td>0.722565</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.714205</td>\n","      <td>0.385249</td>\n","      <td>0.722565</td>\n","      <td>0.388204</td>\n","      <td>0.424997</td>\n","      <td>0.769625</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.716477</td>\n","      <td>0.413840</td>\n","      <td>0.769625</td>\n","      <td>0.415084</td>\n","      <td>0.461104</td>\n","      <td>0.774677</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.705398</td>\n","      <td>0.416486</td>\n","      <td>0.774677</td>\n","      <td>0.421735</td>\n","      <td>0.456023</td>\n","      <td>0.764872</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.715341</td>\n","      <td>0.416753</td>\n","      <td>0.764872</td>\n","      <td>0.421896</td>\n","      <td>0.459225</td>\n","      <td>0.773905</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">pooler</th>\n","      <th rowspan=\"5\" valign=\"top\">0.00005</th>\n","      <th rowspan=\"5\" valign=\"top\">64</th>\n","      <th>16</th>\n","      <td>0.690341</td>\n","      <td>0.414261</td>\n","      <td>0.751552</td>\n","      <td>0.428517</td>\n","      <td>0.430052</td>\n","      <td>0.752674</td>\n","      <td>0.205362</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.690625</td>\n","      <td>0.415757</td>\n","      <td>0.752674</td>\n","      <td>0.427383</td>\n","      <td>0.433727</td>\n","      <td>0.752244</td>\n","      <td>0.136129</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.690341</td>\n","      <td>0.412469</td>\n","      <td>0.752244</td>\n","      <td>0.426431</td>\n","      <td>0.425709</td>\n","      <td>0.753042</td>\n","      <td>0.109672</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.689205</td>\n","      <td>0.414175</td>\n","      <td>0.753042</td>\n","      <td>0.428855</td>\n","      <td>0.425632</td>\n","      <td>0.751826</td>\n","      <td>0.093740</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.691477</td>\n","      <td>0.421846</td>\n","      <td>0.751826</td>\n","      <td>0.435106</td>\n","      <td>0.438013</td>\n","      <td>0.753182</td>\n","      <td>0.083850</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>480 rows √ó 7 columns</p>\n","</div>"],"text/plain":["                                  accuracy  f1_macro  ...       cem      loss\n","SpanishBert hidden 0.00001 8  1   0.685795  0.288260  ...  0.722565       NaN\n","                              2   0.714205  0.385249  ...  0.769625       NaN\n","                              3   0.716477  0.413840  ...  0.774677       NaN\n","                              4   0.705398  0.416486  ...  0.764872       NaN\n","                              5   0.715341  0.416753  ...  0.773905       NaN\n","...                                    ...       ...  ...       ...       ...\n","            pooler 0.00005 64 16  0.690341  0.414261  ...  0.752674  0.205362\n","                              17  0.690625  0.415757  ...  0.752244  0.136129\n","                              18  0.690341  0.412469  ...  0.753042  0.109672\n","                              19  0.689205  0.414175  ...  0.751826  0.093740\n","                              20  0.691477  0.421846  ...  0.753182  0.083850\n","\n","[480 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"r7M8o_lHjzkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619773838978,"user_tz":180,"elapsed":551,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"06395bca-a355-44cb-e8dd-baf3e286d883"},"source":["## Creating LateX Table\n","LabelTaskTable = FileResults\n","print(DfResultsTask.to_latex(multicolumn=True, multirow=False, label=LabelTaskTable))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\\begin{table}\n","\\centering\n","\\label{SpanishBertTask2Results}\n","\\begin{tabular}{lllllrrrrrrr}\n","\\toprule\n","            &        &         &    &    &  accuracy &  f1\\_macro &  f1\\_weighted &    recall &  precision &       cem &      loss \\\\\n","\\midrule\n","SpanishBert & hidden & 0.00001 & 8  & 1  &  0.685795 &  0.288260 &     0.000000 &  0.314073 &   0.331216 &  0.722565 &       NaN \\\\\n","            &        &         &    & 2  &  0.714205 &  0.385249 &     0.722565 &  0.388204 &   0.424997 &  0.769625 &       NaN \\\\\n","            &        &         &    & 3  &  0.716477 &  0.413840 &     0.769625 &  0.415084 &   0.461104 &  0.774677 &       NaN \\\\\n","            &        &         &    & 4  &  0.705398 &  0.416486 &     0.774677 &  0.421735 &   0.456023 &  0.764872 &       NaN \\\\\n","            &        &         &    & 5  &  0.715341 &  0.416753 &     0.764872 &  0.421896 &   0.459225 &  0.773905 &       NaN \\\\\n","            &        &         &    & 6  &  0.706818 &  0.435896 &     0.773905 &  0.446014 &   0.461498 &  0.766749 &       NaN \\\\\n","            &        &         &    & 7  &  0.704261 &  0.436803 &     0.766749 &  0.449535 &   0.457539 &  0.764952 &       NaN \\\\\n","            &        &         &    & 8  &  0.710227 &  0.423075 &     0.764952 &  0.431958 &   0.454768 &  0.771399 &       NaN \\\\\n","            &        &         &    & 9  &  0.697727 &  0.433877 &     0.771399 &  0.446527 &   0.455149 &  0.760324 &       NaN \\\\\n","            &        &         &    & 10 &  0.712500 &  0.447470 &     0.760324 &  0.457609 &   0.472361 &  0.767159 &       NaN \\\\\n","            &        &         &    & 11 &  0.709091 &  0.453229 &     0.767159 &  0.457771 &   0.480959 &  0.762843 &       NaN \\\\\n","            &        &         &    & 12 &  0.712216 &  0.436130 &     0.762843 &  0.438994 &   0.469619 &  0.765927 &       NaN \\\\\n","            &        &         &    & 13 &  0.709659 &  0.443044 &     0.765927 &  0.448801 &   0.474629 &  0.765984 &       NaN \\\\\n","            &        &         &    & 14 &  0.715625 &  0.442322 &     0.765984 &  0.444973 &   0.472712 &  0.770173 &       NaN \\\\\n","            &        &         &    & 15 &  0.716477 &  0.449103 &     0.770173 &  0.449757 &   0.483730 &  0.771236 &       NaN \\\\\n","            &        &         &    & 16 &  0.717898 &  0.451593 &     0.771236 &  0.452176 &   0.486291 &  0.772296 &       NaN \\\\\n","            &        &         &    & 17 &  0.716761 &  0.449948 &     0.772296 &  0.451223 &   0.484324 &  0.771558 &       NaN \\\\\n","            &        &         &    & 18 &  0.715625 &  0.448289 &     0.771558 &  0.450136 &   0.479829 &  0.770829 &       NaN \\\\\n","            &        &         &    & 19 &  0.715625 &  0.448533 &     0.770829 &  0.450332 &   0.480501 &  0.770964 &       NaN \\\\\n","            &        &         &    & 20 &  0.717614 &  0.450320 &     0.770964 &  0.452513 &   0.482516 &  0.772752 &       NaN \\\\\n","            &        &         & 16 & 1  &  0.686364 &  0.289458 &     0.000000 &  0.315581 &   0.322272 &  0.722017 &       NaN \\\\\n","            &        &         &    & 2  &  0.697159 &  0.315201 &     0.722017 &  0.333980 &   0.350734 &  0.748822 &       NaN \\\\\n","            &        &         &    & 3  &  0.711080 &  0.374860 &     0.748822 &  0.380266 &   0.407729 &  0.767770 &       NaN \\\\\n","            &        &         &    & 4  &  0.717045 &  0.403550 &     0.767770 &  0.409123 &   0.446873 &  0.776860 &       NaN \\\\\n","            &        &         &    & 5  &  0.701989 &  0.438691 &     0.776860 &  0.446561 &   0.472469 &  0.760992 &       NaN \\\\\n","            &        &         &    & 6  &  0.690341 &  0.419282 &     0.760992 &  0.432181 &   0.437323 &  0.750110 &       NaN \\\\\n","            &        &         &    & 7  &  0.693182 &  0.425264 &     0.750110 &  0.443550 &   0.444088 &  0.753293 &       NaN \\\\\n","            &        &         &    & 8  &  0.697727 &  0.421497 &     0.753293 &  0.433276 &   0.444855 &  0.755711 &       NaN \\\\\n","            &        &         &    & 9  &  0.703409 &  0.427565 &     0.755711 &  0.433378 &   0.461370 &  0.759595 &       NaN \\\\\n","            &        &         &    & 10 &  0.704545 &  0.438043 &     0.759595 &  0.442847 &   0.466311 &  0.760793 &       NaN \\\\\n","            &        &         &    & 11 &  0.706250 &  0.449745 &     0.760793 &  0.452207 &   0.481837 &  0.763206 &       NaN \\\\\n","            &        &         &    & 12 &  0.701136 &  0.442703 &     0.763206 &  0.450027 &   0.466105 &  0.758758 &       NaN \\\\\n","            &        &         &    & 13 &  0.703409 &  0.445319 &     0.758758 &  0.452970 &   0.465839 &  0.760144 &       NaN \\\\\n","            &        &         &    & 14 &  0.707670 &  0.447397 &     0.760144 &  0.452275 &   0.474318 &  0.762972 &       NaN \\\\\n","            &        &         &    & 15 &  0.704261 &  0.443881 &     0.762972 &  0.449210 &   0.467837 &  0.758882 &       NaN \\\\\n","            &        &         &    & 16 &  0.711932 &  0.448727 &     0.758882 &  0.453714 &   0.477367 &  0.765368 &       NaN \\\\\n","            &        &         &    & 17 &  0.710227 &  0.444237 &     0.765368 &  0.445561 &   0.474403 &  0.763682 &       NaN \\\\\n","            &        &         &    & 18 &  0.711932 &  0.451803 &     0.763682 &  0.454342 &   0.480953 &  0.766449 &       NaN \\\\\n","            &        &         &    & 19 &  0.707955 &  0.437294 &     0.766449 &  0.441515 &   0.464464 &  0.763211 &       NaN \\\\\n","            &        &         &    & 20 &  0.707955 &  0.442903 &     0.763211 &  0.445593 &   0.471848 &  0.762217 &       NaN \\\\\n","            &        &         & 32 & 1  &  0.669886 &  0.228838 &     0.000000 &  0.280993 &   0.223836 &  0.675058 &       NaN \\\\\n","            &        &         &    & 2  &  0.681250 &  0.282276 &     0.675058 &  0.309718 &   0.311559 &  0.717245 &       NaN \\\\\n","            &        &         &    & 3  &  0.689489 &  0.333842 &     0.717245 &  0.344219 &   0.381959 &  0.736779 &       NaN \\\\\n","            &        &         &    & 4  &  0.695739 &  0.367231 &     0.736779 &  0.375116 &   0.387910 &  0.753027 &       NaN \\\\\n","            &        &         &    & 5  &  0.701705 &  0.390725 &     0.753027 &  0.396031 &   0.415014 &  0.759026 &       NaN \\\\\n","            &        &         &    & 6  &  0.708239 &  0.411711 &     0.759026 &  0.413944 &   0.442114 &  0.765904 &       NaN \\\\\n","            &        &         &    & 7  &  0.702273 &  0.413364 &     0.765904 &  0.417827 &   0.444133 &  0.762230 &       NaN \\\\\n","            &        &         &    & 8  &  0.704261 &  0.369214 &     0.762230 &  0.378712 &   0.413870 &  0.761263 &       NaN \\\\\n","            &        &         &    & 9  &  0.708239 &  0.406037 &     0.761263 &  0.416301 &   0.443293 &  0.763638 &       NaN \\\\\n","            &        &         &    & 10 &  0.704545 &  0.428191 &     0.763638 &  0.435462 &   0.450814 &  0.756160 &       NaN \\\\\n","            &        &         &    & 11 &  0.704545 &  0.457854 &     0.756160 &  0.461860 &   0.491147 &  0.761897 &       NaN \\\\\n","            &        &         &    & 12 &  0.683239 &  0.461067 &     0.761897 &  0.473057 &   0.487404 &  0.750978 &       NaN \\\\\n","            &        &         &    & 13 &  0.666477 &  0.459978 &     0.750978 &  0.481655 &   0.479899 &  0.739297 &       NaN \\\\\n","            &        &         &    & 14 &  0.688352 &  0.457268 &     0.739297 &  0.465798 &   0.488823 &  0.750366 &       NaN \\\\\n","            &        &         &    & 15 &  0.703125 &  0.444196 &     0.750366 &  0.449937 &   0.476585 &  0.760673 &       NaN \\\\\n","            &        &         &    & 16 &  0.707386 &  0.445031 &     0.760673 &  0.448861 &   0.478399 &  0.762441 &       NaN \\\\\n","            &        &         &    & 17 &  0.711080 &  0.460162 &     0.762441 &  0.462458 &   0.498847 &  0.766791 &       NaN \\\\\n","            &        &         &    & 18 &  0.709091 &  0.452274 &     0.766791 &  0.454010 &   0.489089 &  0.765392 &       NaN \\\\\n","            &        &         &    & 19 &  0.707955 &  0.455910 &     0.765392 &  0.458758 &   0.492224 &  0.764073 &       NaN \\\\\n","            &        &         &    & 20 &  0.710227 &  0.457968 &     0.764073 &  0.460461 &   0.495797 &  0.766288 &       NaN \\\\\n","            &        &         & 64 & 1  &  0.669602 &  0.251469 &     0.000000 &  0.290450 &   0.290773 &  0.677937 &  1.460240 \\\\\n","            &        &         &    & 2  &  0.672727 &  0.234654 &     0.677937 &  0.284300 &   0.246786 &  0.682844 &  0.877240 \\\\\n","            &        &         &    & 3  &  0.683523 &  0.291784 &     0.682844 &  0.315844 &   0.314068 &  0.721583 &  0.795750 \\\\\n","            &        &         &    & 4  &  0.689489 &  0.310628 &     0.721583 &  0.330760 &   0.325376 &  0.735152 &  0.761259 \\\\\n","            &        &         &    & 5  &  0.690909 &  0.313472 &     0.735152 &  0.331630 &   0.331978 &  0.738665 &  0.704522 \\\\\n","            &        &         &    & 6  &  0.701136 &  0.365833 &     0.738665 &  0.371154 &   0.394304 &  0.752992 &  0.645902 \\\\\n","            &        &         &    & 7  &  0.698580 &  0.377491 &     0.752992 &  0.381161 &   0.405218 &  0.752576 &  0.583658 \\\\\n","            &        &         &    & 8  &  0.699716 &  0.409103 &     0.752576 &  0.413144 &   0.428845 &  0.754451 &  0.538336 \\\\\n","            &        &         &    & 9  &  0.698295 &  0.421650 &     0.754451 &  0.426187 &   0.444189 &  0.755834 &  0.485563 \\\\\n","            &        &         &    & 10 &  0.701136 &  0.430788 &     0.755834 &  0.435622 &   0.455571 &  0.758472 &  0.408645 \\\\\n","            &        &         &    & 11 &  0.700000 &  0.414180 &     0.758472 &  0.419332 &   0.442613 &  0.756893 &  0.362261 \\\\\n","            &        &         &    & 12 &  0.701705 &  0.419230 &     0.756893 &  0.423300 &   0.451310 &  0.758043 &  0.308713 \\\\\n","            &        &         &    & 13 &  0.697443 &  0.400640 &     0.758043 &  0.408306 &   0.432290 &  0.755878 &  0.260709 \\\\\n","            &        &         &    & 14 &  0.706534 &  0.413354 &     0.755878 &  0.418381 &   0.445167 &  0.764287 &  0.253602 \\\\\n","            &        &         &    & 15 &  0.712500 &  0.421520 &     0.764287 &  0.423996 &   0.456247 &  0.766457 &  0.202612 \\\\\n","            &        &         &    & 16 &  0.703409 &  0.423444 &     0.766457 &  0.429341 &   0.453118 &  0.759292 &  0.200886 \\\\\n","            &        &         &    & 17 &  0.710511 &  0.419874 &     0.759292 &  0.423441 &   0.452251 &  0.764885 &  0.165294 \\\\\n","            &        &         &    & 18 &  0.703125 &  0.425134 &     0.764885 &  0.429060 &   0.458676 &  0.757918 &  0.160684 \\\\\n","            &        &         &    & 19 &  0.707670 &  0.426468 &     0.757918 &  0.430656 &   0.459080 &  0.761106 &  0.143354 \\\\\n","            &        &         &    & 20 &  0.707102 &  0.422414 &     0.761106 &  0.425656 &   0.454738 &  0.761313 &  0.137441 \\\\\n","            &        & 0.00003 & 8  & 1  &  0.696023 &  0.338720 &     0.000000 &  0.361838 &   0.406522 &  0.747811 &       NaN \\\\\n","            &        &         &    & 2  &  0.685227 &  0.385910 &     0.747811 &  0.399842 &   0.410953 &  0.750268 &       NaN \\\\\n","            &        &         &    & 3  &  0.680682 &  0.399884 &     0.750268 &  0.416666 &   0.424818 &  0.747078 &       NaN \\\\\n","            &        &         &    & 4  &  0.692330 &  0.447180 &     0.747078 &  0.460870 &   0.474260 &  0.753244 &       NaN \\\\\n","            &        &         &    & 5  &  0.648295 &  0.458754 &     0.753244 &  0.492392 &   0.475468 &  0.725264 &       NaN \\\\\n","            &        &         &    & 6  &  0.660227 &  0.426295 &     0.725264 &  0.445448 &   0.454549 &  0.729975 &       NaN \\\\\n","            &        &         &    & 7  &  0.684943 &  0.415774 &     0.729975 &  0.430702 &   0.445483 &  0.743544 &       NaN \\\\\n","            &        &         &    & 8  &  0.705682 &  0.450623 &     0.743544 &  0.456377 &   0.470623 &  0.755185 &       NaN \\\\\n","            &        &         &    & 9  &  0.695455 &  0.440490 &     0.755185 &  0.449006 &   0.466070 &  0.750301 &       NaN \\\\\n","            &        &         &    & 10 &  0.700568 &  0.434343 &     0.750301 &  0.443387 &   0.457002 &  0.755584 &       NaN \\\\\n","            &        &         &    & 11 &  0.702557 &  0.429390 &     0.755584 &  0.434503 &   0.449983 &  0.752377 &       NaN \\\\\n","            &        &         &    & 12 &  0.705682 &  0.428293 &     0.752377 &  0.431233 &   0.459345 &  0.758896 &       NaN \\\\\n","            &        &         &    & 13 &  0.706534 &  0.431616 &     0.758896 &  0.433922 &   0.460181 &  0.758922 &       NaN \\\\\n","            &        &         &    & 14 &  0.699716 &  0.430047 &     0.758922 &  0.433043 &   0.455700 &  0.753903 &       NaN \\\\\n","            &        &         &    & 15 &  0.705682 &  0.435799 &     0.753903 &  0.438194 &   0.464692 &  0.757663 &       NaN \\\\\n","            &        &         &    & 16 &  0.700852 &  0.436265 &     0.757663 &  0.440404 &   0.458605 &  0.753704 &       NaN \\\\\n","            &        &         &    & 17 &  0.707386 &  0.445296 &     0.753704 &  0.447422 &   0.470786 &  0.758353 &       NaN \\\\\n","            &        &         &    & 18 &  0.711080 &  0.448262 &     0.758353 &  0.449715 &   0.477803 &  0.762165 &       NaN \\\\\n","            &        &         &    & 19 &  0.710227 &  0.446441 &     0.762165 &  0.448478 &   0.475506 &  0.761288 &       NaN \\\\\n","            &        &         &    & 20 &  0.710511 &  0.446789 &     0.761288 &  0.448769 &   0.475876 &  0.761744 &       NaN \\\\\n","            &        &         & 16 & 1  &  0.694886 &  0.325222 &     0.000000 &  0.347650 &   0.364767 &  0.739565 &       NaN \\\\\n","            &        &         &    & 2  &  0.690341 &  0.389759 &     0.739565 &  0.413666 &   0.394824 &  0.752395 &       NaN \\\\\n","            &        &         &    & 3  &  0.651420 &  0.418136 &     0.752395 &  0.444801 &   0.436390 &  0.735420 &       NaN \\\\\n","            &        &         &    & 4  &  0.689773 &  0.414472 &     0.735420 &  0.427724 &   0.433080 &  0.749556 &       NaN \\\\\n","            &        &         &    & 5  &  0.691193 &  0.420372 &     0.749556 &  0.432388 &   0.442557 &  0.755364 &       NaN \\\\\n","            &        &         &    & 6  &  0.712216 &  0.439330 &     0.755364 &  0.448719 &   0.468564 &  0.769149 &       NaN \\\\\n","            &        &         &    & 7  &  0.705114 &  0.430004 &     0.769149 &  0.442338 &   0.459797 &  0.763294 &       NaN \\\\\n","            &        &         &    & 8  &  0.708239 &  0.432830 &     0.763294 &  0.441728 &   0.458313 &  0.763721 &       NaN \\\\\n","            &        &         &    & 9  &  0.710511 &  0.426826 &     0.763721 &  0.430247 &   0.457235 &  0.767181 &       NaN \\\\\n","            &        &         &    & 10 &  0.707955 &  0.431335 &     0.767181 &  0.439818 &   0.450959 &  0.762462 &       NaN \\\\\n","            &        &         &    & 11 &  0.714489 &  0.435607 &     0.762462 &  0.440592 &   0.459687 &  0.769586 &       NaN \\\\\n","            &        &         &    & 12 &  0.705966 &  0.439097 &     0.769586 &  0.453870 &   0.452573 &  0.761180 &       NaN \\\\\n","            &        &         &    & 13 &  0.704261 &  0.451232 &     0.761180 &  0.463283 &   0.467952 &  0.761304 &       NaN \\\\\n","            &        &         &    & 14 &  0.710511 &  0.445619 &     0.761304 &  0.451397 &   0.469228 &  0.763882 &       NaN \\\\\n","            &        &         &    & 15 &  0.715057 &  0.448393 &     0.763882 &  0.454866 &   0.470160 &  0.767476 &       NaN \\\\\n","            &        &         &    & 16 &  0.711932 &  0.447296 &     0.767476 &  0.454449 &   0.466472 &  0.764778 &       NaN \\\\\n","            &        &         &    & 17 &  0.709659 &  0.448387 &     0.764778 &  0.455704 &   0.468287 &  0.762895 &       NaN \\\\\n","            &        &         &    & 18 &  0.707955 &  0.440777 &     0.762895 &  0.448997 &   0.460406 &  0.761360 &       NaN \\\\\n","            &        &         &    & 19 &  0.709091 &  0.444699 &     0.761360 &  0.452480 &   0.466279 &  0.762568 &       NaN \\\\\n","            &        &         &    & 20 &  0.709659 &  0.447294 &     0.762568 &  0.455911 &   0.468452 &  0.763121 &       NaN \\\\\n","            &        &         & 32 & 1  &  0.691477 &  0.310021 &     0.000000 &  0.331362 &   0.332854 &  0.737455 &       NaN \\\\\n","            &        &         &    & 2  &  0.697727 &  0.366747 &     0.737455 &  0.381260 &   0.423431 &  0.752608 &       NaN \\\\\n","            &        &         &    & 3  &  0.709091 &  0.419720 &     0.752608 &  0.434961 &   0.438836 &  0.769664 &       NaN \\\\\n","            &        &         &    & 4  &  0.677557 &  0.398439 &     0.769664 &  0.427041 &   0.407668 &  0.749781 &       NaN \\\\\n","            &        &         &    & 5  &  0.675000 &  0.386170 &     0.749781 &  0.411145 &   0.417778 &  0.748270 &       NaN \\\\\n","            &        &         &    & 6  &  0.718750 &  0.409604 &     0.748270 &  0.417309 &   0.435515 &  0.774593 &       NaN \\\\\n","            &        &         &    & 7  &  0.709091 &  0.453281 &     0.774593 &  0.464062 &   0.484592 &  0.770195 &       NaN \\\\\n","            &        &         &    & 8  &  0.699716 &  0.469718 &     0.770195 &  0.485313 &   0.500719 &  0.756877 &       NaN \\\\\n","            &        &         &    & 9  &  0.693466 &  0.432778 &     0.756877 &  0.448284 &   0.451382 &  0.754056 &       NaN \\\\\n","            &        &         &    & 10 &  0.661080 &  0.436832 &     0.754056 &  0.465262 &   0.455368 &  0.737660 &       NaN \\\\\n","            &        &         &    & 11 &  0.674432 &  0.428465 &     0.737660 &  0.463553 &   0.432631 &  0.743433 &       NaN \\\\\n","            &        &         &    & 12 &  0.703693 &  0.462011 &     0.743433 &  0.478025 &   0.486526 &  0.762128 &       NaN \\\\\n","            &        &         &    & 13 &  0.694034 &  0.448409 &     0.762128 &  0.462971 &   0.476287 &  0.756856 &       NaN \\\\\n","            &        &         &    & 14 &  0.687500 &  0.441878 &     0.756856 &  0.460298 &   0.473443 &  0.757994 &       NaN \\\\\n","            &        &         &    & 15 &  0.706250 &  0.470325 &     0.757994 &  0.479973 &   0.495736 &  0.763566 &       NaN \\\\\n","            &        &         &    & 16 &  0.702273 &  0.457814 &     0.763566 &  0.468785 &   0.480664 &  0.763866 &       NaN \\\\\n","            &        &         &    & 17 &  0.704545 &  0.467355 &     0.763866 &  0.475156 &   0.499649 &  0.765400 &       NaN \\\\\n","            &        &         &    & 18 &  0.706250 &  0.466573 &     0.765400 &  0.473885 &   0.500426 &  0.766013 &       NaN \\\\\n","            &        &         &    & 19 &  0.705114 &  0.460909 &     0.766013 &  0.468032 &   0.494771 &  0.765282 &       NaN \\\\\n","            &        &         &    & 20 &  0.706250 &  0.465617 &     0.765282 &  0.472675 &   0.500691 &  0.766540 &       NaN \\\\\n","            &        &         & 64 & 1  &  0.671307 &  0.232959 &     0.000000 &  0.287657 &   0.209248 &  0.672642 &  1.571047 \\\\\n","            &        &         &    & 2  &  0.687216 &  0.294015 &     0.672642 &  0.325008 &   0.324727 &  0.723794 &  0.869292 \\\\\n","            &        &         &    & 3  &  0.698864 &  0.345168 &     0.723794 &  0.358977 &   0.382298 &  0.750784 &  0.777354 \\\\\n","            &        &         &    & 4  &  0.707102 &  0.383507 &     0.750784 &  0.391628 &   0.411706 &  0.763167 &  0.672110 \\\\\n","            &        &         &    & 5  &  0.714773 &  0.417772 &     0.763167 &  0.423512 &   0.446093 &  0.774551 &  0.541801 \\\\\n","            &        &         &    & 6  &  0.698011 &  0.440131 &     0.774551 &  0.456688 &   0.465704 &  0.762383 &  0.416956 \\\\\n","            &        &         &    & 7  &  0.701136 &  0.404534 &     0.762383 &  0.416618 &   0.429399 &  0.767006 &  0.354609 \\\\\n","            &        &         &    & 8  &  0.699148 &  0.428469 &     0.767006 &  0.439266 &   0.451684 &  0.763133 &  0.278161 \\\\\n","            &        &         &    & 9  &  0.666477 &  0.431253 &     0.763133 &  0.454585 &   0.455690 &  0.739671 &  0.214520 \\\\\n","            &        &         &    & 10 &  0.662216 &  0.409519 &     0.739671 &  0.439983 &   0.428783 &  0.739255 &  0.328597 \\\\\n","            &        &         &    & 11 &  0.705398 &  0.415727 &     0.739255 &  0.426714 &   0.450771 &  0.761482 &  0.291659 \\\\\n","            &        &         &    & 12 &  0.686932 &  0.437884 &     0.761482 &  0.456761 &   0.464289 &  0.755763 &  0.252138 \\\\\n","            &        &         &    & 13 &  0.713068 &  0.436355 &     0.755763 &  0.443717 &   0.471171 &  0.769183 &  0.166651 \\\\\n","            &        &         &    & 14 &  0.709659 &  0.450999 &     0.769183 &  0.457685 &   0.482855 &  0.768370 &  0.130858 \\\\\n","            &        &         &    & 15 &  0.702841 &  0.458641 &     0.768370 &  0.471382 &   0.485378 &  0.763437 &  0.083673 \\\\\n","            &        &         &    & 16 &  0.703693 &  0.438305 &     0.763437 &  0.448720 &   0.461564 &  0.765397 &  0.048646 \\\\\n","            &        &         &    & 17 &  0.707102 &  0.448121 &     0.765397 &  0.453192 &   0.475685 &  0.766967 &  0.036664 \\\\\n","            &        &         &    & 18 &  0.705966 &  0.443912 &     0.766967 &  0.449189 &   0.470044 &  0.765103 &  0.032000 \\\\\n","            &        &         &    & 19 &  0.705114 &  0.442670 &     0.765103 &  0.448207 &   0.469785 &  0.764202 &  0.027655 \\\\\n","            &        &         &    & 20 &  0.705682 &  0.448791 &     0.764202 &  0.453435 &   0.477850 &  0.764870 &  0.021112 \\\\\n","            &        & 0.00005 & 8  & 1  &  0.694886 &  0.330154 &     0.000000 &  0.362443 &   0.381207 &  0.754429 &       NaN \\\\\n","            &        &         &    & 2  &  0.708807 &  0.413074 &     0.754429 &  0.431097 &   0.465479 &  0.771905 &       NaN \\\\\n","            &        &         &    & 3  &  0.691761 &  0.425493 &     0.771905 &  0.444467 &   0.458007 &  0.750206 &       NaN \\\\\n","            &        &         &    & 4  &  0.690625 &  0.412823 &     0.750206 &  0.433960 &   0.445171 &  0.753235 &       NaN \\\\\n","            &        &         &    & 5  &  0.680682 &  0.395244 &     0.753235 &  0.420414 &   0.410342 &  0.740687 &       NaN \\\\\n","            &        &         &    & 6  &  0.686080 &  0.391575 &     0.740687 &  0.410262 &   0.423343 &  0.746980 &       NaN \\\\\n","            &        &         &    & 7  &  0.681534 &  0.407570 &     0.746980 &  0.423984 &   0.423934 &  0.745434 &       NaN \\\\\n","            &        &         &    & 8  &  0.653693 &  0.428841 &     0.745434 &  0.449482 &   0.446368 &  0.721053 &       NaN \\\\\n","            &        &         &    & 9  &  0.663068 &  0.412186 &     0.721053 &  0.434675 &   0.430510 &  0.729091 &       NaN \\\\\n","            &        &         &    & 10 &  0.686648 &  0.419095 &     0.729091 &  0.434290 &   0.432595 &  0.741537 &       NaN \\\\\n","            &        &         &    & 11 &  0.697443 &  0.416392 &     0.741537 &  0.423252 &   0.444147 &  0.752139 &       NaN \\\\\n","            &        &         &    & 12 &  0.696875 &  0.423750 &     0.752139 &  0.434446 &   0.447927 &  0.755437 &       NaN \\\\\n","            &        &         &    & 13 &  0.695739 &  0.418631 &     0.755437 &  0.426531 &   0.439950 &  0.753133 &       NaN \\\\\n","            &        &         &    & 14 &  0.685795 &  0.421543 &     0.753133 &  0.435177 &   0.440088 &  0.745659 &       NaN \\\\\n","            &        &         &    & 15 &  0.676705 &  0.415568 &     0.745659 &  0.428048 &   0.438113 &  0.740990 &       NaN \\\\\n","            &        &         &    & 16 &  0.687500 &  0.420183 &     0.740990 &  0.430191 &   0.440269 &  0.745492 &       NaN \\\\\n","            &        &         &    & 17 &  0.687784 &  0.422334 &     0.745492 &  0.431767 &   0.442068 &  0.746204 &       NaN \\\\\n","            &        &         &    & 18 &  0.686080 &  0.420054 &     0.746204 &  0.430895 &   0.441866 &  0.742690 &       NaN \\\\\n","            &        &         &    & 19 &  0.687784 &  0.422533 &     0.742690 &  0.431948 &   0.444559 &  0.744928 &       NaN \\\\\n","            &        &         &    & 20 &  0.687784 &  0.423742 &     0.744928 &  0.433805 &   0.445193 &  0.745199 &       NaN \\\\\n","            &        &         & 16 & 1  &  0.680398 &  0.298455 &     0.000000 &  0.333046 &   0.335860 &  0.722478 &       NaN \\\\\n","            &        &         &    & 2  &  0.699432 &  0.368435 &     0.722478 &  0.387629 &   0.399578 &  0.753809 &       NaN \\\\\n","            &        &         &    & 3  &  0.708807 &  0.384436 &     0.753809 &  0.402760 &   0.413103 &  0.766773 &       NaN \\\\\n","            &        &         &    & 4  &  0.684375 &  0.418164 &     0.766773 &  0.428660 &   0.442408 &  0.749853 &       NaN \\\\\n","            &        &         &    & 5  &  0.683523 &  0.430044 &     0.749853 &  0.446207 &   0.451276 &  0.746661 &       NaN \\\\\n","            &        &         &    & 6  &  0.683807 &  0.408569 &     0.746661 &  0.418145 &   0.441368 &  0.750965 &       NaN \\\\\n","            &        &         &    & 7  &  0.699432 &  0.393545 &     0.750965 &  0.407073 &   0.419870 &  0.761676 &       NaN \\\\\n","            &        &         &    & 8  &  0.688636 &  0.426405 &     0.761676 &  0.440079 &   0.448568 &  0.749683 &       NaN \\\\\n","            &        &         &    & 9  &  0.701136 &  0.416084 &     0.749683 &  0.421111 &   0.451039 &  0.758461 &       NaN \\\\\n","            &        &         &    & 10 &  0.694034 &  0.406195 &     0.758461 &  0.417611 &   0.432018 &  0.753446 &       NaN \\\\\n","            &        &         &    & 11 &  0.702557 &  0.408510 &     0.753446 &  0.416973 &   0.438744 &  0.755907 &       NaN \\\\\n","            &        &         &    & 12 &  0.701136 &  0.427204 &     0.755907 &  0.437404 &   0.457984 &  0.754561 &       NaN \\\\\n","            &        &         &    & 13 &  0.700284 &  0.435595 &     0.754561 &  0.441004 &   0.458425 &  0.755390 &       NaN \\\\\n","            &        &         &    & 14 &  0.700568 &  0.426520 &     0.755390 &  0.433722 &   0.448348 &  0.756194 &       NaN \\\\\n","            &        &         &    & 15 &  0.695739 &  0.432829 &     0.756194 &  0.438855 &   0.457301 &  0.751669 &       NaN \\\\\n","            &        &         &    & 16 &  0.697443 &  0.436462 &     0.751669 &  0.442716 &   0.462235 &  0.753802 &       NaN \\\\\n","            &        &         &    & 17 &  0.697727 &  0.441943 &     0.753802 &  0.447781 &   0.466177 &  0.753395 &       NaN \\\\\n","            &        &         &    & 18 &  0.699716 &  0.442440 &     0.753395 &  0.448501 &   0.467806 &  0.755786 &       NaN \\\\\n","            &        &         &    & 19 &  0.701989 &  0.446181 &     0.755786 &  0.451196 &   0.474123 &  0.758314 &       NaN \\\\\n","            &        &         &    & 20 &  0.702557 &  0.446744 &     0.758314 &  0.452415 &   0.474260 &  0.758123 &       NaN \\\\\n","            &        &         & 32 & 1  &  0.671875 &  0.295265 &     0.000000 &  0.326787 &   0.305898 &  0.711102 &       NaN \\\\\n","            &        &         &    & 2  &  0.700000 &  0.361216 &     0.711102 &  0.377033 &   0.394730 &  0.754627 &       NaN \\\\\n","            &        &         &    & 3  &  0.707386 &  0.389239 &     0.754627 &  0.401622 &   0.422711 &  0.767690 &       NaN \\\\\n","            &        &         &    & 4  &  0.682955 &  0.387243 &     0.767690 &  0.409583 &   0.416688 &  0.748691 &       NaN \\\\\n","            &        &         &    & 5  &  0.695455 &  0.368392 &     0.748691 &  0.382416 &   0.400509 &  0.753091 &       NaN \\\\\n","            &        &         &    & 6  &  0.705966 &  0.418661 &     0.753091 &  0.423656 &   0.453138 &  0.761754 &       NaN \\\\\n","            &        &         &    & 7  &  0.692898 &  0.412999 &     0.761754 &  0.423885 &   0.433348 &  0.753348 &       NaN \\\\\n","            &        &         &    & 8  &  0.689773 &  0.412704 &     0.753348 &  0.424586 &   0.448857 &  0.751120 &       NaN \\\\\n","            &        &         &    & 9  &  0.687500 &  0.416395 &     0.751120 &  0.428433 &   0.447318 &  0.751015 &       NaN \\\\\n","            &        &         &    & 10 &  0.690909 &  0.424270 &     0.751015 &  0.441495 &   0.442311 &  0.750275 &       NaN \\\\\n","            &        &         &    & 11 &  0.680114 &  0.405989 &     0.750275 &  0.422475 &   0.418922 &  0.742695 &       NaN \\\\\n","            &        &         &    & 12 &  0.688636 &  0.418778 &     0.742695 &  0.429242 &   0.448052 &  0.747968 &       NaN \\\\\n","            &        &         &    & 13 &  0.687500 &  0.423321 &     0.747968 &  0.435685 &   0.440652 &  0.746624 &       NaN \\\\\n","            &        &         &    & 14 &  0.695170 &  0.416535 &     0.746624 &  0.427938 &   0.434660 &  0.753386 &       NaN \\\\\n","            &        &         &    & 15 &  0.694034 &  0.428053 &     0.753386 &  0.437090 &   0.450571 &  0.752614 &       NaN \\\\\n","            &        &         &    & 16 &  0.698295 &  0.436386 &     0.752614 &  0.446091 &   0.456461 &  0.754044 &       NaN \\\\\n","            &        &         &    & 17 &  0.692898 &  0.423231 &     0.754044 &  0.433544 &   0.441928 &  0.750014 &       NaN \\\\\n","            &        &         &    & 18 &  0.693750 &  0.423995 &     0.750014 &  0.432796 &   0.445069 &  0.750219 &       NaN \\\\\n","            &        &         &    & 19 &  0.695455 &  0.423345 &     0.750219 &  0.430857 &   0.443656 &  0.750911 &       NaN \\\\\n","            &        &         &    & 20 &  0.694318 &  0.422312 &     0.750911 &  0.429635 &   0.442884 &  0.749460 &       NaN \\\\\n","            &        &         & 64 & 1  &  0.664205 &  0.226006 &     0.000000 &  0.282139 &   0.197639 &  0.665788 &  1.346646 \\\\\n","            &        &         &    & 2  &  0.672443 &  0.266992 &     0.665788 &  0.306558 &   0.289059 &  0.700221 &  0.904103 \\\\\n","            &        &         &    & 3  &  0.688636 &  0.319120 &     0.700221 &  0.341185 &   0.341541 &  0.737557 &  0.811013 \\\\\n","            &        &         &    & 4  &  0.702841 &  0.391096 &     0.737557 &  0.406705 &   0.427344 &  0.760267 &  0.718346 \\\\\n","            &        &         &    & 5  &  0.694602 &  0.423452 &     0.760267 &  0.431602 &   0.467448 &  0.756049 &  0.530614 \\\\\n","            &        &         &    & 6  &  0.699716 &  0.409914 &     0.756049 &  0.424160 &   0.424694 &  0.764579 &  0.377693 \\\\\n","            &        &         &    & 7  &  0.706818 &  0.407931 &     0.764579 &  0.416646 &   0.448291 &  0.769216 &  0.323412 \\\\\n","            &        &         &    & 8  &  0.699432 &  0.391672 &     0.769216 &  0.406580 &   0.423037 &  0.755703 &  0.246760 \\\\\n","            &        &         &    & 9  &  0.692330 &  0.394437 &     0.755703 &  0.404890 &   0.435533 &  0.752578 &  0.227526 \\\\\n","            &        &         &    & 10 &  0.694602 &  0.421195 &     0.752578 &  0.429175 &   0.451274 &  0.760784 &  0.202071 \\\\\n","            &        &         &    & 11 &  0.692045 &  0.425321 &     0.760784 &  0.435597 &   0.465460 &  0.753943 &  0.176851 \\\\\n","            &        &         &    & 12 &  0.684943 &  0.432901 &     0.753943 &  0.442906 &   0.463235 &  0.750108 &  0.156477 \\\\\n","            &        &         &    & 13 &  0.707670 &  0.435521 &     0.750108 &  0.441478 &   0.468936 &  0.766844 &  0.099715 \\\\\n","            &        &         &    & 14 &  0.698580 &  0.445490 &     0.766844 &  0.454865 &   0.478474 &  0.758729 &  0.062737 \\\\\n","            &        &         &    & 15 &  0.700284 &  0.443160 &     0.758729 &  0.452495 &   0.473713 &  0.760606 &  0.045645 \\\\\n","            &        &         &    & 16 &  0.702557 &  0.433478 &     0.760606 &  0.440242 &   0.457577 &  0.758887 &  0.022036 \\\\\n","            &        &         &    & 17 &  0.699432 &  0.440867 &     0.758887 &  0.445143 &   0.469428 &  0.756010 &  0.014645 \\\\\n","            &        &         &    & 18 &  0.700000 &  0.445033 &     0.756010 &  0.450451 &   0.473698 &  0.756908 &  0.010099 \\\\\n","            &        &         &    & 19 &  0.698011 &  0.446671 &     0.756908 &  0.451582 &   0.478436 &  0.755695 &  0.007593 \\\\\n","            &        &         &    & 20 &  0.696875 &  0.438682 &     0.755695 &  0.442427 &   0.473098 &  0.755032 &  0.006457 \\\\\n","            & pooler & 0.00001 & 8  & 1  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 2  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 3  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 4  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 5  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 6  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 7  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 8  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 9  &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 10 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 11 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 12 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 13 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 14 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 15 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 16 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 17 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 18 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 19 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         &    & 20 &       NaN &       NaN &          NaN &       NaN &        NaN &       NaN &       NaN \\\\\n","            &        &         & 16 & 1  &  0.684659 &  0.315357 &     0.000000 &  0.335568 &   0.330602 &  0.733411 &       NaN \\\\\n","            &        &         &    & 2  &  0.710795 &  0.437316 &     0.733411 &  0.446456 &   0.470488 &  0.766456 &       NaN \\\\\n","            &        &         &    & 3  &  0.706250 &  0.446603 &     0.766456 &  0.453207 &   0.465790 &  0.763926 &       NaN \\\\\n","            &        &         &    & 4  &  0.673864 &  0.428052 &     0.763926 &  0.456849 &   0.456554 &  0.745300 &       NaN \\\\\n","            &        &         &    & 5  &  0.697159 &  0.418894 &     0.745300 &  0.430037 &   0.465985 &  0.756899 &       NaN \\\\\n","            &        &         &    & 6  &  0.706250 &  0.410353 &     0.756899 &  0.418045 &   0.447754 &  0.766931 &       NaN \\\\\n","            &        &         &    & 7  &  0.706250 &  0.428800 &     0.766931 &  0.435184 &   0.453218 &  0.761623 &       NaN \\\\\n","            &        &         &    & 8  &  0.702841 &  0.456495 &     0.761623 &  0.462849 &   0.480946 &  0.756479 &       NaN \\\\\n","            &        &         &    & 9  &  0.694886 &  0.438787 &     0.756479 &  0.450755 &   0.453597 &  0.755616 &       NaN \\\\\n","            &        &         &    & 10 &  0.697159 &  0.459667 &     0.755616 &  0.468843 &   0.484674 &  0.756932 &       NaN \\\\\n","            &        &         &    & 11 &  0.696875 &  0.454788 &     0.756932 &  0.465941 &   0.482547 &  0.758694 &       NaN \\\\\n","            &        &         &    & 12 &  0.707955 &  0.464723 &     0.758694 &  0.472635 &   0.493068 &  0.763693 &       NaN \\\\\n","            &        &         &    & 13 &  0.707386 &  0.437768 &     0.763693 &  0.447041 &   0.464103 &  0.765081 &       NaN \\\\\n","            &        &         &    & 14 &  0.710511 &  0.447614 &     0.765081 &  0.458116 &   0.473606 &  0.766077 &       NaN \\\\\n","            &        &         &    & 15 &  0.706818 &  0.448954 &     0.766077 &  0.458273 &   0.476762 &  0.765969 &       NaN \\\\\n","            &        &         &    & 16 &  0.703693 &  0.436953 &     0.765969 &  0.445835 &   0.456410 &  0.761963 &       NaN \\\\\n","            &        &         &    & 17 &  0.701136 &  0.450840 &     0.761963 &  0.461097 &   0.474682 &  0.761109 &       NaN \\\\\n","            &        &         &    & 18 &  0.704261 &  0.449337 &     0.761109 &  0.460335 &   0.473366 &  0.763855 &       NaN \\\\\n","            &        &         &    & 19 &  0.703125 &  0.448979 &     0.763855 &  0.457816 &   0.472169 &  0.761925 &       NaN \\\\\n","            &        &         &    & 20 &  0.703977 &  0.453694 &     0.761925 &  0.462481 &   0.477337 &  0.762711 &       NaN \\\\\n","            &        &         & 32 & 1  &  0.667045 &  0.226561 &     0.000000 &  0.282792 &   0.195004 &  0.668017 &       NaN \\\\\n","            &        &         &    & 2  &  0.692045 &  0.334213 &     0.668017 &  0.354263 &   0.326624 &  0.746976 &       NaN \\\\\n","            &        &         &    & 3  &  0.711364 &  0.403007 &     0.746976 &  0.404933 &   0.453589 &  0.767749 &       NaN \\\\\n","            &        &         &    & 4  &  0.711648 &  0.420303 &     0.767749 &  0.421262 &   0.464443 &  0.770644 &       NaN \\\\\n","            &        &         &    & 5  &  0.706250 &  0.431256 &     0.770644 &  0.437149 &   0.467417 &  0.766696 &       NaN \\\\\n","            &        &         &    & 6  &  0.705966 &  0.410549 &     0.766696 &  0.417010 &   0.440707 &  0.766303 &       NaN \\\\\n","            &        &         &    & 7  &  0.696875 &  0.434834 &     0.766303 &  0.447035 &   0.462246 &  0.760697 &       NaN \\\\\n","            &        &         &    & 8  &  0.705114 &  0.417699 &     0.760697 &  0.426451 &   0.450092 &  0.765711 &       NaN \\\\\n","            &        &         &    & 9  &  0.701989 &  0.442524 &     0.765711 &  0.450228 &   0.478654 &  0.761464 &       NaN \\\\\n","            &        &         &    & 10 &  0.696023 &  0.442650 &     0.761464 &  0.455108 &   0.468625 &  0.755705 &       NaN \\\\\n","            &        &         &    & 11 &  0.702557 &  0.455445 &     0.755705 &  0.462203 &   0.483924 &  0.759304 &       NaN \\\\\n","            &        &         &    & 12 &  0.684375 &  0.444080 &     0.759304 &  0.458166 &   0.461506 &  0.749045 &       NaN \\\\\n","            &        &         &    & 13 &  0.671591 &  0.446731 &     0.749045 &  0.461794 &   0.477451 &  0.747017 &       NaN \\\\\n","            &        &         &    & 14 &  0.701705 &  0.433629 &     0.747017 &  0.441503 &   0.465338 &  0.763889 &       NaN \\\\\n","            &        &         &    & 15 &  0.702841 &  0.434195 &     0.763889 &  0.440831 &   0.466742 &  0.762732 &       NaN \\\\\n","            &        &         &    & 16 &  0.700000 &  0.448735 &     0.762732 &  0.453907 &   0.477411 &  0.758505 &       NaN \\\\\n","            &        &         &    & 17 &  0.711080 &  0.457755 &     0.758505 &  0.460753 &   0.492825 &  0.767153 &       NaN \\\\\n","            &        &         &    & 18 &  0.704261 &  0.453095 &     0.767153 &  0.457395 &   0.483059 &  0.761373 &       NaN \\\\\n","            &        &         &    & 19 &  0.707102 &  0.452183 &     0.761373 &  0.456019 &   0.483260 &  0.764211 &       NaN \\\\\n","            &        &         &    & 20 &  0.707955 &  0.451308 &     0.764211 &  0.454494 &   0.482848 &  0.764678 &       NaN \\\\\n","            &        &         & 64 & 1  &  0.667045 &  0.226601 &     0.000000 &  0.282816 &   0.195004 &  0.668013 &  1.302419 \\\\\n","            &        &         &    & 2  &  0.670739 &  0.234175 &     0.668013 &  0.286799 &   0.231125 &  0.676792 &  0.863735 \\\\\n","            &        &         &    & 3  &  0.686932 &  0.302915 &     0.676792 &  0.325826 &   0.319430 &  0.732966 &  0.815515 \\\\\n","            &        &         &    & 4  &  0.694886 &  0.332134 &     0.732966 &  0.349507 &   0.339275 &  0.748255 &  0.763141 \\\\\n","            &        &         &    & 5  &  0.698864 &  0.384301 &     0.748255 &  0.389318 &   0.426975 &  0.754217 &  0.677368 \\\\\n","            &        &         &    & 6  &  0.704545 &  0.405760 &     0.754217 &  0.407656 &   0.448993 &  0.760130 &  0.616013 \\\\\n","            &        &         &    & 7  &  0.708239 &  0.411344 &     0.760130 &  0.416939 &   0.452266 &  0.767386 &  0.547971 \\\\\n","            &        &         &    & 8  &  0.708807 &  0.413370 &     0.767386 &  0.416014 &   0.463595 &  0.767751 &  0.485706 \\\\\n","            &        &         &    & 9  &  0.708239 &  0.424478 &     0.767751 &  0.429894 &   0.461259 &  0.766661 &  0.410102 \\\\\n","            &        &         &    & 10 &  0.697443 &  0.431591 &     0.766661 &  0.436721 &   0.457477 &  0.758704 &  0.365459 \\\\\n","            &        &         &    & 11 &  0.697443 &  0.421316 &     0.758704 &  0.430622 &   0.451449 &  0.757800 &  0.339010 \\\\\n","            &        &         &    & 12 &  0.695455 &  0.429638 &     0.757800 &  0.438090 &   0.453507 &  0.756209 &  0.309910 \\\\\n","            &        &         &    & 13 &  0.703977 &  0.422482 &     0.756209 &  0.425414 &   0.460989 &  0.760962 &  0.281901 \\\\\n","            &        &         &    & 14 &  0.688636 &  0.428049 &     0.760962 &  0.442044 &   0.453377 &  0.750977 &  0.226282 \\\\\n","            &        &         &    & 15 &  0.703693 &  0.428635 &     0.750977 &  0.432996 &   0.462143 &  0.762335 &  0.270888 \\\\\n","            &        &         &    & 16 &  0.697443 &  0.438239 &     0.762335 &  0.446961 &   0.464606 &  0.755344 &  0.182847 \\\\\n","            &        &         &    & 17 &  0.698011 &  0.436353 &     0.755344 &  0.441963 &   0.458925 &  0.757192 &  0.170683 \\\\\n","            &        &         &    & 18 &  0.701705 &  0.429589 &     0.757192 &  0.433903 &   0.456799 &  0.759348 &  0.152592 \\\\\n","            &        &         &    & 19 &  0.701989 &  0.432688 &     0.759348 &  0.439553 &   0.455276 &  0.759004 &  0.141757 \\\\\n","            &        &         &    & 20 &  0.700284 &  0.434628 &     0.759004 &  0.441403 &   0.457017 &  0.757446 &  0.138339 \\\\\n","            &        & 0.00003 & 8  & 1  &  0.694602 &  0.374942 &     0.000000 &  0.392816 &   0.390997 &  0.756882 &  0.910976 \\\\\n","            &        &         &    & 2  &  0.669318 &  0.424186 &     0.756882 &  0.440263 &   0.446438 &  0.744147 &  0.606981 \\\\\n","            &        &         &    & 3  &  0.700568 &  0.392566 &     0.744147 &  0.412087 &   0.417716 &  0.758550 &  0.501802 \\\\\n","            &        &         &    & 4  &  0.690625 &  0.390649 &     0.758550 &  0.402694 &   0.419915 &  0.748792 &  0.357999 \\\\\n","            &        &         &    & 5  &  0.658807 &  0.408033 &     0.748792 &  0.436370 &   0.426010 &  0.735390 &  0.242368 \\\\\n","            &        &         &    & 6  &  0.695170 &  0.408600 &     0.735390 &  0.419462 &   0.427039 &  0.757305 &  0.186305 \\\\\n","            &        &         &    & 7  &  0.686648 &  0.381093 &     0.757305 &  0.396267 &   0.397000 &  0.747718 &  0.172951 \\\\\n","            &        &         &    & 8  &  0.682670 &  0.398773 &     0.747718 &  0.415665 &   0.414764 &  0.747663 &  0.150313 \\\\\n","            &        &         &    & 9  &  0.662500 &  0.400324 &     0.747663 &  0.421039 &   0.408427 &  0.732800 &  0.069255 \\\\\n","            &        &         &    & 10 &  0.685511 &  0.407787 &     0.732800 &  0.421398 &   0.419100 &  0.749124 &  0.046373 \\\\\n","            &        &         &    & 11 &  0.690625 &  0.413270 &     0.749124 &  0.427536 &   0.427440 &  0.751370 &  0.041826 \\\\\n","            &        &         &    & 12 &  0.685795 &  0.432589 &     0.751370 &  0.442647 &   0.454478 &  0.744420 &  0.047019 \\\\\n","            &        &         &    & 13 &  0.689205 &  0.427774 &     0.744420 &  0.434049 &   0.450832 &  0.746838 &  0.024811 \\\\\n","            &        &         &    & 14 &  0.686932 &  0.421787 &     0.746838 &  0.431194 &   0.443343 &  0.743851 &  0.012255 \\\\\n","            &        &         &    & 15 &  0.690909 &  0.414210 &     0.743851 &  0.422361 &   0.430198 &  0.747642 &  0.014482 \\\\\n","            &        &         &    & 16 &  0.694034 &  0.408884 &     0.747642 &  0.417139 &   0.424699 &  0.750097 &  0.004120 \\\\\n","            &        &         &    & 17 &  0.690057 &  0.414640 &     0.750097 &  0.422458 &   0.433135 &  0.746317 &  0.004691 \\\\\n","            &        &         &    & 18 &  0.690625 &  0.425284 &     0.746317 &  0.433074 &   0.448876 &  0.749688 &  0.003723 \\\\\n","            &        &         &    & 19 &  0.691761 &  0.422390 &     0.749688 &  0.430446 &   0.442947 &  0.748034 &  0.002597 \\\\\n","            &        &         &    & 20 &  0.692045 &  0.421888 &     0.748034 &  0.429618 &   0.440253 &  0.748594 &  0.002456 \\\\\n","            &        &         & 16 & 1  &  0.671875 &  0.283723 &     0.000000 &  0.329585 &   0.287662 &  0.703506 &  1.017502 \\\\\n","            &        &         &    & 2  &  0.681250 &  0.380583 &     0.703506 &  0.398441 &   0.387243 &  0.749466 &  0.681931 \\\\\n","            &        &         &    & 3  &  0.692614 &  0.378105 &     0.749466 &  0.393932 &   0.399572 &  0.757368 &  0.480591 \\\\\n","            &        &         &    & 4  &  0.688920 &  0.397386 &     0.757368 &  0.424864 &   0.402870 &  0.756320 &  0.376944 \\\\\n","            &        &         &    & 5  &  0.698864 &  0.398359 &     0.756320 &  0.416316 &   0.411445 &  0.758225 &  0.313608 \\\\\n","            &        &         &    & 6  &  0.682386 &  0.405933 &     0.758225 &  0.426538 &   0.413493 &  0.747026 &  0.201650 \\\\\n","            &        &         &    & 7  &  0.675000 &  0.407767 &     0.747026 &  0.436016 &   0.419310 &  0.744127 &  0.190277 \\\\\n","            &        &         &    & 8  &  0.672159 &  0.411418 &     0.744127 &  0.426398 &   0.440691 &  0.741075 &  0.182369 \\\\\n","            &        &         &    & 9  &  0.688068 &  0.404802 &     0.741075 &  0.415244 &   0.428506 &  0.751500 &  0.104643 \\\\\n","            &        &         &    & 10 &  0.701420 &  0.432292 &     0.751500 &  0.445693 &   0.446839 &  0.759029 &  0.052050 \\\\\n","            &        &         &    & 11 &  0.697443 &  0.415880 &     0.759029 &  0.432480 &   0.438434 &  0.755194 &  0.022599 \\\\\n","            &        &         &    & 12 &  0.700852 &  0.412187 &     0.755194 &  0.429286 &   0.423380 &  0.758664 &  0.021574 \\\\\n","            &        &         &    & 13 &  0.688352 &  0.395630 &     0.758664 &  0.413370 &   0.405824 &  0.748993 &  0.019328 \\\\\n","            &        &         &    & 14 &  0.697727 &  0.399701 &     0.748993 &  0.411540 &   0.410430 &  0.756676 &  0.021357 \\\\\n","            &        &         &    & 15 &  0.704261 &  0.415964 &     0.756676 &  0.423603 &   0.431500 &  0.760517 &  0.018305 \\\\\n","            &        &         &    & 16 &  0.702557 &  0.429011 &     0.760517 &  0.435756 &   0.447788 &  0.756263 &  0.013253 \\\\\n","            &        &         &    & 17 &  0.695455 &  0.420302 &     0.756263 &  0.427124 &   0.437298 &  0.751484 &  0.007636 \\\\\n","            &        &         &    & 18 &  0.697159 &  0.426430 &     0.751484 &  0.433356 &   0.448534 &  0.753356 &  0.006258 \\\\\n","            &        &         &    & 19 &  0.697727 &  0.430464 &     0.753356 &  0.435489 &   0.452882 &  0.753844 &  0.003707 \\\\\n","            &        &         &    & 20 &  0.697443 &  0.429785 &     0.753844 &  0.434749 &   0.452316 &  0.753627 &  0.002640 \\\\\n","            &        &         & 32 & 1  &  0.673864 &  0.278835 &     0.000000 &  0.309614 &   0.281381 &  0.703846 &  1.087540 \\\\\n","            &        &         &    & 2  &  0.692330 &  0.333149 &     0.703846 &  0.354468 &   0.358072 &  0.743455 &  0.744621 \\\\\n","            &        &         &    & 3  &  0.703693 &  0.371264 &     0.743455 &  0.384225 &   0.412196 &  0.763789 &  0.590557 \\\\\n","            &        &         &    & 4  &  0.706818 &  0.384234 &     0.763789 &  0.401845 &   0.405223 &  0.767323 &  0.468943 \\\\\n","            &        &         &    & 5  &  0.705682 &  0.396446 &     0.767323 &  0.412985 &   0.409958 &  0.760854 &  0.419484 \\\\\n","            &        &         &    & 6  &  0.676136 &  0.420697 &     0.760854 &  0.449217 &   0.445648 &  0.743929 &  0.305776 \\\\\n","            &        &         &    & 7  &  0.692330 &  0.425662 &     0.743929 &  0.442326 &   0.441103 &  0.754457 &  0.242671 \\\\\n","            &        &         &    & 8  &  0.682102 &  0.428380 &     0.754457 &  0.446725 &   0.447004 &  0.749513 &  0.157723 \\\\\n","            &        &         &    & 9  &  0.667045 &  0.418706 &     0.749513 &  0.441532 &   0.438266 &  0.740777 &  0.104805 \\\\\n","            &        &         &    & 10 &  0.676705 &  0.414945 &     0.740777 &  0.438131 &   0.431096 &  0.745720 &  0.128074 \\\\\n","            &        &         &    & 11 &  0.674716 &  0.403881 &     0.745720 &  0.430984 &   0.417263 &  0.739940 &  0.157102 \\\\\n","            &        &         &    & 12 &  0.681250 &  0.402465 &     0.739940 &  0.426237 &   0.413978 &  0.744410 &  0.172027 \\\\\n","            &        &         &    & 13 &  0.688920 &  0.396348 &     0.744410 &  0.407679 &   0.418812 &  0.753605 &  0.162815 \\\\\n","            &        &         &    & 14 &  0.699432 &  0.397077 &     0.753605 &  0.408697 &   0.418260 &  0.761622 &  0.124254 \\\\\n","            &        &         &    & 15 &  0.686648 &  0.415837 &     0.761622 &  0.424390 &   0.441424 &  0.747597 &  0.094842 \\\\\n","            &        &         &    & 16 &  0.698864 &  0.416069 &     0.747597 &  0.426261 &   0.437990 &  0.757812 &  0.044605 \\\\\n","            &        &         &    & 17 &  0.699716 &  0.427670 &     0.757812 &  0.437159 &   0.450188 &  0.757450 &  0.024654 \\\\\n","            &        &         &    & 18 &  0.700568 &  0.424968 &     0.757450 &  0.434695 &   0.445778 &  0.758389 &  0.018887 \\\\\n","            &        &         &    & 19 &  0.700852 &  0.430831 &     0.758389 &  0.440048 &   0.453574 &  0.758236 &  0.016428 \\\\\n","            &        &         &    & 20 &  0.700284 &  0.430283 &     0.758236 &  0.439785 &   0.452025 &  0.757598 &  0.011986 \\\\\n","            &        &         & 64 & 1  &  0.669034 &  0.222021 &     0.000000 &  0.277083 &   0.185890 &  0.669034 &  1.342941 \\\\\n","            &        &         &    & 2  &  0.675284 &  0.266282 &     0.669034 &  0.300813 &   0.279435 &  0.697940 &  0.827343 \\\\\n","            &        &         &    & 3  &  0.664489 &  0.344324 &     0.697940 &  0.368222 &   0.349941 &  0.729831 &  0.752816 \\\\\n","            &        &         &    & 4  &  0.700852 &  0.392832 &     0.729831 &  0.405610 &   0.419065 &  0.755112 &  0.734873 \\\\\n","            &        &         &    & 5  &  0.704261 &  0.388803 &     0.755112 &  0.399772 &   0.408593 &  0.762879 &  0.588699 \\\\\n","            &        &         &    & 6  &  0.711648 &  0.391810 &     0.762879 &  0.400263 &   0.417526 &  0.768930 &  0.486449 \\\\\n","            &        &         &    & 7  &  0.708807 &  0.403680 &     0.768930 &  0.411203 &   0.428761 &  0.767458 &  0.382907 \\\\\n","            &        &         &    & 8  &  0.682386 &  0.422467 &     0.767458 &  0.436797 &   0.445018 &  0.751894 &  0.289041 \\\\\n","            &        &         &    & 9  &  0.696591 &  0.401311 &     0.751894 &  0.416705 &   0.426709 &  0.756496 &  0.235298 \\\\\n","            &        &         &    & 10 &  0.704545 &  0.416051 &     0.756496 &  0.430802 &   0.442547 &  0.763801 &  0.240030 \\\\\n","            &        &         &    & 11 &  0.681534 &  0.410150 &     0.763801 &  0.430098 &   0.434197 &  0.747402 &  0.196173 \\\\\n","            &        &         &    & 12 &  0.685511 &  0.405994 &     0.747402 &  0.421467 &   0.423916 &  0.751237 &  0.164082 \\\\\n","            &        &         &    & 13 &  0.701420 &  0.430558 &     0.751237 &  0.436113 &   0.469041 &  0.759025 &  0.162715 \\\\\n","            &        &         &    & 14 &  0.694318 &  0.426089 &     0.759025 &  0.439755 &   0.450711 &  0.755559 &  0.147829 \\\\\n","            &        &         &    & 15 &  0.695455 &  0.435502 &     0.755559 &  0.444156 &   0.467312 &  0.756768 &  0.075476 \\\\\n","            &        &         &    & 16 &  0.713352 &  0.446749 &     0.756768 &  0.445788 &   0.488542 &  0.770458 &  0.051371 \\\\\n","            &        &         &    & 17 &  0.710227 &  0.443010 &     0.770458 &  0.443836 &   0.483378 &  0.766134 &  0.043091 \\\\\n","            &        &         &    & 18 &  0.707386 &  0.443817 &     0.766134 &  0.446179 &   0.484419 &  0.763970 &  0.029504 \\\\\n","            &        &         &    & 19 &  0.705398 &  0.447836 &     0.763970 &  0.450221 &   0.487626 &  0.763307 &  0.026090 \\\\\n","            &        &         &    & 20 &  0.703693 &  0.447088 &     0.763307 &  0.449524 &   0.485386 &  0.761777 &  0.021109 \\\\\n","            &        & 0.00005 & 8  & 1  &  0.667898 &  0.291763 &     0.000000 &  0.325117 &   0.296207 &  0.707127 &  0.980859 \\\\\n","            &        &         &    & 2  &  0.649716 &  0.362857 &     0.707127 &  0.393933 &   0.360040 &  0.727132 &  0.746650 \\\\\n","            &        &         &    & 3  &  0.684943 &  0.363907 &     0.727132 &  0.386020 &   0.364373 &  0.748698 &  0.598344 \\\\\n","            &        &         &    & 4  &  0.682955 &  0.353444 &     0.748698 &  0.377555 &   0.358873 &  0.740254 &  0.529013 \\\\\n","            &        &         &    & 5  &  0.696875 &  0.371913 &     0.740254 &  0.390344 &   0.385112 &  0.757260 &  0.491546 \\\\\n","            &        &         &    & 6  &  0.686648 &  0.381624 &     0.757260 &  0.401223 &   0.383045 &  0.743843 &  0.392842 \\\\\n","            &        &         &    & 7  &  0.648295 &  0.386250 &     0.743843 &  0.419273 &   0.391149 &  0.727009 &  0.350833 \\\\\n","            &        &         &    & 8  &  0.666477 &  0.374572 &     0.727009 &  0.400683 &   0.374804 &  0.729870 &  0.361424 \\\\\n","            &        &         &    & 9  &  0.675000 &  0.380067 &     0.729870 &  0.402877 &   0.386997 &  0.733509 &  0.344172 \\\\\n","            &        &         &    & 10 &  0.671307 &  0.381183 &     0.733509 &  0.407154 &   0.384334 &  0.732836 &  0.284031 \\\\\n","            &        &         &    & 11 &  0.677841 &  0.390787 &     0.732836 &  0.412180 &   0.404034 &  0.736180 &  0.255039 \\\\\n","            &        &         &    & 12 &  0.677841 &  0.379631 &     0.736180 &  0.404187 &   0.385499 &  0.737058 &  0.214885 \\\\\n","            &        &         &    & 13 &  0.668182 &  0.380272 &     0.737058 &  0.402146 &   0.393782 &  0.729224 &  0.185441 \\\\\n","            &        &         &    & 14 &  0.668182 &  0.388768 &     0.729224 &  0.404983 &   0.404112 &  0.728040 &  0.183169 \\\\\n","            &        &         &    & 15 &  0.662500 &  0.390884 &     0.728040 &  0.409461 &   0.405445 &  0.725973 &  0.170084 \\\\\n","            &        &         &    & 16 &  0.673580 &  0.397130 &     0.725973 &  0.411975 &   0.410584 &  0.731614 &  0.157378 \\\\\n","            &        &         &    & 17 &  0.680398 &  0.400567 &     0.731614 &  0.414452 &   0.415729 &  0.737132 &  0.152393 \\\\\n","            &        &         &    & 18 &  0.676989 &  0.396365 &     0.737132 &  0.411977 &   0.411022 &  0.735161 &  0.151063 \\\\\n","            &        &         &    & 19 &  0.679545 &  0.399850 &     0.735161 &  0.414424 &   0.415520 &  0.737538 &  0.144178 \\\\\n","            &        &         &    & 20 &  0.679261 &  0.399998 &     0.737538 &  0.413680 &   0.416325 &  0.736946 &  0.141019 \\\\\n","            &        &         & 16 & 1  &  0.672159 &  0.257925 &     0.000000 &  0.300245 &   0.249215 &  0.689323 &  1.042742 \\\\\n","            &        &         &    & 2  &  0.678409 &  0.325731 &     0.689323 &  0.356422 &   0.332865 &  0.729516 &  0.748411 \\\\\n","            &        &         &    & 3  &  0.694034 &  0.358187 &     0.729516 &  0.369764 &   0.371648 &  0.753084 &  0.567249 \\\\\n","            &        &         &    & 4  &  0.677273 &  0.380665 &     0.753084 &  0.407379 &   0.382621 &  0.748098 &  0.472782 \\\\\n","            &        &         &    & 5  &  0.676989 &  0.382533 &     0.748098 &  0.401487 &   0.399942 &  0.744842 &  0.345264 \\\\\n","            &        &         &    & 6  &  0.664205 &  0.390236 &     0.744842 &  0.416144 &   0.393544 &  0.736041 &  0.294501 \\\\\n","            &        &         &    & 7  &  0.644886 &  0.389082 &     0.736041 &  0.420021 &   0.405199 &  0.728691 &  0.265912 \\\\\n","            &        &         &    & 8  &  0.668182 &  0.412082 &     0.728691 &  0.440470 &   0.417759 &  0.734830 &  0.235636 \\\\\n","            &        &         &    & 9  &  0.660227 &  0.415745 &     0.734830 &  0.443714 &   0.427588 &  0.735514 &  0.215247 \\\\\n","            &        &         &    & 10 &  0.690057 &  0.411281 &     0.735514 &  0.425390 &   0.426181 &  0.751547 &  0.197066 \\\\\n","            &        &         &    & 11 &  0.675568 &  0.410312 &     0.751547 &  0.431729 &   0.416013 &  0.742029 &  0.121202 \\\\\n","            &        &         &    & 12 &  0.680398 &  0.417341 &     0.742029 &  0.437442 &   0.431065 &  0.745006 &  0.078498 \\\\\n","            &        &         &    & 13 &  0.686364 &  0.422262 &     0.745006 &  0.435553 &   0.440116 &  0.748991 &  0.064956 \\\\\n","            &        &         &    & 14 &  0.690909 &  0.411904 &     0.748991 &  0.421952 &   0.427335 &  0.750356 &  0.061138 \\\\\n","            &        &         &    & 15 &  0.675568 &  0.402588 &     0.750356 &  0.419669 &   0.415644 &  0.741847 &  0.056929 \\\\\n","            &        &         &    & 16 &  0.683807 &  0.408163 &     0.741847 &  0.421445 &   0.421527 &  0.747496 &  0.059033 \\\\\n","            &        &         &    & 17 &  0.685795 &  0.421595 &     0.747496 &  0.433674 &   0.437711 &  0.747105 &  0.044112 \\\\\n","            &        &         &    & 18 &  0.684091 &  0.408440 &     0.747105 &  0.421763 &   0.423251 &  0.747509 &  0.040110 \\\\\n","            &        &         &    & 19 &  0.684943 &  0.407502 &     0.747509 &  0.421191 &   0.419154 &  0.747878 &  0.038690 \\\\\n","            &        &         &    & 20 &  0.687500 &  0.413952 &     0.747878 &  0.424212 &   0.436273 &  0.750504 &  0.034745 \\\\\n","            &        &         & 32 & 1  &  0.671307 &  0.232212 &     0.000000 &  0.283307 &   0.200872 &  0.675517 &  1.102678 \\\\\n","            &        &         &    & 2  &  0.675000 &  0.275279 &     0.675517 &  0.315622 &   0.263890 &  0.699228 &  0.822549 \\\\\n","            &        &         &    & 3  &  0.691761 &  0.356363 &     0.699228 &  0.379069 &   0.362901 &  0.744013 &  0.724929 \\\\\n","            &        &         &    & 4  &  0.679545 &  0.369743 &     0.744013 &  0.392892 &   0.382770 &  0.746050 &  0.555405 \\\\\n","            &        &         &    & 5  &  0.692045 &  0.387817 &     0.746050 &  0.404064 &   0.406992 &  0.751034 &  0.460617 \\\\\n","            &        &         &    & 6  &  0.671307 &  0.379992 &     0.751034 &  0.405941 &   0.388881 &  0.745479 &  0.392693 \\\\\n","            &        &         &    & 7  &  0.674148 &  0.403640 &     0.745479 &  0.422802 &   0.417406 &  0.743288 &  0.347659 \\\\\n","            &        &         &    & 8  &  0.684091 &  0.397991 &     0.743288 &  0.414449 &   0.406156 &  0.747781 &  0.232957 \\\\\n","            &        &         &    & 9  &  0.680966 &  0.409056 &     0.747781 &  0.424644 &   0.428413 &  0.745888 &  0.172356 \\\\\n","            &        &         &    & 10 &  0.688352 &  0.412282 &     0.745888 &  0.429350 &   0.435802 &  0.752047 &  0.139320 \\\\\n","            &        &         &    & 11 &  0.682955 &  0.411996 &     0.752047 &  0.425940 &   0.426161 &  0.747996 &  0.125404 \\\\\n","            &        &         &    & 12 &  0.677841 &  0.417911 &     0.747996 &  0.437634 &   0.434112 &  0.747971 &  0.101716 \\\\\n","            &        &         &    & 13 &  0.684659 &  0.403756 &     0.747971 &  0.423878 &   0.424238 &  0.750970 &  0.103921 \\\\\n","            &        &         &    & 14 &  0.685795 &  0.392786 &     0.750970 &  0.406060 &   0.406082 &  0.750093 &  0.158181 \\\\\n","            &        &         &    & 15 &  0.678977 &  0.392480 &     0.750093 &  0.411677 &   0.401153 &  0.744921 &  0.117417 \\\\\n","            &        &         &    & 16 &  0.677841 &  0.407595 &     0.744921 &  0.424970 &   0.425993 &  0.747490 &  0.107862 \\\\\n","            &        &         &    & 17 &  0.671307 &  0.430268 &     0.747490 &  0.447720 &   0.448693 &  0.741660 &  0.064095 \\\\\n","            &        &         &    & 18 &  0.686080 &  0.417418 &     0.741660 &  0.426057 &   0.443061 &  0.749918 &  0.056754 \\\\\n","            &        &         &    & 19 &  0.682386 &  0.420719 &     0.749918 &  0.428974 &   0.449572 &  0.748546 &  0.040720 \\\\\n","            &        &         &    & 20 &  0.682102 &  0.416880 &     0.748546 &  0.426274 &   0.442056 &  0.747855 &  0.038028 \\\\\n","            &        &         & 64 & 1  &  0.669034 &  0.222021 &     0.000000 &  0.277083 &   0.185890 &  0.669034 &  1.342941 \\\\\n","            &        &         &    & 2  &  0.669034 &  0.222021 &     0.669034 &  0.277083 &   0.185890 &  0.669034 &  0.842273 \\\\\n","            &        &         &    & 3  &  0.669318 &  0.222708 &     0.669034 &  0.277431 &   0.190118 &  0.669817 &  0.841254 \\\\\n","            &        &         &    & 4  &  0.657102 &  0.283300 &     0.669817 &  0.326904 &   0.281874 &  0.693769 &  0.824898 \\\\\n","            &        &         &    & 5  &  0.684091 &  0.280383 &     0.693769 &  0.314171 &   0.292673 &  0.716953 &  0.798271 \\\\\n","            &        &         &    & 6  &  0.675000 &  0.339453 &     0.716953 &  0.365793 &   0.347339 &  0.729068 &  0.718342 \\\\\n","            &        &         &    & 7  &  0.687216 &  0.365421 &     0.729068 &  0.378923 &   0.379540 &  0.737681 &  0.638249 \\\\\n","            &        &         &    & 8  &  0.686364 &  0.394791 &     0.737681 &  0.410324 &   0.409551 &  0.745411 &  0.519521 \\\\\n","            &        &         &    & 9  &  0.684091 &  0.391380 &     0.745411 &  0.405403 &   0.409151 &  0.748959 &  0.413705 \\\\\n","            &        &         &    & 10 &  0.667045 &  0.384141 &     0.748959 &  0.407999 &   0.392647 &  0.742452 &  0.338004 \\\\\n","            &        &         &    & 11 &  0.632955 &  0.378396 &     0.742452 &  0.423744 &   0.393745 &  0.715081 &  0.320029 \\\\\n","            &        &         &    & 12 &  0.691761 &  0.375984 &     0.715081 &  0.396078 &   0.396002 &  0.757535 &  0.401518 \\\\\n","            &        &         &    & 13 &  0.681534 &  0.395861 &     0.757535 &  0.421755 &   0.419109 &  0.745496 &  0.375214 \\\\\n","            &        &         &    & 14 &  0.692614 &  0.386201 &     0.745496 &  0.402112 &   0.402563 &  0.754561 &  0.274045 \\\\\n","            &        &         &    & 15 &  0.680682 &  0.401499 &     0.754561 &  0.423401 &   0.409766 &  0.751552 &  0.215012 \\\\\n","            &        &         &    & 16 &  0.690341 &  0.414261 &     0.751552 &  0.428517 &   0.430052 &  0.752674 &  0.205362 \\\\\n","            &        &         &    & 17 &  0.690625 &  0.415757 &     0.752674 &  0.427383 &   0.433727 &  0.752244 &  0.136129 \\\\\n","            &        &         &    & 18 &  0.690341 &  0.412469 &     0.752244 &  0.426431 &   0.425709 &  0.753042 &  0.109672 \\\\\n","            &        &         &    & 19 &  0.689205 &  0.414175 &     0.753042 &  0.428855 &   0.425632 &  0.751826 &  0.093740 \\\\\n","            &        &         &    & 20 &  0.691477 &  0.421846 &     0.751826 &  0.435106 &   0.438013 &  0.753182 &  0.083850 \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\end{table}\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pR_Zy_eIcl2i"},"source":["# Inference"]},{"cell_type":"markdown","metadata":{"id":"Hx8CXkIr3NuW"},"source":["##Train the model with Full Train dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"suGc2RmyuPzk","executionInfo":{"status":"ok","timestamp":1619773850622,"user_tz":180,"elapsed":892,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"013c6baa-a81a-498d-bd1e-e5e7a6c4bef7"},"source":["## 10 Best resuts\n","MetricForBestResults = 'cem' if df_train['Label'].nunique() > 2 else 'f1'\n","DfResultsTask.nlargest(n=10, columns= MetricForBestResults )"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1_macro</th>\n","      <th>f1_weighted</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>cem</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"10\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"10\" valign=\"top\">hidden</th>\n","      <th rowspan=\"2\" valign=\"top\">0.00001</th>\n","      <th>16</th>\n","      <th>4</th>\n","      <td>0.717045</td>\n","      <td>0.403550</td>\n","      <td>0.767770</td>\n","      <td>0.409123</td>\n","      <td>0.446873</td>\n","      <td>0.776860</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <th>3</th>\n","      <td>0.716477</td>\n","      <td>0.413840</td>\n","      <td>0.769625</td>\n","      <td>0.415084</td>\n","      <td>0.461104</td>\n","      <td>0.774677</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">0.00003</th>\n","      <th>32</th>\n","      <th>6</th>\n","      <td>0.718750</td>\n","      <td>0.409604</td>\n","      <td>0.748270</td>\n","      <td>0.417309</td>\n","      <td>0.435515</td>\n","      <td>0.774593</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <th>5</th>\n","      <td>0.714773</td>\n","      <td>0.417772</td>\n","      <td>0.763167</td>\n","      <td>0.423512</td>\n","      <td>0.446093</td>\n","      <td>0.774551</td>\n","      <td>0.541801</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"3\" valign=\"top\">8</th>\n","      <th>5</th>\n","      <td>0.715341</td>\n","      <td>0.416753</td>\n","      <td>0.764872</td>\n","      <td>0.421896</td>\n","      <td>0.459225</td>\n","      <td>0.773905</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.717614</td>\n","      <td>0.450320</td>\n","      <td>0.770964</td>\n","      <td>0.452513</td>\n","      <td>0.482516</td>\n","      <td>0.772752</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.717898</td>\n","      <td>0.451593</td>\n","      <td>0.771236</td>\n","      <td>0.452176</td>\n","      <td>0.486291</td>\n","      <td>0.772296</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0.00005</th>\n","      <th>8</th>\n","      <th>2</th>\n","      <td>0.708807</td>\n","      <td>0.413074</td>\n","      <td>0.754429</td>\n","      <td>0.431097</td>\n","      <td>0.465479</td>\n","      <td>0.771905</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"2\" valign=\"top\">8</th>\n","      <th>17</th>\n","      <td>0.716761</td>\n","      <td>0.449948</td>\n","      <td>0.772296</td>\n","      <td>0.451223</td>\n","      <td>0.484324</td>\n","      <td>0.771558</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.710227</td>\n","      <td>0.423075</td>\n","      <td>0.764952</td>\n","      <td>0.431958</td>\n","      <td>0.454768</td>\n","      <td>0.771399</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  accuracy  f1_macro  ...       cem      loss\n","SpanishBert hidden 0.00001 16 4   0.717045  0.403550  ...  0.776860       NaN\n","                           8  3   0.716477  0.413840  ...  0.774677       NaN\n","                   0.00003 32 6   0.718750  0.409604  ...  0.774593       NaN\n","                           64 5   0.714773  0.417772  ...  0.774551  0.541801\n","                   0.00001 8  5   0.715341  0.416753  ...  0.773905       NaN\n","                              20  0.717614  0.450320  ...  0.772752       NaN\n","                              16  0.717898  0.451593  ...  0.772296       NaN\n","                   0.00005 8  2   0.708807  0.413074  ...  0.771905       NaN\n","                   0.00001 8  17  0.716761  0.449948  ...  0.771558       NaN\n","                              8   0.710227  0.423075  ...  0.771399       NaN\n","\n","[10 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9Dh28hHvWw8","executionInfo":{"status":"ok","timestamp":1619773868806,"user_tz":180,"elapsed":503,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"7a85d042-d36a-4949-867e-e31ab880a160"},"source":["## Get best parameters from cross-validation DataFrame \n","BestResultParameters = DfResultsTask.sort_values(MetricForBestResults, ascending=False)[:1].index\n","print(f'Best parameters : {BestResultParameters}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best parameters : MultiIndex([('SpanishBert', 'hidden', 1e-05, 16, 4)],\n","           )\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-2gzpiLswJOL"},"source":["## Add best parameters to variables in the final train\n","BertPath = BertVersion[BestResultParameters[0][0]]\n","BertVersion = {BestResultParameters[0][0] : BertVersion[BestResultParameters[0][0]]}\n","OutputBert = [BestResultParameters[0][1]]\n","LearningRate = [float(BestResultParameters[0][2])]\n","BatchSize = [int(BestResultParameters[0][3])]\n","Epochs = int(BestResultParameters[0][4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8BPtlIz7ZXW"},"source":["## Criate dictinaril results\n","ResultsTaskBestParameters = { bert:{ output:{ lr:{ bat:{ epoc:{ metric:[] for metric in Metrics + ['loss']} for epoc in range(1, Epochs+1) } for bat in BatchSize} for lr in LearningRate} for output in OutputBert } for bert in BertVersion.keys() }\n","\n","## Create file to save results BEST Parameters\n","#### Create file name\n","FileResultsBestModel = FileResults + 'BestModel'\n","#### Save the file fro results BEST Parameters\n","with open(Path + FileResultsBestModel + \".pkl\",'wb') as f:\n","  pickle.dump(ResultsTaskBestParameters, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7e2YUAfxufW","executionInfo":{"status":"ok","timestamp":1619774084429,"user_tz":180,"elapsed":188517,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"d95a64f5-3d2b-42fe-bbe2-077092aa77c5"},"source":["## Train with Best parameters\n","\n","## Best parameters\n","BertV = BestResultParameters[0][0]\n","BertPath = BertVersion[BestResultParameters[0][0]]\n","OutputB = OutputBert[0]\n","lr = LearningRate[0]\n","Batch = BatchSize[0]\n","Epochs = Epochs\n","\n","### Loading Bert trained weights\n","mx = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique())\n","\n","## Split train and test\n","X_train = df_train['Data']\n","y_train = df_train['Label']\n","_, X_test, _, y_test = train_test_split(df_train['Data'], df_train['Label'], test_size=0.33, random_state=42)\n","\n","print(f'parameters: Bertmodel: {BertV}, Output: {OutputB}, lr: {lr}, Batch: {Batch}, Totsl Num. Epochs: {Epochs}')\n","MoDeL = TrainModel(PathSaveFiles = Path,\n","                  BertVersion=BertV,\n","                  BertPath=BertPath,\n","                  OutputBert=OutputB,\n","                  LearningRate=lr,\n","                  BatchSize=Batch,\n","                  Epochs=Epochs,\n","                  FileName= FileResultsBestModel,\n","                  X_train=X_train, \n","                  X_valid=X_test,\n","                  y_train=y_train,\n","                  y_valid=y_test,\n","                  SaveModel=True)\n","\n","\n","def _mp_fn(rank, flags):\n","  torch.set_default_tensor_type('torch.FloatTensor')\n","  a = MoDeL._run()\n","\n","FLAGS={}\n","xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ were not used when initializing BertModel: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["parameters: Bertmodel: SpanishBert, Output: hidden, lr: 1e-05, Batch: 16, Totsl Num. Epochs: 4\n","num_train_steps = 108, world_size=8\n","Epoch: 1 of 4\n","bi=0, loss=1.690983772277832\n","bi=10, loss=0.8270350694656372\n","bi=20, loss=1.0059112310409546\n","Accuracy = 0.701048951048951\n","Epoch: 2 of 4\n","bi=0, loss=0.8150615096092224\n","bi=10, loss=0.7906311750411987\n","bi=20, loss=0.6265846490859985\n","Accuracy = 0.7736013986013985\n","Epoch: 3 of 4\n","bi=0, loss=0.7270472049713135\n","bi=10, loss=0.5866357088088989\n","bi=20, loss=0.547871470451355\n","Accuracy = 0.8155594405594406\n","Epoch: 4 of 4\n","bi=0, loss=0.550194263458252\n","bi=10, loss=0.5447127819061279\n","bi=20, loss=0.5726341605186462\n","Accuracy = 0.8374125874125874\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kC7Qio7rfXpM"},"source":["## Average and Save Results\n","AverageResultsTaskBestModel = AveragResults(FileName=FileResultsBestModel, Path=Path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"1sjtPlRWfhOU","executionInfo":{"status":"ok","timestamp":1619774127261,"user_tz":180,"elapsed":527,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"218a449a-5880-4f1d-ffd3-160af2964272"},"source":["## Create a Data Frame\n","DfResultsTaskBestModel = create_Data_Frame(all_resultas=AverageResultsTaskBestModel)\n","\n","### save results to a CSV file\n","DfResultsTaskBestModel.to_csv(Path + 'Average' + FileResultsBestModel + '_CSV_' + '.csv')\n","\n","### See the Avarage results in the Pandas data Frame\n","DfResultsTaskBestModel"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1_macro</th>\n","      <th>f1_weighted</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>cem</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"4\" valign=\"top\">hidden</th>\n","      <th rowspan=\"4\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"4\" valign=\"top\">16</th>\n","      <th>1</th>\n","      <td>0.701049</td>\n","      <td>0.273298</td>\n","      <td>0.000000</td>\n","      <td>0.291250</td>\n","      <td>0.320689</td>\n","      <td>0.751806</td>\n","      <td>1.076546</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.773601</td>\n","      <td>0.456156</td>\n","      <td>0.751806</td>\n","      <td>0.444534</td>\n","      <td>0.488135</td>\n","      <td>0.827710</td>\n","      <td>0.674776</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.815559</td>\n","      <td>0.515485</td>\n","      <td>0.827710</td>\n","      <td>0.516504</td>\n","      <td>0.524467</td>\n","      <td>0.862626</td>\n","      <td>0.592203</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.837413</td>\n","      <td>0.559376</td>\n","      <td>0.862626</td>\n","      <td>0.540827</td>\n","      <td>0.617811</td>\n","      <td>0.885832</td>\n","      <td>0.507718</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 accuracy  f1_macro  ...       cem      loss\n","SpanishBert hidden 0.00001 16 1  0.701049  0.273298  ...  0.751806  1.076546\n","                              2  0.773601  0.456156  ...  0.827710  0.674776\n","                              3  0.815559  0.515485  ...  0.862626  0.592203\n","                              4  0.837413  0.559376  ...  0.885832  0.507718\n","\n","[4 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Iei-0sLc3QRI"},"source":["## Inference on Test Dataset"]},{"cell_type":"markdown","metadata":{"id":"TdgiDIeC4XbB"},"source":["###Load data"]},{"cell_type":"code","metadata":{"id":"AJb7MHXj4XbC"},"source":["PathDataSet = \"../content/drive/MyDrive/Code/DETOXIS/Data/test.csv\"\n","df_test = pd.read_csv(PathDataSet, usecols=[\"comment_id\",\"comment\"]).fillna(\"none\")\n","NewColumnsNames = {\"comment\":\"Data\"}\n","df_test = df_test.rename(columns=NewColumnsNames)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5h4GIZCiqEV"},"source":["class BERTDatasetTest:\n","    def __init__(self, comment_text, tokenizer, max_length):\n","        self.comment_text = comment_text\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, item):\n","        comment_text = str(self.comment_text[item])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            truncation=True,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_length = self.max_length - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVIuyw9PiqIC"},"source":["## Bert tozenizer\n","tokenizer = transformers.BertTokenizer.from_pretrained(BertPath, do_lower_case=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-wbaFWcitwr","executionInfo":{"status":"ok","timestamp":1619777301857,"user_tz":180,"elapsed":13660,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"5c113765-1162-43a2-972e-115be6e73e66"},"source":["## Loading the best model\n","device = torch.device(\"xla\")\n","model = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique()).to(device)\n","FileBestModel = Path + FileResultsBestModel + '.bin'\n","model.load_state_dict(torch.load(FileBestModel))\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ were not used when initializing BertModel: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BERTBaseUncased(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (bert_drop): Dropout(p=0.3, inplace=False)\n","  (OutPutHidden): Linear(in_features=1536, out_features=4, bias=True)\n","  (OutPoller): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"fZwzG9CD3De_"},"source":["## Prepresing the data\n","valid_dataset = BERTDatasetTest(\n","        comment_text=df_test['Data'].values,\n","        tokenizer=tokenizer,\n","        max_length=192\n",")\n","\n","valid_data_loader = torch.utils.data.DataLoader(\n","    valid_dataset,\n","    batch_size=Batch,\n","    drop_last=False,\n","    num_workers=4,\n","    shuffle=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r70_1m-3Dhh","executionInfo":{"status":"ok","timestamp":1619777313509,"user_tz":180,"elapsed":4800,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"f2e359fd-a03d-4aa3-acef-0b5728184a32"},"source":["## Making the Inferences\n","with torch.no_grad():\n","    fin_outputs = []\n","    for bi, d in tqdm(enumerate(valid_data_loader)):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","\n","        outputs = model(\n","            ids=ids,\n","            mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","\n","        outputs_np = outputs.detach().cpu().numpy().tolist()\n","        fin_outputs.extend(outputs_np) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["56it [00:04, 13.99it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"BeS8ujZQApyS","executionInfo":{"status":"ok","timestamp":1619777316661,"user_tz":180,"elapsed":501,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"195c7bce-d01f-48d7-b8ae-b27e73a6caed"},"source":["## List with Results\n","fin_outputs\n","\n","## create a Dataframe from List of Results\n","df_results = pd.DataFrame.from_records(fin_outputs)\n","\n","## get the model inference\n","df_results['Inference'] = df_results.idxmax(axis=1)\n","\n","## Visualize results\n","df_results.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.658062</td>\n","      <td>0.943345</td>\n","      <td>-1.198898</td>\n","      <td>-2.257798</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.619709</td>\n","      <td>0.493857</td>\n","      <td>-1.665284</td>\n","      <td>-2.801999</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.769244</td>\n","      <td>0.947111</td>\n","      <td>-0.199099</td>\n","      <td>-2.062467</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.175360</td>\n","      <td>0.991688</td>\n","      <td>-0.431575</td>\n","      <td>-2.116301</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.066046</td>\n","      <td>1.194659</td>\n","      <td>-0.406725</td>\n","      <td>-2.346515</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0         1         2         3  Inference\n","0  1.658062  0.943345 -1.198898 -2.257798          0\n","1  2.619709  0.493857 -1.665284 -2.801999          0\n","2 -0.769244  0.947111 -0.199099 -2.062467          1\n","3  0.175360  0.991688 -0.431575 -2.116301          1\n","4  1.066046  1.194659 -0.406725 -2.346515          1"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"LHcAnfzkMO-q","executionInfo":{"status":"ok","timestamp":1619777322914,"user_tz":180,"elapsed":506,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"403dce1a-f804-49b5-ba56-cb66d05b4ba0"},"source":["## Get rows index\n","df_idex = df_test.loc[:,\"comment_id\"].to_frame()\n","\n","## Add index to the Results dataframe\n","df_results = df_results.join(df_idex)\n","\n","### save results to a CSV file\n","df_results.to_csv(Path + 'ModelInfereneces' + FileResultsBestModel + '_CSV_' + '.csv')\n","\n","## ## Visualize results\n","df_results.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>Inference</th>\n","      <th>comment_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.658062</td>\n","      <td>0.943345</td>\n","      <td>-1.198898</td>\n","      <td>-2.257798</td>\n","      <td>0</td>\n","      <td>10_001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.619709</td>\n","      <td>0.493857</td>\n","      <td>-1.665284</td>\n","      <td>-2.801999</td>\n","      <td>0</td>\n","      <td>10_002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.769244</td>\n","      <td>0.947111</td>\n","      <td>-0.199099</td>\n","      <td>-2.062467</td>\n","      <td>1</td>\n","      <td>10_003</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.175360</td>\n","      <td>0.991688</td>\n","      <td>-0.431575</td>\n","      <td>-2.116301</td>\n","      <td>1</td>\n","      <td>10_004</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.066046</td>\n","      <td>1.194659</td>\n","      <td>-0.406725</td>\n","      <td>-2.346515</td>\n","      <td>1</td>\n","      <td>10_005</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0         1         2         3  Inference comment_id\n","0  1.658062  0.943345 -1.198898 -2.257798          0     10_001\n","1  2.619709  0.493857 -1.665284 -2.801999          0     10_002\n","2 -0.769244  0.947111 -0.199099 -2.062467          1     10_003\n","3  0.175360  0.991688 -0.431575 -2.116301          1     10_004\n","4  1.066046  1.194659 -0.406725 -2.346515          1     10_005"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"rn24j0HBIUfD","executionInfo":{"status":"ok","timestamp":1619777330316,"user_tz":180,"elapsed":624,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"f8032963-e57e-4f76-9cc9-18dd0bd54ed9"},"source":["# Change the data to the DETOXIS format submition\n","### cerate a data frame only with the labels and ids\n","df_SubmationResults = df_results.loc[:, ['Inference', 'comment_id']]\n","df_SubmationResults.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Inference</th>\n","      <th>comment_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>10_001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>10_002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>10_003</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>10_004</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>10_005</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Inference comment_id\n","0          0     10_001\n","1          0     10_002\n","2          1     10_003\n","3          1     10_004\n","4          1     10_005"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"VTgjyaYvJk_5","executionInfo":{"status":"ok","timestamp":1619777334146,"user_tz":180,"elapsed":509,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"1f6aab88-5538-4a52-cdf9-dc907997589d"},"source":["## create a new id column\n","df_SubmationResults['id'] = np.arange(len(df_SubmationResults))\n","## removing olde id comment_id column\n","df_SubmationResults = df_SubmationResults.loc[:,['id', 'Inference']]\n","#submation format\n","df_SubmationResults.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference\n","0   0          0\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          1"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"code","metadata":{"id":"lJsKqLRYxE3j"},"source":["## saive inferences as .tsv\n","Path = 'drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/'\n","FileName = 'AI-UPV_subtask2_1'\n","df_SubmationResults.to_csv( Path + FileName + '.tsv', header=False, sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAAfHs5FZd7Q"},"source":["# Reviewing results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"aCjcopNlE_1L","executionInfo":{"status":"ok","timestamp":1619777336952,"user_tz":180,"elapsed":516,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"d458b795-672e-4c4b-8d49-492962fda3cb"},"source":["df = pd.read_csv(Path + 'Submited_' + FileName + '.tsv', header=None, sep='\\t')\n","df = df.rename({0:'id', 1:'Inference'}, axis=1)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference\n","0   0          1\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          1"]},"metadata":{"tags":[]},"execution_count":127}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"KoRuAYAVJuVc","executionInfo":{"status":"ok","timestamp":1619777339313,"user_tz":180,"elapsed":598,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"7b1db9a1-818b-47b3-d387-a1529a464111"},"source":["df['Submited_Inference'] = df_SubmationResults['Inference']\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","      <th>Submited_Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference  Submited_Inference\n","0   0          1                   0\n","1   1          0                   0\n","2   2          1                   1\n","3   3          1                   1\n","4   4          1                   1"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"ovXgMFHDLAHV","executionInfo":{"status":"ok","timestamp":1619777342541,"user_tz":180,"elapsed":533,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"55929f24-eab7-4354-bb57-7c767cbd14e6"},"source":["df['check'] = df.apply(lambda x: 1 if x.Inference != x.Submited_Inference else 0, axis=1)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","      <th>Submited_Inference</th>\n","      <th>check</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference  Submited_Inference  check\n","0   0          1                   0      1\n","1   1          0                   0      0\n","2   2          1                   1      0\n","3   3          1                   1      0\n","4   4          1                   1      0"]},"metadata":{"tags":[]},"execution_count":129}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBda5EoLL6U3","executionInfo":{"status":"ok","timestamp":1619777347435,"user_tz":180,"elapsed":572,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"42031e8b-a140-4e75-e302-b1656da3ec94"},"source":["df.check.sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["112"]},"metadata":{"tags":[]},"execution_count":130}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMntv6YbKiqS","executionInfo":{"status":"ok","timestamp":1619775877374,"user_tz":180,"elapsed":494,"user":{"displayName":"Angel Felipe Magnoss√£o de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"fb668a05-18b3-491f-db15-7955fb94c324"},"source":["# df_SubmationResults['Inference'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2])"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"ZexzKD0k8nN5"},"source":["# Util when the process stops sandly"]},{"cell_type":"code","metadata":{"id":"EyR_w29Jl71U"},"source":["# import pickle\n","# with open('drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/SpanishBertTask2Results' + \".pkl\", \"rb\") as f:\n","#   Re = pickle.load(f)\n","# Re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-eBqL2fl1Ci"},"source":["# def CleanBrokeTrain(FileName, Path, NumberOfFoldes=10):\n","#   with open(Path + FileName + \".pkl\", \"rb\") as f:\n","#               Results = pickle.load(f)\n","\n","#   for BT, ModelBertType,  in Results.items():\n","#     for OP, OutPut in ModelBertType.items():\n","#       for LR, LearningRate in OutPut.items():\n","#         for BS, BatchSize in LearningRate.items():\n","#           for EP, Epoch in BatchSize.items():\n","#             for Metrics, ValuesCrossValidation in  Epoch.items():\n"," \n","#               if len(ValuesCrossValidation) != 0 and not len(ValuesCrossValidation) == NumberOfFoldes:\n","#                 Results[BT][OP][LR][BS][EP][Metrics] = []\n","            \n","#   with open(FileName + '.pkl','wb') as f:\n","#     pickle.dump(Results, f)\n","\n","#   with open(Path + FileName + '.pkl','wb') as f:\n","#     pickle.dump(Results, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CpYDTBzcXt8"},"source":["# Path = 'drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/'\n","# File = 'SpanishBertTask2Results'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mmbpB-nzzBhE"},"source":["# CleanBrokeTrain(FileName=File, Path=Path, NumberOfFoldes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5T-xnlXctx4"},"source":["# import pickle\n","# with open('drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/SpanishBertTask2Results' + \".pkl\", \"rb\") as f:\n","#   RE = pickle.load(f)\n","# RE"],"execution_count":null,"outputs":[]}]}