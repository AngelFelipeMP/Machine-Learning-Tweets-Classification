{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"[Offical Task 1 code]  bert-base-spanish-wwm-uncased.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOeW/+YgdgdtTua/0RhK5Zw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U94-LXc92z44"},"source":["# Downloading Dependences"]},{"cell_type":"code","metadata":{"id":"ELwqFdCntJfH"},"source":["# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcnNlFEcvTw0"},"source":["# !apt-get install git-lfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HXbKnk1fO_b"},"source":["# !git lfs install\n","# !git clone https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEv-emIkgeLm"},"source":["# !git lfs install\n","# !git clone https://huggingface.co/bert-base-multilingual-uncased"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VK_nUhVyxjWy"},"source":["# !pip install transformers==3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjFmFhWd2hq1"},"source":["# Load Dependences"]},{"cell_type":"code","metadata":{"id":"8EGoAnTU2BNc","executionInfo":{"status":"ok","timestamp":1619785023111,"user_tz":180,"elapsed":3072,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["### add NLP dependences\n","import pickle\n","import os\n","import torch\n","import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","from sklearn import metrics\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","\n","from tqdm import tqdm\n","from collections import OrderedDict, namedtuple\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","import joblib\n","\n","import logging\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","import sys\n","from sklearn import metrics, model_selection\n","\n","import warnings\n","import torch_xla\n","import torch_xla.debug.metrics as met\n","import torch_xla.distributed.data_parallel as dp\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.utils.utils as xu\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.test.test_utils as test_utils\n","import warnings\n","\n","from torch_xla.core.xla_model import mesh_reduce\n","\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOH2Op99s4XU","executionInfo":{"status":"ok","timestamp":1619785025300,"user_tz":180,"elapsed":543,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"9bf3ea4a-fb72-4549-8547-d3e5e3254205"},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jd3akA1t3l_R"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"gDbmObs7ZGnx","executionInfo":{"status":"ok","timestamp":1619785027672,"user_tz":180,"elapsed":616,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["class AverageMeter:\n","    \"\"\"\n","    Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pz-cjmTkHXGE","executionInfo":{"status":"ok","timestamp":1619785031711,"user_tz":180,"elapsed":632,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["class BERTBaseUncased(nn.Module):\n","    def __init__(self, bert_path, output_bert='pooler', NumberOfClasses=2):\n","        super(BERTBaseUncased, self).__init__()\n","        self.bert_path = bert_path\n","        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n","        self.bert_drop = nn.Dropout(0.3)\n","        self.out_last_hidden = nn.Linear(768 * 2, 1)\n","        self.out_poller = nn.Linear(768, 1)\n","        self.output_bert = output_bert\n","\n","        self.NumberOfClasses = NumberOfClasses\n","        self.out_last_hidden_MultiClasss = nn.Linear(768 * 2, NumberOfClasses)\n","        self.out_poller_MultiClasss = nn.Linear(768, NumberOfClasses)\n","\n","\n","    def forward(\n","            self,\n","            ids,\n","            mask,\n","            token_type_ids\n","    ):\n","        o1, o2 = self.bert(\n","            ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids)\n","          \n","        if self.output_bert=='hidden':\n","          apool = torch.mean(o1, 1)\n","          mpool, _ = torch.max(o1, 1)\n","          cat = torch.cat((apool, mpool), 1)\n","          bo = self.bert_drop(cat)\n","\n","          if self.NumberOfClasses > 2:\n","            output = self.out_last_hidden_MultiClasss(bo)\n","          else:\n","            output = self.out_last_hidden(bo)\n","\n","        else:\n","          bo = self.bert_drop(o2)\n","\n","          if self.NumberOfClasses > 2:\n","            output = self.out_poller_MultiClasss(bo)\n"," \n","          else:\n","            output = self.out_poller(bo)\n","        \n","        return output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"5a6lgUAKZrA-","executionInfo":{"status":"ok","timestamp":1619785034480,"user_tz":180,"elapsed":537,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["class BERTDatasetTraining:\n","    def __init__(self, comment, targets, tokenizer, max_length):\n","        self.comment = comment\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.comment)\n","\n","    def __getitem__(self, item):\n","        comment = str(self.comment[item])\n","        comment = \" \".join(comment.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment,\n","            None,\n","            truncation=True,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_length = self.max_length - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n","        }"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0HJ8r3Uapyb","executionInfo":{"status":"ok","timestamp":1619785036004,"user_tz":180,"elapsed":819,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["#### Handcraft metric for multiclass evaluation\n","def cem_metric(target, output):\n","  conf_metrix = metrics.confusion_matrix(target, output)\n","  cem_metrix = np.zeros(conf_metrix.shape)\n","\n","  for column in range(conf_metrix.shape[1]):\n","    for row in range(conf_metrix.shape[0]):\n","    \n","      if row == column :\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2)/conf_metrix.sum()\n","                                          \n","      elif row < column:\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2 + conf_metrix.sum(axis=0)[row:column].sum())/conf_metrix.sum()\n","\n","      elif row > column:\n","        cem_metrix[row,column] = (conf_metrix.sum(axis=0)[column]/2 + conf_metrix.sum(axis=0)[column+1:row+1].sum())/conf_metrix.sum()\n","\n","  cem_metrix= - np.log2( np.where(cem_metrix !=0, cem_metrix, cem_metrix+0000000.1 ))\n","\n","  return np.sum(cem_metrix * conf_metrix.T) / np.sum( np.diag(cem_metrix) * conf_metrix.sum(axis=0))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLmqHi5aPf7b","executionInfo":{"status":"ok","timestamp":1619785038361,"user_tz":180,"elapsed":1212,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["class TrainModel():\n","  def __init__(self, PathSaveFiles, BertVersion, BertPath,  OutputBert, LearningRate, BatchSize, Epochs, FileName, X_train, X_valid, y_train ,y_valid, MaxLen = 192, SaveModel=False):\n","    self.BertVersion = BertVersion\n","    self.BertPath = BertPath\n","    self.OutputBert = OutputBert\n","    self.LearningRate = LearningRate\n","    self.BatchSize = BatchSize\n","    self.Epochs = Epochs\n","    self.FileName = FileName\n","    self.X_train = X_train\n","    self.X_valid = X_valid\n","    self.y_train = y_train\n","    self.y_valid = y_valid\n","    self.NumberOfLabels = y_train.nunique()\n","    self.average_metrics =  'macro' if self.NumberOfLabels > 2 else 'binary'\n","    self.PathSaveFiles = PathSaveFiles\n","    self.MaxLen = MaxLen\n","    self.SaveModel = SaveModel\n","\n","\n","  def _run(self):\n","      def OpenEndSave(CurrentEpoch, module):\n","          if module == 'open'and CurrentEpoch == 1:\n","            with open(self.PathSaveFiles + self.FileName + \".pkl\", \"rb\") as f:\n","              self.Results = pickle.load(f)\n","\n","          elif module == 'save' and CurrentEpoch == (self.Epochs):\n","            with open(self.PathSaveFiles + self.FileName + \".pkl\",'wb') as f:\n","              pickle.dump(self.Results, f)\n","\n","\n","      def loss_fn(outputs, targets): \n","          if self.NumberOfLabels == 2:\n","            return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n","          else:\n","            return nn.CrossEntropyLoss()(outputs, targets.view(-1, 1))\n","     \n","            \n","\n","\n","      def train_loop_fn(data_loader, model, optimizer, device, scheduler=None, epoch=None):\n","          model.train()\n","          for bi, d in enumerate(data_loader):\n","              ids = d[\"ids\"]\n","              mask = d[\"mask\"]\n","              token_type_ids = d[\"token_type_ids\"]\n","              targets = d[\"targets\"]\n","\n","              ids = ids.to(device, dtype=torch.long)\n","              mask = mask.to(device, dtype=torch.long)\n","              token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","              targets = targets.to(device, dtype=torch.float)\n","              \n","\n","              optimizer.zero_grad()\n","              outputs = model(\n","                  ids=ids,\n","                  mask=mask,\n","                  token_type_ids=token_type_ids\n","              )\n","\n","              loss = loss_fn(outputs, targets)\n","              if bi % 10 == 0:\n","                  xm.master_print(f'bi={bi}, loss={loss}')\n","\n","                  ValueLoss = loss.cpu().detach().numpy().tolist()\n","                  ValueLoss = xm.mesh_reduce('test_loss',ValueLoss, np.mean)\n","                  self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['loss'].append(ValueLoss)\n","\n","              loss.backward()\n","              xm.optimizer_step(optimizer)\n","              if scheduler is not None:\n","                  scheduler.step()\n","\n","      def eval_loop_fn(data_loader, model, device):\n","          model.eval()\n","          fin_targets = []\n","          fin_outputs = []\n","          for bi, d in enumerate(data_loader):\n","              ids = d[\"ids\"]\n","              mask = d[\"mask\"]\n","              token_type_ids = d[\"token_type_ids\"]\n","              targets = d[\"targets\"]\n","\n","              ids = ids.to(device, dtype=torch.long)\n","              mask = mask.to(device, dtype=torch.long)\n","              token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","              targets = targets.to(device, dtype=torch.float)\n","\n","              outputs = model(\n","                  ids=ids,\n","                  mask=mask,\n","                  token_type_ids=token_type_ids\n","              )\n","\n","              targets_np = targets.cpu().detach().numpy().tolist()\n","              if self.NumberOfLabels == 2:\n","                outputs_np = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n","              else:\n","                _, outputs = torch.max(outputs, 1)\n","                outputs_np = outputs.cpu().detach().numpy().tolist()\n","      \n","              fin_targets.extend(targets_np)\n","              fin_outputs.extend(outputs_np)    \n","\n","          return fin_outputs, fin_targets\n","\n","      ## Main parameters\n","      MAX_LEN = self.MaxLen\n","      tokenizer = transformers.BertTokenizer.from_pretrained(self.BertPath, do_lower_case=True)\n","\n","\n","      train_dataset = BERTDatasetTraining(\n","          comment=self.X_train.values,\n","          targets=self.y_train.values,\n","          tokenizer=tokenizer,\n","          max_length=MAX_LEN\n","      )\n","\n","      train_sampler = torch.utils.data.distributed.DistributedSampler(\n","            train_dataset,\n","            num_replicas=xm.xrt_world_size(),\n","            rank=xm.get_ordinal(),\n","            shuffle=True)\n","\n","      train_data_loader = torch.utils.data.DataLoader(\n","          train_dataset,\n","          batch_size=self.BatchSize,\n","          sampler=train_sampler,\n","          drop_last=True,\n","          num_workers=1\n","      )\n","\n","      valid_dataset = BERTDatasetTraining(\n","          comment=self.X_valid.values,\n","          targets=self.y_valid.values,\n","          tokenizer=tokenizer,\n","          max_length=MAX_LEN\n","      )\n","\n","      valid_sampler = torch.utils.data.distributed.DistributedSampler(\n","            valid_dataset,\n","            num_replicas=xm.xrt_world_size(),\n","            rank=xm.get_ordinal(),\n","            shuffle=False)\n","\n","      valid_data_loader = torch.utils.data.DataLoader(\n","          valid_dataset,\n","          batch_size=16,\n","          sampler=valid_sampler,\n","          drop_last=False,\n","          num_workers=1\n","      )\n","\n","      device = xm.xla_device()\n","      model = mx.to(device)\n","      \n","\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","      \n","      lr = 0.4 * self.LearningRate * xm.xrt_world_size()\n","      num_train_steps = int(len(train_dataset) / self.BatchSize / xm.xrt_world_size() * self.Epochs)\n","      xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n","\n","      optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","      scheduler = get_linear_schedule_with_warmup(\n","          optimizer,\n","          num_warmup_steps=0,\n","          num_training_steps=num_train_steps\n","      )\n","\n","      best_f1, f1, best_cem, cem = 0,0,0,0\n","\n","      for epoch in range(1, self.Epochs+1):\n","        ## print epoch\n","          xm.master_print(f'Epoch: {epoch} of {self.Epochs}')\n","        ## Open file to save results\n","          OpenEndSave(CurrentEpoch=epoch, module='open')\n","\n","          para_loader = pl.ParallelLoader(train_data_loader, [device])\n","          train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler, epoch=epoch)\n","\n","          para_loader = pl.ParallelLoader(valid_data_loader, [device])\n","          o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n","          \n","          if self.NumberOfLabels == 2:\n","            outputs = list(np.where(np.array(o) > 0.5, 1, 0))\n","            f1 = xm.mesh_reduce('validation_f1', metrics.f1_score(t, outputs), np.mean)\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1'].append(f1)\n","\n","          else:\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1_macro'].append(xm.mesh_reduce('validation_f1_macro', metrics.f1_score(t, outputs, average=self.average_metrics), np.mean))\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['f1_weighted'].append(cem)\n","            cem = xm.mesh_reduce('validation_cem', cem_metric(t, outputs), np.mean)\n","            self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['cem'].append(xm.mesh_reduce('validation_cem', cem_metric(t, outputs), np.mean))\n","\n","          accuracy = metrics.accuracy_score(t, outputs)\n","          accuracy = xm.mesh_reduce('test_accuracy', accuracy, np.mean)\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['accuracy'].append(accuracy)\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['recall'].append(xm.mesh_reduce('validation_recall', metrics.recall_score(t, outputs, average=self.average_metrics), np.mean))\n","          self.Results[self.BertVersion][self.OutputBert][self.LearningRate][self.BatchSize][epoch]['precision'].append(xm.mesh_reduce('validation_precison', metrics.precision_score(t, outputs, average=self.average_metrics), np.mean))\n","              \n","        ## save file with save results\n","          OpenEndSave(CurrentEpoch=epoch, module='save')\n","\n","        ## Save model\n","          if self.SaveModel and epoch == self.Epochs:\n","            xm.save(model.state_dict(), self.PathSaveFiles + self.FileName + '.bin')\n","        \n","        ## print accuracy\n","          xm.master_print(f'Accuracy = {accuracy}')\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzvib7kT2Ku6"},"source":["#Load data"]},{"cell_type":"code","metadata":{"id":"NbmqdZs-cKb-","executionInfo":{"status":"ok","timestamp":1619785043533,"user_tz":180,"elapsed":586,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["PathDataSet = \"../content/drive/MyDrive/Code/DETOXIS/Data/train.csv\"\n","# Task 1\n","df_train = pd.read_csv(PathDataSet, usecols=[\"comment\", \"toxicity\"]).fillna(\"none\")\n","NewColumnsNames = {\"comment\":\"Data\",\"toxicity\":\"Label\"}\n","\n","# ## Task 2\n","# df_train = pd.read_csv(PathDataSet, usecols=[\"comment\", \"toxicity_level\"]).fillna(\"none\")\n","# NewColumnsNames = {\"comment\":\"Data\",\"toxicity_level\":\"Label\"}\n","\n","df_train = df_train.rename(columns=NewColumnsNames)\n","df_train = df_train.sample(frac=1).reset_index(drop=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"z19LsZrQ2Q5N"},"source":["# df_train = pd.read_csv(\"../content/drive/MyDrive/Code/DETOXIS/Data/train.csv\", usecols=[\"comment\", \"toxicity\"]).fillna(\"none\")\n","# # df_valid = df_train.loc[2770:,['comment','toxicity']].reset_index(drop=True)\n","# # df_train = df_train.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Va6l36Yc2Tth","executionInfo":{"status":"ok","timestamp":1619785046550,"user_tz":180,"elapsed":551,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"979ead68-b805-4d17-97f1-a79161c0c676"},"source":["df_train.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>he pensado lo mismo al leer el titular</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Señora Nuria ,aunque supongo será un nombre fi...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Visto desde ese punto de vista ni tan mal...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Vaya costumbres que tienen en Extremadura.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAHAHAHAHAHAHAHAAHAH\\nse me ha escuchado desde...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  Label\n","0            he pensado lo mismo al leer el titular       0\n","1  Señora Nuria ,aunque supongo será un nombre fi...      1\n","2       Visto desde ese punto de vista ni tan mal...      1\n","3         Vaya costumbres que tienen en Extremadura.      0\n","4  HAHAHAHAHAHAHAHAAHAH\\nse me ha escuchado desde...      0"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"3I3eUmvE3zPa"},"source":["#Load Weights"]},{"cell_type":"code","metadata":{"id":"a9pbFMZOcXkp","executionInfo":{"status":"ok","timestamp":1619785050804,"user_tz":180,"elapsed":557,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["def CriateFileName(BertVersionDict, NumberOfClasses):\n","  \n","  NameFile = str()\n","  for BertModel in BertVersionDict.keys():\n","    NameFile += BertModel\n","\n","  if NumberOfClasses > 2:\n","    NameFile += 'Task2'\n","  else:\n","    NameFile += 'Task1'\n","\n","  return NameFile"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"miaaavjhYUp-"},"source":["# BertVersion = {'SpanishBert':'../content/bert-base-spanish-wwm-uncased/', 'MultilingualBert':'../content/bert-base-multilingual-uncased/'}\n","# OutputBert = ['hidden', 'pooler']\n","# LearningRate = [1e-5, 3e-5, 5e-5]\n","# BatchSize = [8, 16, 32 , 64]\n","# Epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCClRd-HVprC","executionInfo":{"status":"ok","timestamp":1619786349235,"user_tz":180,"elapsed":541,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["BertVersion = {'SpanishBert':'../content/bert-base-spanish-wwm-uncased/'}\n","OutputBert = ['hidden', 'pooler']\n","LearningRate = [1e-5, 3e-5, 5e-5]\n","BatchSize = [8, 16, 32 , 64]\n","Epochs = 20"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrWS9-eNcnK8","executionInfo":{"status":"ok","timestamp":1619786353141,"user_tz":180,"elapsed":650,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Evalute matrics\n","###### Task 1\n","MetricsTask1 = ['accuracy', 'f1', 'recall', 'precision']\n","###### Task 2\n","MetricsTask2 = ['accuracy', 'f1_macro', 'f1_weighted', 'recall', 'precision', 'cem']\n","\n","## Get for 'Binary' classification' task1 or 'Multilabel classifcation' task2\n","Metrics = MetricsTask2 if df_train['Label'].nunique() > 2 else MetricsTask1\n","\n","## Criate dictinaril results\n","ResultsTask = { bert:{ output:{ lr:{ bat:{ epoc:{ metric:[] for metric in Metrics + ['loss']} for epoc in range(1, Epochs+1) } for bat in BatchSize} for lr in LearningRate} for output in OutputBert } for bert in BertVersion.keys() }"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjxtRdNL2Kus","executionInfo":{"status":"ok","timestamp":1619786356685,"user_tz":180,"elapsed":563,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Where to Save Files\n","Path = 'drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/'\n","\n","## Creating Main Parte Bert File Name\n","MainParteBertFileName = CriateFileName(BertVersion, NumberOfClasses=df_train['Label'].nunique())\n","\n","## Create file to save results\n","FileResults = MainParteBertFileName + 'Results'\n","# with open(Path + FileResults + \".pkl\",'wb') as f:\n","#   pickle.dump(ResultsTask, f)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"oi7hvjrtdhGe"},"source":["### Cross Validation\n","for BertV, BertPath in BertVersion.items():\n","  for OutputB in OutputBert:\n","\n","    ### Loading Bert trained weights\n","    mx = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique())\n","\n","    for lr in LearningRate:\n","      for Batch in BatchSize:\n","\n","        ## StratifiedKFold\n","        skf = StratifiedKFold(n_splits=10)\n","        fold = 1\n","        for train_index, valid_index in skf.split(df_train['Data'], df_train['Label']):\n","          X_train, X_valid = df_train.loc[train_index, 'Data'], df_train.loc[valid_index, 'Data']\n","          y_train, y_valid = df_train.loc[train_index, 'Label'], df_train.loc[valid_index, 'Label']\n","\n","          print(f'parameters: Bertmodel: {BertV}, Output: {OutputB}, lr: {lr}, Batch: {Batch}, Totsl Num. Epochs: {Epochs}, Fold: {fold}')\n","          fold += 1\n","          MoDeL = TrainModel(PathSaveFiles = Path,\n","                            BertVersion=BertV,\n","                            BertPath=BertPath,\n","                            OutputBert=OutputB,\n","                            LearningRate=lr,\n","                            BatchSize=Batch,\n","                            Epochs=Epochs,\n","                            FileName= FileResults,\n","                            X_train=X_train, \n","                            X_valid=X_valid,\n","                            y_train=y_train,\n","                            y_valid=y_valid)\n","        \n","\n","          def _mp_fn(rank, flags):\n","            torch.set_default_tensor_type('torch.FloatTensor')\n","            a = MoDeL._run()\n","\n","          FLAGS={}\n","          xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUbui2-D_FP3","executionInfo":{"status":"ok","timestamp":1619786360334,"user_tz":180,"elapsed":587,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["def AveragResults(FileName, Path):\n","  with open(Path + FileName + \".pkl\", \"rb\") as f:\n","              Results = pickle.load(f)\n","\n","  for BT, ModelBertType,  in Results.items():\n","    for OP, OutPut in ModelBertType.items():\n","      for LR, LearningRate in OutPut.items():\n","        for BS, BatchSize in LearningRate.items():\n","          for EP, Epoch in BatchSize.items():\n","            for Metrics, ValuesCrossValidation in  Epoch.items():\n"," \n","              # Metrics = np.mean(ValuesCrossValidation)\n","              Results[BT][OP][LR][BS][EP][Metrics] = np.mean(ValuesCrossValidation)\n","            \n","    with open('Mean' + FileName + '.pkl','wb') as f:\n","      pickle.dump(Results, f)\n","\n","    with open(Path + 'Mean' + FileName + '.pkl','wb') as f:\n","      pickle.dump(Results, f)\n","    \n","    return Results"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUvUZ5K7ejW_","executionInfo":{"status":"ok","timestamp":1619786361999,"user_tz":180,"elapsed":1114,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Average and Save Results\n","AverageResultsTask = AveragResults(FileName=FileResults, Path=Path)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3Ux_xwvHkUV","executionInfo":{"status":"ok","timestamp":1619786362296,"user_tz":180,"elapsed":384,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["### create dataframe for our results\n","def create_Data_Frame(all_resultas):\n","\n","  \n","\n","  ### Criate a pandas da Frame with all results\n","  df_results = pd.DataFrame.from_dict({(BertType, OutpuType, LearningRate, BactSize, Epochs): all_resultas[BertType][OutpuType][LearningRate][BactSize][Epochs]\n","                            for BertType in all_resultas.keys()\n","                            for OutpuType in all_resultas[BertType].keys()\n","                            for LearningRate in all_resultas[BertType][OutpuType].keys()\n","                            for BactSize in all_resultas[BertType][OutpuType][LearningRate].keys()\n","                            for Epochs in all_resultas[BertType][OutpuType][LearningRate][BactSize].keys()},\n","                        orient='index')\n","  return df_results"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"1d4TLbSNeq3j","executionInfo":{"status":"ok","timestamp":1619786364847,"user_tz":180,"elapsed":707,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"be00160d-a438-4a11-9171-75466d221968"},"source":["## Create a Data Frame\n","DfResultsTask = create_Data_Frame(all_resultas=AverageResultsTask)\n","\n","### save results to a CSV file\n","DfResultsTask.to_csv(Path + 'Average' + FileResults + '_CSV_' + '.csv')\n","\n","### See the Avarage results in the Pandas data Frame\n","DfResultsTask"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"11\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"5\" valign=\"top\">hidden</th>\n","      <th rowspan=\"5\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"5\" valign=\"top\">8</th>\n","      <th>1</th>\n","      <td>0.728125</td>\n","      <td>0.450232</td>\n","      <td>0.354750</td>\n","      <td>0.721950</td>\n","      <td>0.624056</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.750852</td>\n","      <td>0.511294</td>\n","      <td>0.429728</td>\n","      <td>0.739075</td>\n","      <td>0.438842</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.742045</td>\n","      <td>0.576718</td>\n","      <td>0.557383</td>\n","      <td>0.664583</td>\n","      <td>0.243240</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.747159</td>\n","      <td>0.591687</td>\n","      <td>0.585898</td>\n","      <td>0.654069</td>\n","      <td>0.172412</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.749148</td>\n","      <td>0.561331</td>\n","      <td>0.498089</td>\n","      <td>0.713554</td>\n","      <td>0.163945</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">pooler</th>\n","      <th rowspan=\"5\" valign=\"top\">0.00005</th>\n","      <th rowspan=\"5\" valign=\"top\">64</th>\n","      <th>16</th>\n","      <td>0.753125</td>\n","      <td>0.615895</td>\n","      <td>0.607050</td>\n","      <td>0.647149</td>\n","      <td>0.004763</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.748864</td>\n","      <td>0.608242</td>\n","      <td>0.598901</td>\n","      <td>0.641783</td>\n","      <td>0.002519</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.749432</td>\n","      <td>0.607221</td>\n","      <td>0.594453</td>\n","      <td>0.643004</td>\n","      <td>0.002429</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.749432</td>\n","      <td>0.607502</td>\n","      <td>0.594756</td>\n","      <td>0.643796</td>\n","      <td>0.002171</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.749148</td>\n","      <td>0.607561</td>\n","      <td>0.595589</td>\n","      <td>0.643114</td>\n","      <td>0.002141</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>480 rows × 5 columns</p>\n","</div>"],"text/plain":["                                  accuracy        f1  ...  precision      loss\n","SpanishBert hidden 0.00001 8  1   0.728125  0.450232  ...   0.721950  0.624056\n","                              2   0.750852  0.511294  ...   0.739075  0.438842\n","                              3   0.742045  0.576718  ...   0.664583  0.243240\n","                              4   0.747159  0.591687  ...   0.654069  0.172412\n","                              5   0.749148  0.561331  ...   0.713554  0.163945\n","...                                    ...       ...  ...        ...       ...\n","            pooler 0.00005 64 16  0.753125  0.615895  ...   0.647149  0.004763\n","                              17  0.748864  0.608242  ...   0.641783  0.002519\n","                              18  0.749432  0.607221  ...   0.643004  0.002429\n","                              19  0.749432  0.607502  ...   0.643796  0.002171\n","                              20  0.749148  0.607561  ...   0.643114  0.002141\n","\n","[480 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRDkAIR7e3FU","executionInfo":{"status":"ok","timestamp":1619786368101,"user_tz":180,"elapsed":604,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"3355129b-2bbc-4659-f0f2-0c47e3c28e68"},"source":["## Creating LateX Table\n","LabelTaskTable = FileResults\n","print(DfResultsTask.to_latex(multicolumn=True, multirow=False, label=LabelTaskTable))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["\\begin{table}\n","\\centering\n","\\label{SpanishBertTask1Results}\n","\\begin{tabular}{lllllrrrrr}\n","\\toprule\n","            &        &         &    &    &  accuracy &        f1 &    recall &  precision &      loss \\\\\n","\\midrule\n","SpanishBert & hidden & 0.00001 & 8  & 1  &  0.728125 &  0.450232 &  0.354750 &   0.721950 &  0.624056 \\\\\n","            &        &         &    & 2  &  0.750852 &  0.511294 &  0.429728 &   0.739075 &  0.438842 \\\\\n","            &        &         &    & 3  &  0.742045 &  0.576718 &  0.557383 &   0.664583 &  0.243240 \\\\\n","            &        &         &    & 4  &  0.747159 &  0.591687 &  0.585898 &   0.654069 &  0.172412 \\\\\n","            &        &         &    & 5  &  0.749148 &  0.561331 &  0.498089 &   0.713554 &  0.163945 \\\\\n","            &        &         &    & 6  &  0.743750 &  0.603858 &  0.592132 &   0.651411 &  0.062831 \\\\\n","            &        &         &    & 7  &  0.746591 &  0.598161 &  0.580012 &   0.656462 &  0.016762 \\\\\n","            &        &         &    & 8  &  0.750000 &  0.611967 &  0.609053 &   0.654805 &  0.013874 \\\\\n","            &        &         &    & 9  &  0.754830 &  0.612937 &  0.595004 &   0.651580 &  0.004492 \\\\\n","            &        &         &    & 10 &  0.749716 &  0.603408 &  0.587549 &   0.651130 &  0.004739 \\\\\n","            &        &         &    & 11 &  0.756250 &  0.604414 &  0.579777 &   0.670404 &  0.006008 \\\\\n","            &        &         &    & 12 &  0.754261 &  0.606956 &  0.578905 &   0.667539 &  0.004733 \\\\\n","            &        &         &    & 13 &  0.750568 &  0.607900 &  0.589542 &   0.651849 &  0.002482 \\\\\n","            &        &         &    & 14 &  0.754830 &  0.599598 &  0.568713 &   0.667744 &  0.002092 \\\\\n","            &        &         &    & 15 &  0.751989 &  0.608703 &  0.591738 &   0.651221 &  0.001477 \\\\\n","            &        &         &    & 16 &  0.757386 &  0.610383 &  0.585907 &   0.664738 &  0.003657 \\\\\n","            &        &         &    & 17 &  0.753693 &  0.598866 &  0.572988 &   0.660845 &  0.002229 \\\\\n","            &        &         &    & 18 &  0.753693 &  0.604535 &  0.579423 &   0.659001 &  0.001366 \\\\\n","            &        &         &    & 19 &  0.753125 &  0.605780 &  0.581977 &   0.656300 &  0.002067 \\\\\n","            &        &         &    & 20 &  0.754261 &  0.603960 &  0.575424 &   0.660698 &  0.001360 \\\\\n","            &        &         & 16 & 1  &  0.713636 &  0.403370 &  0.301400 &   0.713183 &  0.649045 \\\\\n","            &        &         &    & 2  &  0.744318 &  0.532374 &  0.450515 &   0.699710 &  0.471376 \\\\\n","            &        &         &    & 3  &  0.751989 &  0.555953 &  0.477223 &   0.711974 &  0.329873 \\\\\n","            &        &         &    & 4  &  0.744034 &  0.558538 &  0.500088 &   0.693714 &  0.188306 \\\\\n","            &        &         &    & 5  &  0.746591 &  0.555536 &  0.505089 &   0.709998 &  0.129732 \\\\\n","            &        &         &    & 6  &  0.736364 &  0.573380 &  0.570697 &   0.663627 &  0.130788 \\\\\n","            &        &         &    & 7  &  0.718182 &  0.564547 &  0.605801 &   0.604946 &  0.069591 \\\\\n","            &        &         &    & 8  &  0.732670 &  0.575843 &  0.569655 &   0.647622 &  0.078873 \\\\\n","            &        &         &    & 9  &  0.749716 &  0.582351 &  0.548726 &   0.674585 &  0.049638 \\\\\n","            &        &         &    & 10 &  0.749716 &  0.592061 &  0.563070 &   0.662762 &  0.015256 \\\\\n","            &        &         &    & 11 &  0.753409 &  0.593150 &  0.547816 &   0.676011 &  0.006576 \\\\\n","            &        &         &    & 12 &  0.749148 &  0.589071 &  0.552015 &   0.672733 &  0.005746 \\\\\n","            &        &         &    & 13 &  0.748011 &  0.590922 &  0.555195 &   0.665368 &  0.004601 \\\\\n","            &        &         &    & 14 &  0.745170 &  0.590164 &  0.560920 &   0.658142 &  0.002794 \\\\\n","            &        &         &    & 15 &  0.750000 &  0.587549 &  0.543733 &   0.674653 &  0.003267 \\\\\n","            &        &         &    & 16 &  0.747443 &  0.587919 &  0.553095 &   0.662460 &  0.001268 \\\\\n","            &        &         &    & 17 &  0.748864 &  0.587403 &  0.545160 &   0.669147 &  0.001495 \\\\\n","            &        &         &    & 18 &  0.747443 &  0.584325 &  0.542306 &   0.665944 &  0.001696 \\\\\n","            &        &         &    & 19 &  0.747159 &  0.586326 &  0.546878 &   0.664643 &  0.001559 \\\\\n","            &        &         &    & 20 &  0.747443 &  0.589201 &  0.552325 &   0.662279 &  0.001281 \\\\\n","            &        &         & 32 & 1  &  0.676136 &  0.157569 &  0.097898 &   0.560536 &  0.673901 \\\\\n","            &        &         &    & 2  &  0.723864 &  0.485493 &  0.400683 &   0.680768 &  0.546316 \\\\\n","            &        &         &    & 3  &  0.741193 &  0.537151 &  0.477916 &   0.690075 &  0.444503 \\\\\n","            &        &         &    & 4  &  0.745170 &  0.583780 &  0.557059 &   0.650921 &  0.343097 \\\\\n","            &        &         &    & 5  &  0.742614 &  0.619629 &  0.627808 &   0.642867 &  0.230715 \\\\\n","            &        &         &    & 6  &  0.742898 &  0.562196 &  0.528442 &   0.688690 &  0.157653 \\\\\n","            &        &         &    & 7  &  0.755682 &  0.607038 &  0.575419 &   0.671895 &  0.102437 \\\\\n","            &        &         &    & 8  &  0.743466 &  0.560970 &  0.532576 &   0.665720 &  0.063025 \\\\\n","            &        &         &    & 9  &  0.728693 &  0.600847 &  0.627918 &   0.613713 &  0.055879 \\\\\n","            &        &         &    & 10 &  0.714773 &  0.602054 &  0.663588 &   0.582828 &  0.037703 \\\\\n","            &        &         &    & 11 &  0.700852 &  0.602239 &  0.694513 &   0.560608 &  0.049263 \\\\\n","            &        &         &    & 12 &  0.711932 &  0.583599 &  0.632343 &   0.594145 &  0.080942 \\\\\n","            &        &         &    & 13 &  0.748011 &  0.566672 &  0.518385 &   0.682066 &  0.098328 \\\\\n","            &        &         &    & 14 &  0.747159 &  0.558845 &  0.500340 &   0.695151 &  0.067446 \\\\\n","            &        &         &    & 15 &  0.748864 &  0.588736 &  0.548374 &   0.661964 &  0.033291 \\\\\n","            &        &         &    & 16 &  0.748580 &  0.596566 &  0.565725 &   0.659556 &  0.013592 \\\\\n","            &        &         &    & 17 &  0.749432 &  0.599624 &  0.568087 &   0.660048 &  0.010401 \\\\\n","            &        &         &    & 18 &  0.750852 &  0.595584 &  0.558836 &   0.664844 &  0.008286 \\\\\n","            &        &         &    & 19 &  0.751705 &  0.597230 &  0.560950 &   0.664515 &  0.006553 \\\\\n","            &        &         &    & 20 &  0.750568 &  0.597591 &  0.563346 &   0.660820 &  0.005028 \\\\\n","            &        &         & 64 & 1  &  0.659659 &  0.000000 &  0.000000 &   0.000000 &  0.746853 \\\\\n","            &        &         &    & 2  &  0.670170 &  0.083932 &  0.047305 &   0.479167 &  0.620303 \\\\\n","            &        &         &    & 3  &  0.722159 &  0.490518 &  0.412211 &   0.657712 &  0.573224 \\\\\n","            &        &         &    & 4  &  0.730398 &  0.520640 &  0.449420 &   0.659195 &  0.510271 \\\\\n","            &        &         &    & 5  &  0.742898 &  0.554919 &  0.491796 &   0.673872 &  0.444971 \\\\\n","            &        &         &    & 6  &  0.749148 &  0.557804 &  0.488737 &   0.688000 &  0.375717 \\\\\n","            &        &         &    & 7  &  0.749716 &  0.595028 &  0.556195 &   0.665966 &  0.320383 \\\\\n","            &        &         &    & 8  &  0.748011 &  0.588028 &  0.542555 &   0.672826 &  0.249498 \\\\\n","            &        &         &    & 9  &  0.749716 &  0.578354 &  0.521762 &   0.686286 &  0.185014 \\\\\n","            &        &         &    & 10 &  0.743182 &  0.564256 &  0.518796 &   0.661571 &  0.137778 \\\\\n","            &        &         &    & 11 &  0.737216 &  0.538299 &  0.494372 &   0.662931 &  0.114764 \\\\\n","            &        &         &    & 12 &  0.739489 &  0.595993 &  0.582581 &   0.647892 &  0.091629 \\\\\n","            &        &         &    & 13 &  0.722727 &  0.557172 &  0.561268 &   0.633308 &  0.064937 \\\\\n","            &        &         &    & 14 &  0.737784 &  0.567969 &  0.530209 &   0.650776 &  0.118409 \\\\\n","            &        &         &    & 15 &  0.743466 &  0.612850 &  0.617777 &   0.629567 &  0.046155 \\\\\n","            &        &         &    & 16 &  0.740625 &  0.570277 &  0.539287 &   0.652850 &  0.050753 \\\\\n","            &        &         &    & 17 &  0.747443 &  0.589158 &  0.549563 &   0.661466 &  0.029862 \\\\\n","            &        &         &    & 18 &  0.746307 &  0.604696 &  0.582981 &   0.647718 &  0.021657 \\\\\n","            &        &         &    & 19 &  0.743182 &  0.600591 &  0.583384 &   0.641677 &  0.016521 \\\\\n","            &        &         &    & 20 &  0.742614 &  0.596938 &  0.578518 &   0.643424 &  0.015890 \\\\\n","            &        & 0.00003 & 8  & 1  &  0.721307 &  0.336143 &  0.237130 &   0.788713 &  0.598861 \\\\\n","            &        &         &    & 2  &  0.737784 &  0.474142 &  0.379288 &   0.747709 &  0.409022 \\\\\n","            &        &         &    & 3  &  0.729545 &  0.524917 &  0.510059 &   0.638490 &  0.269675 \\\\\n","            &        &         &    & 4  &  0.741477 &  0.573878 &  0.546680 &   0.658811 &  0.201156 \\\\\n","            &        &         &    & 5  &  0.750284 &  0.566537 &  0.505479 &   0.682251 &  0.118234 \\\\\n","            &        &         &    & 6  &  0.724716 &  0.601288 &  0.619313 &   0.607603 &  0.069841 \\\\\n","            &        &         &    & 7  &  0.729261 &  0.573972 &  0.571741 &   0.609675 &  0.024738 \\\\\n","            &        &         &    & 8  &  0.731818 &  0.570068 &  0.546922 &   0.621448 &  0.014221 \\\\\n","            &        &         &    & 9  &  0.742898 &  0.595368 &  0.580747 &   0.637868 &  0.006277 \\\\\n","            &        &         &    & 10 &  0.741193 &  0.585290 &  0.557617 &   0.640778 &  0.002025 \\\\\n","            &        &         &    & 11 &  0.740057 &  0.579663 &  0.547071 &   0.645731 &  0.001654 \\\\\n","            &        &         &    & 12 &  0.741477 &  0.587835 &  0.561480 &   0.641457 &  0.001571 \\\\\n","            &        &         &    & 13 &  0.740909 &  0.580721 &  0.549075 &   0.640187 &  0.001368 \\\\\n","            &        &         &    & 14 &  0.742045 &  0.587646 &  0.560008 &   0.640786 &  0.001080 \\\\\n","            &        &         &    & 15 &  0.742045 &  0.588823 &  0.561507 &   0.638382 &  0.001331 \\\\\n","            &        &         &    & 16 &  0.740909 &  0.581052 &  0.547651 &   0.644975 &  0.001179 \\\\\n","            &        &         &    & 17 &  0.740341 &  0.581591 &  0.552308 &   0.641310 &  0.001599 \\\\\n","            &        &         &    & 18 &  0.741477 &  0.582731 &  0.551291 &   0.643558 &  0.001271 \\\\\n","            &        &         &    & 19 &  0.742614 &  0.585878 &  0.556345 &   0.643745 &  0.001735 \\\\\n","            &        &         &    & 20 &  0.740341 &  0.585707 &  0.559380 &   0.636949 &  0.001082 \\\\\n","            &        &         & 16 & 1  &  0.720739 &  0.359354 &  0.279477 &   0.740957 &  0.621572 \\\\\n","            &        &         &    & 2  &  0.733807 &  0.424495 &  0.328960 &   0.736068 &  0.436629 \\\\\n","            &        &         &    & 3  &  0.737500 &  0.521486 &  0.474512 &   0.716766 &  0.274345 \\\\\n","            &        &         &    & 4  &  0.742045 &  0.598246 &  0.593789 &   0.634409 &  0.185858 \\\\\n","            &        &         &    & 5  &  0.717330 &  0.604975 &  0.666867 &   0.596580 &  0.111569 \\\\\n","            &        &         &    & 6  &  0.715341 &  0.603353 &  0.661188 &   0.594418 &  0.093867 \\\\\n","            &        &         &    & 7  &  0.748580 &  0.597141 &  0.571149 &   0.658007 &  0.067619 \\\\\n","            &        &         &    & 8  &  0.753977 &  0.608132 &  0.587541 &   0.661994 &  0.020050 \\\\\n","            &        &         &    & 9  &  0.748295 &  0.596942 &  0.573117 &   0.648847 &  0.004400 \\\\\n","            &        &         &    & 10 &  0.743182 &  0.596646 &  0.581108 &   0.639991 &  0.004953 \\\\\n","            &        &         &    & 11 &  0.743466 &  0.590234 &  0.564747 &   0.641324 &  0.001505 \\\\\n","            &        &         &    & 12 &  0.749432 &  0.600573 &  0.574898 &   0.652104 &  0.001815 \\\\\n","            &        &         &    & 13 &  0.748580 &  0.605231 &  0.589858 &   0.652123 &  0.001066 \\\\\n","            &        &         &    & 14 &  0.748011 &  0.600328 &  0.578601 &   0.650809 &  0.002330 \\\\\n","            &        &         &    & 15 &  0.753409 &  0.601713 &  0.571623 &   0.664165 &  0.001052 \\\\\n","            &        &         &    & 16 &  0.752841 &  0.606656 &  0.584270 &   0.659507 &  0.003881 \\\\\n","            &        &         &    & 17 &  0.754261 &  0.603986 &  0.573042 &   0.666019 &  0.003117 \\\\\n","            &        &         &    & 18 &  0.752273 &  0.604035 &  0.576677 &   0.660782 &  0.002190 \\\\\n","            &        &         &    & 19 &  0.752557 &  0.603625 &  0.576531 &   0.660569 &  0.000682 \\\\\n","            &        &         &    & 20 &  0.751989 &  0.603116 &  0.577364 &   0.658201 &  0.000695 \\\\\n","            &        &         & 32 & 1  &  0.715057 &  0.385470 &  0.283927 &   0.729920 &  0.649937 \\\\\n","            &        &         &    & 2  &  0.724716 &  0.519031 &  0.469547 &   0.663227 &  0.478559 \\\\\n","            &        &         &    & 3  &  0.748295 &  0.559675 &  0.503350 &   0.700882 &  0.308366 \\\\\n","            &        &         &    & 4  &  0.737784 &  0.561783 &  0.559310 &   0.685725 &  0.197227 \\\\\n","            &        &         &    & 5  &  0.735795 &  0.574359 &  0.579923 &   0.667195 &  0.153408 \\\\\n","            &        &         &    & 6  &  0.740057 &  0.597294 &  0.603694 &   0.635021 &  0.089625 \\\\\n","            &        &         &    & 7  &  0.734375 &  0.598323 &  0.625448 &   0.610140 &  0.037381 \\\\\n","            &        &         &    & 8  &  0.725852 &  0.589377 &  0.619884 &   0.608463 &  0.023227 \\\\\n","            &        &         &    & 9  &  0.721307 &  0.569049 &  0.576948 &   0.617306 &  0.032578 \\\\\n","            &        &         &    & 10 &  0.740057 &  0.565236 &  0.576057 &   0.640601 &  0.040790 \\\\\n","            &        &         &    & 11 &  0.746307 &  0.614529 &  0.610655 &   0.657742 &  0.064438 \\\\\n","            &        &         &    & 12 &  0.755682 &  0.590315 &  0.539390 &   0.679816 &  0.044614 \\\\\n","            &        &         &    & 13 &  0.751136 &  0.573729 &  0.534124 &   0.681469 &  0.023965 \\\\\n","            &        &         &    & 14 &  0.750000 &  0.613774 &  0.605269 &   0.645995 &  0.013005 \\\\\n","            &        &         &    & 15 &  0.751989 &  0.610674 &  0.590434 &   0.655455 &  0.001949 \\\\\n","            &        &         &    & 16 &  0.755966 &  0.605269 &  0.570873 &   0.667860 &  0.002435 \\\\\n","            &        &         &    & 17 &  0.755114 &  0.608929 &  0.580587 &   0.662222 &  0.000981 \\\\\n","            &        &         &    & 18 &  0.754545 &  0.606724 &  0.577130 &   0.661021 &  0.002380 \\\\\n","            &        &         &    & 19 &  0.755114 &  0.608892 &  0.580544 &   0.661187 &  0.000746 \\\\\n","            &        &         &    & 20 &  0.756534 &  0.611161 &  0.583104 &   0.663121 &  0.000844 \\\\\n","            &        &         & 64 & 1  &  0.666477 &  0.051593 &  0.028683 &   0.302083 &  0.746853 \\\\\n","            &        &         &    & 2  &  0.726705 &  0.454804 &  0.353079 &   0.729068 &  0.593538 \\\\\n","            &        &         &    & 3  &  0.740341 &  0.530367 &  0.461768 &   0.701867 &  0.493526 \\\\\n","            &        &         &    & 4  &  0.740909 &  0.583071 &  0.554040 &   0.668152 &  0.372581 \\\\\n","            &        &         &    & 5  &  0.739773 &  0.593793 &  0.576344 &   0.662666 &  0.250941 \\\\\n","            &        &         &    & 6  &  0.735227 &  0.586648 &  0.583722 &   0.641939 &  0.139865 \\\\\n","            &        &         &    & 7  &  0.707670 &  0.569256 &  0.610053 &   0.601607 &  0.081604 \\\\\n","            &        &         &    & 8  &  0.705114 &  0.560639 &  0.617410 &   0.589242 &  0.086071 \\\\\n","            &        &         &    & 9  &  0.729545 &  0.527151 &  0.508554 &   0.659633 &  0.115803 \\\\\n","            &        &         &    & 10 &  0.720170 &  0.525818 &  0.514815 &   0.667651 &  0.114807 \\\\\n","            &        &         &    & 11 &  0.737784 &  0.585822 &  0.569638 &   0.644129 &  0.150879 \\\\\n","            &        &         &    & 12 &  0.747443 &  0.559450 &  0.508936 &   0.677298 &  0.052482 \\\\\n","            &        &         &    & 13 &  0.754830 &  0.601399 &  0.566448 &   0.669199 &  0.027292 \\\\\n","            &        &         &    & 14 &  0.750852 &  0.610142 &  0.590773 &   0.653076 &  0.015656 \\\\\n","            &        &         &    & 15 &  0.744886 &  0.593754 &  0.570849 &   0.648588 &  0.007076 \\\\\n","            &        &         &    & 16 &  0.746023 &  0.586177 &  0.557000 &   0.658864 &  0.004028 \\\\\n","            &        &         &    & 17 &  0.750000 &  0.590333 &  0.555883 &   0.661935 &  0.005486 \\\\\n","            &        &         &    & 18 &  0.750284 &  0.591093 &  0.556874 &   0.660139 &  0.002771 \\\\\n","            &        &         &    & 19 &  0.750000 &  0.590796 &  0.555972 &   0.657031 &  0.002030 \\\\\n","            &        &         &    & 20 &  0.750852 &  0.593054 &  0.558323 &   0.657992 &  0.002095 \\\\\n","            &        & 0.00005 & 8  & 1  &  0.727841 &  0.378921 &  0.278507 &   0.740915 &  0.605261 \\\\\n","            &        &         &    & 2  &  0.731534 &  0.511527 &  0.463375 &   0.679124 &  0.417763 \\\\\n","            &        &         &    & 3  &  0.742045 &  0.507956 &  0.420843 &   0.732376 &  0.301211 \\\\\n","            &        &         &    & 4  &  0.719318 &  0.545660 &  0.553564 &   0.612591 &  0.231046 \\\\\n","            &        &         &    & 5  &  0.734375 &  0.531862 &  0.483342 &   0.654786 &  0.165795 \\\\\n","            &        &         &    & 6  &  0.723295 &  0.571266 &  0.565064 &   0.609208 &  0.066000 \\\\\n","            &        &         &    & 7  &  0.726989 &  0.581217 &  0.576138 &   0.611081 &  0.044734 \\\\\n","            &        &         &    & 8  &  0.722727 &  0.558892 &  0.542754 &   0.618926 &  0.037557 \\\\\n","            &        &         &    & 9  &  0.734943 &  0.578867 &  0.562301 &   0.628458 &  0.020029 \\\\\n","            &        &         &    & 10 &  0.736932 &  0.567275 &  0.534464 &   0.634556 &  0.015228 \\\\\n","            &        &         &    & 11 &  0.733523 &  0.557700 &  0.525811 &   0.633345 &  0.011575 \\\\\n","            &        &         &    & 12 &  0.732102 &  0.576836 &  0.564347 &   0.620884 &  0.011032 \\\\\n","            &        &         &    & 13 &  0.729830 &  0.563631 &  0.538984 &   0.626157 &  0.009530 \\\\\n","            &        &         &    & 14 &  0.730682 &  0.570046 &  0.549390 &   0.625152 &  0.008894 \\\\\n","            &        &         &    & 15 &  0.738636 &  0.578916 &  0.550532 &   0.634663 &  0.009907 \\\\\n","            &        &         &    & 16 &  0.731818 &  0.572699 &  0.552139 &   0.620403 &  0.008054 \\\\\n","            &        &         &    & 17 &  0.731818 &  0.569579 &  0.546808 &   0.621047 &  0.008186 \\\\\n","            &        &         &    & 18 &  0.731250 &  0.569320 &  0.547641 &   0.621166 &  0.008094 \\\\\n","            &        &         &    & 19 &  0.733807 &  0.571523 &  0.550472 &   0.623577 &  0.009746 \\\\\n","            &        &         &    & 20 &  0.733807 &  0.571406 &  0.549789 &   0.623789 &  0.008022 \\\\\n","            &        &         & 16 & 1  &  0.707102 &  0.377363 &  0.325596 &   0.714161 &  0.655163 \\\\\n","            &        &         &    & 2  &  0.738068 &  0.518979 &  0.492469 &   0.687417 &  0.464146 \\\\\n","            &        &         &    & 3  &  0.743182 &  0.550381 &  0.499974 &   0.682486 &  0.280322 \\\\\n","            &        &         &    & 4  &  0.728125 &  0.560386 &  0.543855 &   0.649427 &  0.175250 \\\\\n","            &        &         &    & 5  &  0.746875 &  0.563725 &  0.503103 &   0.691580 &  0.133531 \\\\\n","            &        &         &    & 6  &  0.722727 &  0.575926 &  0.596545 &   0.594192 &  0.066310 \\\\\n","            &        &         &    & 7  &  0.718182 &  0.597548 &  0.638783 &   0.601700 &  0.043791 \\\\\n","            &        &         &    & 8  &  0.728977 &  0.594325 &  0.607870 &   0.611113 &  0.035392 \\\\\n","            &        &         &    & 9  &  0.737216 &  0.580871 &  0.565533 &   0.639203 &  0.018326 \\\\\n","            &        &         &    & 10 &  0.738068 &  0.597731 &  0.598937 &   0.636175 &  0.011002 \\\\\n","            &        &         &    & 11 &  0.739205 &  0.614291 &  0.627259 &   0.623571 &  0.009075 \\\\\n","            &        &         &    & 12 &  0.747443 &  0.603058 &  0.595391 &   0.637047 &  0.003432 \\\\\n","            &        &         &    & 13 &  0.743182 &  0.597965 &  0.587868 &   0.633186 &  0.003453 \\\\\n","            &        &         &    & 14 &  0.742045 &  0.587576 &  0.563406 &   0.639781 &  0.003534 \\\\\n","            &        &         &    & 15 &  0.742614 &  0.599347 &  0.587691 &   0.632431 &  0.002629 \\\\\n","            &        &         &    & 16 &  0.743750 &  0.599721 &  0.587325 &   0.634334 &  0.000678 \\\\\n","            &        &         &    & 17 &  0.745455 &  0.601618 &  0.590114 &   0.634987 &  0.000724 \\\\\n","            &        &         &    & 18 &  0.746023 &  0.599830 &  0.584976 &   0.636828 &  0.000708 \\\\\n","            &        &         &    & 19 &  0.746023 &  0.599808 &  0.585362 &   0.636599 &  0.000693 \\\\\n","            &        &         &    & 20 &  0.746307 &  0.599390 &  0.584469 &   0.637744 &  0.000779 \\\\\n","            &        &         & 32 & 1  &  0.698580 &  0.317249 &  0.235234 &   0.697206 &  0.660301 \\\\\n","            &        &         &    & 2  &  0.734375 &  0.483301 &  0.404762 &   0.704606 &  0.515623 \\\\\n","            &        &         &    & 3  &  0.731818 &  0.498394 &  0.449625 &   0.686061 &  0.337374 \\\\\n","            &        &         &    & 4  &  0.745739 &  0.529183 &  0.461500 &   0.716274 &  0.211324 \\\\\n","            &        &         &    & 5  &  0.718182 &  0.564667 &  0.583841 &   0.642848 &  0.121985 \\\\\n","            &        &         &    & 6  &  0.734659 &  0.575638 &  0.570705 &   0.651549 &  0.101079 \\\\\n","            &        &         &    & 7  &  0.714205 &  0.578078 &  0.596889 &   0.604715 &  0.025443 \\\\\n","            &        &         &    & 8  &  0.724148 &  0.586665 &  0.602912 &   0.611447 &  0.019971 \\\\\n","            &        &         &    & 9  &  0.729261 &  0.571003 &  0.566300 &   0.629826 &  0.019249 \\\\\n","            &        &         &    & 10 &  0.740625 &  0.577761 &  0.550341 &   0.643201 &  0.021666 \\\\\n","            &        &         &    & 11 &  0.742330 &  0.585044 &  0.555806 &   0.647330 &  0.022336 \\\\\n","            &        &         &    & 12 &  0.739205 &  0.583337 &  0.552343 &   0.641400 &  0.010572 \\\\\n","            &        &         &    & 13 &  0.731250 &  0.556028 &  0.527827 &   0.637111 &  0.011155 \\\\\n","            &        &         &    & 14 &  0.743182 &  0.571722 &  0.523517 &   0.655311 &  0.004630 \\\\\n","            &        &         &    & 15 &  0.740909 &  0.571584 &  0.528204 &   0.651670 &  0.001485 \\\\\n","            &        &         &    & 16 &  0.740625 &  0.570568 &  0.528989 &   0.652222 &  0.002190 \\\\\n","            &        &         &    & 17 &  0.739489 &  0.574315 &  0.538748 &   0.644219 &  0.003517 \\\\\n","            &        &         &    & 18 &  0.740341 &  0.577315 &  0.541755 &   0.644691 &  0.002293 \\\\\n","            &        &         &    & 19 &  0.740057 &  0.577606 &  0.542946 &   0.644524 &  0.001363 \\\\\n","            &        &         &    & 20 &  0.740341 &  0.578398 &  0.544196 &   0.645059 &  0.000709 \\\\\n","            &        &         & 64 & 1  &  0.659659 &  0.000000 &  0.000000 &   0.000000 &  0.746853 \\\\\n","            &        &         &    & 2  &  0.698580 &  0.313441 &  0.236841 &   0.687983 &  0.635013 \\\\\n","            &        &         &    & 3  &  0.723011 &  0.458806 &  0.404484 &   0.671783 &  0.552581 \\\\\n","            &        &         &    & 4  &  0.731818 &  0.501713 &  0.469971 &   0.700992 &  0.464042 \\\\\n","            &        &         &    & 5  &  0.734943 &  0.550106 &  0.529897 &   0.676981 &  0.342555 \\\\\n","            &        &         &    & 6  &  0.719602 &  0.543040 &  0.552926 &   0.637193 &  0.222210 \\\\\n","            &        &         &    & 7  &  0.728977 &  0.586155 &  0.618248 &   0.633943 &  0.153658 \\\\\n","            &        &         &    & 8  &  0.732386 &  0.564897 &  0.584736 &   0.660508 &  0.087982 \\\\\n","            &        &         &    & 9  &  0.715057 &  0.586868 &  0.621829 &   0.614445 &  0.179882 \\\\\n","            &        &         &    & 10 &  0.749716 &  0.551050 &  0.510524 &   0.710383 &  0.177443 \\\\\n","            &        &         &    & 11 &  0.737216 &  0.603935 &  0.616860 &   0.639918 &  0.132369 \\\\\n","            &        &         &    & 12 &  0.754261 &  0.602557 &  0.578493 &   0.660822 &  0.055369 \\\\\n","            &        &         &    & 13 &  0.749432 &  0.596617 &  0.573660 &   0.656836 &  0.029358 \\\\\n","            &        &         &    & 14 &  0.743182 &  0.613747 &  0.616252 &   0.631444 &  0.019277 \\\\\n","            &        &         &    & 15 &  0.745455 &  0.610370 &  0.608275 &   0.635502 &  0.009729 \\\\\n","            &        &         &    & 16 &  0.746591 &  0.604452 &  0.589602 &   0.642842 &  0.005676 \\\\\n","            &        &         &    & 17 &  0.749432 &  0.607913 &  0.590104 &   0.649647 &  0.004185 \\\\\n","            &        &         &    & 18 &  0.747443 &  0.605365 &  0.587702 &   0.646762 &  0.003710 \\\\\n","            &        &         &    & 19 &  0.747443 &  0.606710 &  0.590098 &   0.646975 &  0.003105 \\\\\n","            &        &         &    & 20 &  0.747727 &  0.608285 &  0.592558 &   0.647118 &  0.002565 \\\\\n","            & pooler & 0.00001 & 8  & 1  &  0.738068 &  0.485181 &  0.409531 &   0.722901 &  0.594425 \\\\\n","            &        &         &    & 2  &  0.736364 &  0.511464 &  0.466754 &   0.689953 &  0.407095 \\\\\n","            &        &         &    & 3  &  0.740057 &  0.565074 &  0.549358 &   0.672707 &  0.256299 \\\\\n","            &        &         &    & 4  &  0.757102 &  0.570974 &  0.496623 &   0.720476 &  0.179775 \\\\\n","            &        &         &    & 5  &  0.738068 &  0.575255 &  0.541654 &   0.672865 &  0.112246 \\\\\n","            &        &         &    & 6  &  0.750568 &  0.597508 &  0.579181 &   0.663015 &  0.046855 \\\\\n","            &        &         &    & 7  &  0.753125 &  0.608221 &  0.593495 &   0.660134 &  0.021523 \\\\\n","            &        &         &    & 8  &  0.756534 &  0.607416 &  0.583316 &   0.668870 &  0.012201 \\\\\n","            &        &         &    & 9  &  0.753409 &  0.601419 &  0.569124 &   0.665005 &  0.009918 \\\\\n","            &        &         &    & 10 &  0.753977 &  0.596512 &  0.567279 &   0.665957 &  0.005558 \\\\\n","            &        &         &    & 11 &  0.750284 &  0.610536 &  0.599022 &   0.646285 &  0.004997 \\\\\n","            &        &         &    & 12 &  0.748011 &  0.595186 &  0.568542 &   0.655095 &  0.003495 \\\\\n","            &        &         &    & 13 &  0.756250 &  0.607153 &  0.573657 &   0.675074 &  0.003921 \\\\\n","            &        &         &    & 14 &  0.754545 &  0.596883 &  0.557825 &   0.667857 &  0.002899 \\\\\n","            &        &         &    & 15 &  0.755114 &  0.604897 &  0.571825 &   0.674633 &  0.002914 \\\\\n","            &        &         &    & 16 &  0.757670 &  0.605402 &  0.572597 &   0.668336 &  0.003471 \\\\\n","            &        &         &    & 17 &  0.757670 &  0.605191 &  0.571788 &   0.667568 &  0.001941 \\\\\n","            &        &         &    & 18 &  0.755682 &  0.602574 &  0.564993 &   0.669096 &  0.001544 \\\\\n","            &        &         &    & 19 &  0.757386 &  0.607956 &  0.573970 &   0.668982 &  0.003623 \\\\\n","            &        &         &    & 20 &  0.757386 &  0.606484 &  0.571967 &   0.669105 &  0.001529 \\\\\n","            &        &         & 16 & 1  &  0.723864 &  0.421509 &  0.319521 &   0.748966 &  0.604794 \\\\\n","            &        &         &    & 2  &  0.749716 &  0.531498 &  0.437305 &   0.736135 &  0.445762 \\\\\n","            &        &         &    & 3  &  0.745455 &  0.556836 &  0.496784 &   0.689549 &  0.285996 \\\\\n","            &        &         &    & 4  &  0.737216 &  0.520374 &  0.464721 &   0.695318 &  0.170603 \\\\\n","            &        &         &    & 5  &  0.721591 &  0.548044 &  0.547540 &   0.639491 &  0.111787 \\\\\n","            &        &         &    & 6  &  0.738068 &  0.577356 &  0.578592 &   0.640146 &  0.086893 \\\\\n","            &        &         &    & 7  &  0.726705 &  0.626452 &  0.682886 &   0.601620 &  0.105680 \\\\\n","            &        &         &    & 8  &  0.746591 &  0.587403 &  0.571630 &   0.660468 &  0.060108 \\\\\n","            &        &         &    & 9  &  0.749432 &  0.594776 &  0.558404 &   0.664639 &  0.017692 \\\\\n","            &        &         &    & 10 &  0.751989 &  0.580132 &  0.531129 &   0.671688 &  0.009490 \\\\\n","            &        &         &    & 11 &  0.745455 &  0.592619 &  0.568105 &   0.649085 &  0.007645 \\\\\n","            &        &         &    & 12 &  0.747443 &  0.587608 &  0.552298 &   0.661761 &  0.003915 \\\\\n","            &        &         &    & 13 &  0.747159 &  0.581107 &  0.544180 &   0.663727 &  0.004380 \\\\\n","            &        &         &    & 14 &  0.748011 &  0.584422 &  0.542571 &   0.657464 &  0.003544 \\\\\n","            &        &         &    & 15 &  0.748580 &  0.576783 &  0.528886 &   0.666041 &  0.003501 \\\\\n","            &        &         &    & 16 &  0.749148 &  0.585852 &  0.543987 &   0.664072 &  0.002327 \\\\\n","            &        &         &    & 17 &  0.750000 &  0.584484 &  0.542467 &   0.665670 &  0.003309 \\\\\n","            &        &         &    & 18 &  0.750284 &  0.586091 &  0.541516 &   0.669469 &  0.002015 \\\\\n","            &        &         &    & 19 &  0.751705 &  0.590022 &  0.545771 &   0.669792 &  0.002328 \\\\\n","            &        &         &    & 20 &  0.751705 &  0.591471 &  0.547557 &   0.669381 &  0.001456 \\\\\n","            &        &         & 32 & 1  &  0.684375 &  0.182084 &  0.115432 &   0.593824 &  0.637095 \\\\\n","            &        &         &    & 2  &  0.740057 &  0.538148 &  0.465742 &   0.688016 &  0.516220 \\\\\n","            &        &         &    & 3  &  0.740057 &  0.602705 &  0.602886 &   0.632674 &  0.393078 \\\\\n","            &        &         &    & 4  &  0.744602 &  0.631424 &  0.651356 &   0.633806 &  0.292655 \\\\\n","            &        &         &    & 5  &  0.747443 &  0.553119 &  0.477457 &   0.702528 &  0.208746 \\\\\n","            &        &         &    & 6  &  0.725568 &  0.553037 &  0.561247 &   0.634190 &  0.115074 \\\\\n","            &        &         &    & 7  &  0.736648 &  0.573485 &  0.555472 &   0.643124 &  0.091043 \\\\\n","            &        &         &    & 8  &  0.717330 &  0.589348 &  0.626390 &   0.585828 &  0.051900 \\\\\n","            &        &         &    & 9  &  0.712784 &  0.588468 &  0.652024 &   0.587379 &  0.044101 \\\\\n","            &        &         &    & 10 &  0.729830 &  0.589240 &  0.613294 &   0.611710 &  0.087665 \\\\\n","            &        &         &    & 11 &  0.719602 &  0.582352 &  0.609353 &   0.611802 &  0.065018 \\\\\n","            &        &         &    & 12 &  0.732955 &  0.574469 &  0.558934 &   0.643683 &  0.103707 \\\\\n","            &        &         &    & 13 &  0.734943 &  0.536524 &  0.491672 &   0.668185 &  0.060319 \\\\\n","            &        &         &    & 14 &  0.742898 &  0.577030 &  0.552140 &   0.653827 &  0.036312 \\\\\n","            &        &         &    & 15 &  0.745455 &  0.588576 &  0.559978 &   0.643886 &  0.016260 \\\\\n","            &        &         &    & 16 &  0.748295 &  0.579352 &  0.537932 &   0.654451 &  0.009844 \\\\\n","            &        &         &    & 17 &  0.747443 &  0.582955 &  0.545549 &   0.649163 &  0.006361 \\\\\n","            &        &         &    & 18 &  0.748295 &  0.584557 &  0.547692 &   0.650756 &  0.005977 \\\\\n","            &        &         &    & 19 &  0.748295 &  0.584270 &  0.547850 &   0.649335 &  0.005497 \\\\\n","            &        &         &    & 20 &  0.748011 &  0.584112 &  0.547335 &   0.649052 &  0.004977 \\\\\n","            &        &         & 64 & 1  &  0.675284 &  0.143390 &  0.086421 &   0.541042 &  0.696791 \\\\\n","            &        &         &    & 2  &  0.690057 &  0.205857 &  0.128132 &   0.692173 &  0.599492 \\\\\n","            &        &         &    & 3  &  0.729830 &  0.467602 &  0.370764 &   0.701892 &  0.540064 \\\\\n","            &        &         &    & 4  &  0.742898 &  0.525292 &  0.445798 &   0.707524 &  0.460440 \\\\\n","            &        &         &    & 5  &  0.743750 &  0.565882 &  0.521523 &   0.671199 &  0.386186 \\\\\n","            &        &         &    & 6  &  0.738352 &  0.605450 &  0.606373 &   0.631243 &  0.313955 \\\\\n","            &        &         &    & 7  &  0.741477 &  0.627647 &  0.657827 &   0.618375 &  0.264552 \\\\\n","            &        &         &    & 8  &  0.736648 &  0.561382 &  0.549425 &   0.652188 &  0.210716 \\\\\n","            &        &         &    & 9  &  0.731250 &  0.551062 &  0.524480 &   0.636879 &  0.179984 \\\\\n","            &        &         &    & 10 &  0.740341 &  0.595472 &  0.581988 &   0.641583 &  0.123178 \\\\\n","            &        &         &    & 11 &  0.753409 &  0.574412 &  0.510760 &   0.687572 &  0.090759 \\\\\n","            &        &         &    & 12 &  0.739773 &  0.599764 &  0.595441 &   0.631140 &  0.088289 \\\\\n","            &        &         &    & 13 &  0.749148 &  0.585388 &  0.543260 &   0.659917 &  0.046569 \\\\\n","            &        &         &    & 14 &  0.745455 &  0.592277 &  0.567492 &   0.642869 &  0.038510 \\\\\n","            &        &         &    & 15 &  0.744318 &  0.586615 &  0.555703 &   0.647838 &  0.031474 \\\\\n","            &        &         &    & 16 &  0.749432 &  0.597147 &  0.569583 &   0.650810 &  0.025029 \\\\\n","            &        &         &    & 17 &  0.748580 &  0.594345 &  0.566854 &   0.649779 &  0.019668 \\\\\n","            &        &         &    & 18 &  0.746023 &  0.589678 &  0.562678 &   0.643569 &  0.019256 \\\\\n","            &        &         &    & 19 &  0.746023 &  0.589601 &  0.562672 &   0.643639 &  0.017258 \\\\\n","            &        &         &    & 20 &  0.744602 &  0.586058 &  0.556797 &   0.643360 &  0.021335 \\\\\n","            &        & 0.00003 & 8  & 1  &  0.730398 &  0.416903 &  0.340438 &   0.726747 &  0.568380 \\\\\n","            &        &         &    & 2  &  0.746307 &  0.475379 &  0.388921 &   0.711680 &  0.361804 \\\\\n","            &        &         &    & 3  &  0.737216 &  0.605261 &  0.616364 &   0.620869 &  0.248099 \\\\\n","            &        &         &    & 4  &  0.749148 &  0.598147 &  0.576100 &   0.663423 &  0.178820 \\\\\n","            &        &         &    & 5  &  0.756534 &  0.563553 &  0.487622 &   0.717397 &  0.097873 \\\\\n","            &        &         &    & 6  &  0.737216 &  0.566194 &  0.537410 &   0.649990 &  0.058942 \\\\\n","            &        &         &    & 7  &  0.739773 &  0.588995 &  0.570849 &   0.638528 &  0.027938 \\\\\n","            &        &         &    & 8  &  0.741193 &  0.577600 &  0.546548 &   0.643584 &  0.017836 \\\\\n","            &        &         &    & 9  &  0.750568 &  0.595936 &  0.566095 &   0.657540 &  0.019374 \\\\\n","            &        &         &    & 10 &  0.738920 &  0.581998 &  0.568637 &   0.635579 &  0.011785 \\\\\n","            &        &         &    & 11 &  0.734943 &  0.581224 &  0.575232 &   0.626719 &  0.009799 \\\\\n","            &        &         &    & 12 &  0.744318 &  0.606175 &  0.591647 &   0.641022 &  0.010042 \\\\\n","            &        &         &    & 13 &  0.748295 &  0.591403 &  0.574356 &   0.650336 &  0.011478 \\\\\n","            &        &         &    & 14 &  0.741477 &  0.587627 &  0.567780 &   0.640633 &  0.005237 \\\\\n","            &        &         &    & 15 &  0.747159 &  0.598086 &  0.569551 &   0.652322 &  0.003643 \\\\\n","            &        &         &    & 16 &  0.748295 &  0.590696 &  0.558153 &   0.655159 &  0.004961 \\\\\n","            &        &         &    & 17 &  0.746023 &  0.587370 &  0.555724 &   0.648801 &  0.003364 \\\\\n","            &        &         &    & 18 &  0.744034 &  0.589437 &  0.563959 &   0.641539 &  0.002880 \\\\\n","            &        &         &    & 19 &  0.745170 &  0.598704 &  0.576970 &   0.642962 &  0.002463 \\\\\n","            &        &         &    & 20 &  0.745455 &  0.594245 &  0.569172 &   0.645451 &  0.001965 \\\\\n","            &        &         & 16 & 1  &  0.722159 &  0.474478 &  0.422491 &   0.681749 &  0.610953 \\\\\n","            &        &         &    & 2  &  0.748295 &  0.554941 &  0.518881 &   0.705884 &  0.423295 \\\\\n","            &        &         &    & 3  &  0.738068 &  0.488841 &  0.406030 &   0.752849 &  0.253386 \\\\\n","            &        &         &    & 4  &  0.730682 &  0.601221 &  0.620451 &   0.640178 &  0.202045 \\\\\n","            &        &         &    & 5  &  0.742898 &  0.618628 &  0.640485 &   0.627488 &  0.095520 \\\\\n","            &        &         &    & 6  &  0.739489 &  0.587564 &  0.574845 &   0.640081 &  0.038000 \\\\\n","            &        &         &    & 7  &  0.748864 &  0.612969 &  0.601430 &   0.654567 &  0.023270 \\\\\n","            &        &         &    & 8  &  0.759091 &  0.609837 &  0.583927 &   0.675805 &  0.009722 \\\\\n","            &        &         &    & 9  &  0.754261 &  0.612923 &  0.589210 &   0.667659 &  0.006324 \\\\\n","            &        &         &    & 10 &  0.755398 &  0.609045 &  0.581940 &   0.669821 &  0.004187 \\\\\n","            &        &         &    & 11 &  0.753693 &  0.608465 &  0.579369 &   0.668896 &  0.002656 \\\\\n","            &        &         &    & 12 &  0.757955 &  0.608516 &  0.570951 &   0.678479 &  0.004365 \\\\\n","            &        &         &    & 13 &  0.755682 &  0.609783 &  0.582744 &   0.669932 &  0.003541 \\\\\n","            &        &         &    & 14 &  0.758807 &  0.610118 &  0.575690 &   0.677924 &  0.001511 \\\\\n","            &        &         &    & 15 &  0.755966 &  0.613976 &  0.592167 &   0.662003 &  0.001317 \\\\\n","            &        &         &    & 16 &  0.755398 &  0.609819 &  0.586971 &   0.664779 &  0.002013 \\\\\n","            &        &         &    & 17 &  0.753977 &  0.612133 &  0.594930 &   0.661492 &  0.001870 \\\\\n","            &        &         &    & 18 &  0.753693 &  0.612457 &  0.596496 &   0.657158 &  0.001026 \\\\\n","            &        &         &    & 19 &  0.755398 &  0.613067 &  0.593268 &   0.661962 &  0.000902 \\\\\n","            &        &         &    & 20 &  0.755682 &  0.612853 &  0.591654 &   0.663237 &  0.001101 \\\\\n","            &        &         & 32 & 1  &  0.724716 &  0.445735 &  0.356971 &   0.708811 &  0.621925 \\\\\n","            &        &         &    & 2  &  0.747443 &  0.574052 &  0.523042 &   0.676230 &  0.445120 \\\\\n","            &        &         &    & 3  &  0.737500 &  0.602474 &  0.601852 &   0.643728 &  0.259680 \\\\\n","            &        &         &    & 4  &  0.705398 &  0.558056 &  0.591331 &   0.607838 &  0.181557 \\\\\n","            &        &         &    & 5  &  0.735795 &  0.541282 &  0.525686 &   0.666306 &  0.117148 \\\\\n","            &        &         &    & 6  &  0.712784 &  0.606164 &  0.675886 &   0.577795 &  0.067896 \\\\\n","            &        &         &    & 7  &  0.725000 &  0.587009 &  0.629922 &   0.609497 &  0.057182 \\\\\n","            &        &         &    & 8  &  0.742045 &  0.573570 &  0.573218 &   0.646984 &  0.058748 \\\\\n","            &        &         &    & 9  &  0.749432 &  0.581661 &  0.535782 &   0.679536 &  0.051445 \\\\\n","            &        &         &    & 10 &  0.753409 &  0.599840 &  0.588266 &   0.654292 &  0.043222 \\\\\n","            &        &         &    & 11 &  0.752273 &  0.584777 &  0.537812 &   0.689421 &  0.007136 \\\\\n","            &        &         &    & 12 &  0.753125 &  0.599318 &  0.574820 &   0.659098 &  0.006920 \\\\\n","            &        &         &    & 13 &  0.750000 &  0.600811 &  0.585418 &   0.641660 &  0.002622 \\\\\n","            &        &         &    & 14 &  0.750568 &  0.597548 &  0.575158 &   0.646887 &  0.001188 \\\\\n","            &        &         &    & 15 &  0.751705 &  0.598361 &  0.574825 &   0.650964 &  0.001770 \\\\\n","            &        &         &    & 16 &  0.751136 &  0.599057 &  0.577989 &   0.649781 &  0.000945 \\\\\n","            &        &         &    & 17 &  0.752273 &  0.599701 &  0.577265 &   0.650914 &  0.000788 \\\\\n","            &        &         &    & 18 &  0.753409 &  0.602177 &  0.580634 &   0.650867 &  0.000832 \\\\\n","            &        &         &    & 19 &  0.752841 &  0.601397 &  0.579801 &   0.649845 &  0.000823 \\\\\n","            &        &         &    & 20 &  0.753409 &  0.603079 &  0.582202 &   0.650311 &  0.000754 \\\\\n","            &        &         & 64 & 1  &  0.667898 &  0.049202 &  0.027732 &   0.262500 &  0.676184 \\\\\n","            &        &         &    & 2  &  0.727273 &  0.524165 &  0.492772 &   0.668511 &  0.602200 \\\\\n","            &        &         &    & 3  &  0.742330 &  0.552548 &  0.503843 &   0.688433 &  0.483126 \\\\\n","            &        &         &    & 4  &  0.740057 &  0.572694 &  0.555840 &   0.665340 &  0.347744 \\\\\n","            &        &         &    & 5  &  0.731250 &  0.559571 &  0.538326 &   0.643230 &  0.204938 \\\\\n","            &        &         &    & 6  &  0.719318 &  0.569388 &  0.593591 &   0.611365 &  0.096699 \\\\\n","            &        &         &    & 7  &  0.730398 &  0.614030 &  0.651592 &   0.604445 &  0.055887 \\\\\n","            &        &         &    & 8  &  0.698864 &  0.587195 &  0.659716 &   0.617618 &  0.037142 \\\\\n","            &        &         &    & 9  &  0.730398 &  0.571850 &  0.562422 &   0.655818 &  0.295193 \\\\\n","            &        &         &    & 10 &  0.736080 &  0.550355 &  0.531383 &   0.657906 &  0.084605 \\\\\n","            &        &         &    & 11 &  0.739205 &  0.593338 &  0.585559 &   0.659547 &  0.080419 \\\\\n","            &        &         &    & 12 &  0.747727 &  0.602826 &  0.595928 &   0.642876 &  0.043749 \\\\\n","            &        &         &    & 13 &  0.753409 &  0.585923 &  0.549365 &   0.668339 &  0.015038 \\\\\n","            &        &         &    & 14 &  0.757386 &  0.597961 &  0.560370 &   0.673127 &  0.012119 \\\\\n","            &        &         &    & 15 &  0.756250 &  0.614176 &  0.592546 &   0.660486 &  0.007123 \\\\\n","            &        &         &    & 16 &  0.756534 &  0.623729 &  0.611697 &   0.656825 &  0.003672 \\\\\n","            &        &         &    & 17 &  0.755682 &  0.621099 &  0.608495 &   0.654847 &  0.003256 \\\\\n","            &        &         &    & 18 &  0.753409 &  0.616349 &  0.604010 &   0.650783 &  0.002883 \\\\\n","            &        &         &    & 19 &  0.752557 &  0.611934 &  0.595841 &   0.650419 &  0.002379 \\\\\n","            &        &         &    & 20 &  0.752557 &  0.611164 &  0.594028 &   0.651765 &  0.002287 \\\\\n","            &        & 0.00005 & 8  & 1  &  0.743466 &  0.480045 &  0.411992 &   0.645081 &  0.568105 \\\\\n","            &        &         &    & 2  &  0.731818 &  0.479047 &  0.405832 &   0.721610 &  0.386073 \\\\\n","            &        &         &    & 3  &  0.729545 &  0.473595 &  0.432812 &   0.669986 &  0.315250 \\\\\n","            &        &         &    & 4  &  0.713636 &  0.548287 &  0.554436 &   0.622555 &  0.253840 \\\\\n","            &        &         &    & 5  &  0.719886 &  0.525363 &  0.520856 &   0.599726 &  0.171244 \\\\\n","            &        &         &    & 6  &  0.728693 &  0.566020 &  0.544238 &   0.647539 &  0.122376 \\\\\n","            &        &         &    & 7  &  0.731534 &  0.573234 &  0.541201 &   0.647705 &  0.109755 \\\\\n","            &        &         &    & 8  &  0.727557 &  0.553420 &  0.515724 &   0.639584 &  0.064753 \\\\\n","            &        &         &    & 9  &  0.728409 &  0.578694 &  0.572846 &   0.616277 &  0.057503 \\\\\n","            &        &         &    & 10 &  0.723864 &  0.571866 &  0.561236 &   0.608730 &  0.038649 \\\\\n","            &        &         &    & 11 &  0.730398 &  0.567524 &  0.551345 &   0.620335 &  0.028184 \\\\\n","            &        &         &    & 12 &  0.731534 &  0.583958 &  0.570805 &   0.618583 &  0.021988 \\\\\n","            &        &         &    & 13 &  0.729545 &  0.577043 &  0.559747 &   0.618595 &  0.021109 \\\\\n","            &        &         &    & 14 &  0.730966 &  0.571801 &  0.550690 &   0.621460 &  0.020798 \\\\\n","            &        &         &    & 15 &  0.729830 &  0.577297 &  0.565367 &   0.617557 &  0.018157 \\\\\n","            &        &         &    & 16 &  0.734091 &  0.577789 &  0.553932 &   0.629503 &  0.017657 \\\\\n","            &        &         &    & 17 &  0.733807 &  0.572518 &  0.545499 &   0.628682 &  0.015347 \\\\\n","            &        &         &    & 18 &  0.733523 &  0.577196 &  0.555183 &   0.625502 &  0.013816 \\\\\n","            &        &         &    & 19 &  0.731534 &  0.578275 &  0.561740 &   0.620740 &  0.014852 \\\\\n","            &        &         &    & 20 &  0.730114 &  0.574960 &  0.556274 &   0.619500 &  0.012317 \\\\\n","            &        &         & 16 & 1  &  0.726136 &  0.400186 &  0.333317 &   0.709965 &  0.592262 \\\\\n","            &        &         &    & 2  &  0.729545 &  0.525476 &  0.486420 &   0.695291 &  0.415986 \\\\\n","            &        &         &    & 3  &  0.752841 &  0.582566 &  0.547036 &   0.689287 &  0.250513 \\\\\n","            &        &         &    & 4  &  0.738352 &  0.516846 &  0.465018 &   0.702782 &  0.169142 \\\\\n","            &        &         &    & 5  &  0.737500 &  0.574418 &  0.552717 &   0.644675 &  0.144534 \\\\\n","            &        &         &    & 6  &  0.738636 &  0.618375 &  0.628391 &   0.633590 &  0.043906 \\\\\n","            &        &         &    & 7  &  0.738068 &  0.604417 &  0.614823 &   0.628002 &  0.039012 \\\\\n","            &        &         &    & 8  &  0.740909 &  0.583849 &  0.560890 &   0.656340 &  0.032116 \\\\\n","            &        &         &    & 9  &  0.755682 &  0.611705 &  0.600598 &   0.674662 &  0.034604 \\\\\n","            &        &         &    & 10 &  0.750000 &  0.613445 &  0.602759 &   0.647751 &  0.014855 \\\\\n","            &        &         &    & 11 &  0.740909 &  0.604720 &  0.600558 &   0.643103 &  0.007530 \\\\\n","            &        &         &    & 12 &  0.750000 &  0.601949 &  0.569785 &   0.661294 &  0.007084 \\\\\n","            &        &         &    & 13 &  0.755114 &  0.602364 &  0.570262 &   0.665586 &  0.001797 \\\\\n","            &        &         &    & 14 &  0.756534 &  0.601992 &  0.566196 &   0.672830 &  0.001719 \\\\\n","            &        &         &    & 15 &  0.753693 &  0.605973 &  0.575556 &   0.665863 &  0.001876 \\\\\n","            &        &         &    & 16 &  0.755114 &  0.605103 &  0.572197 &   0.667542 &  0.000920 \\\\\n","            &        &         &    & 17 &  0.757102 &  0.605660 &  0.570002 &   0.672221 &  0.000908 \\\\\n","            &        &         &    & 18 &  0.756534 &  0.604639 &  0.569344 &   0.671141 &  0.000886 \\\\\n","            &        &         &    & 19 &  0.754830 &  0.604808 &  0.574050 &   0.666162 &  0.000842 \\\\\n","            &        &         &    & 20 &  0.754261 &  0.605172 &  0.575845 &   0.664807 &  0.001148 \\\\\n","            &        &         & 32 & 1  &  0.706534 &  0.372696 &  0.339919 &   0.675125 &  0.629337 \\\\\n","            &        &         &    & 2  &  0.745170 &  0.553538 &  0.496659 &   0.701636 &  0.477708 \\\\\n","            &        &         &    & 3  &  0.736648 &  0.574278 &  0.558392 &   0.670568 &  0.271096 \\\\\n","            &        &         &    & 4  &  0.728125 &  0.579045 &  0.591277 &   0.633256 &  0.216913 \\\\\n","            &        &         &    & 5  &  0.711080 &  0.584991 &  0.630074 &   0.592400 &  0.149220 \\\\\n","            &        &         &    & 6  &  0.733523 &  0.567434 &  0.565024 &   0.646328 &  0.073937 \\\\\n","            &        &         &    & 7  &  0.738636 &  0.583709 &  0.571479 &   0.626242 &  0.035704 \\\\\n","            &        &         &    & 8  &  0.752273 &  0.588036 &  0.559301 &   0.663631 &  0.017882 \\\\\n","            &        &         &    & 9  &  0.743182 &  0.569603 &  0.520163 &   0.669030 &  0.023962 \\\\\n","            &        &         &    & 10 &  0.743750 &  0.569860 &  0.536265 &   0.662268 &  0.022195 \\\\\n","            &        &         &    & 11 &  0.746591 &  0.602086 &  0.584532 &   0.657296 &  0.012790 \\\\\n","            &        &         &    & 12 &  0.742898 &  0.608584 &  0.614711 &   0.624339 &  0.004343 \\\\\n","            &        &         &    & 13 &  0.741761 &  0.582772 &  0.555863 &   0.640257 &  0.003365 \\\\\n","            &        &         &    & 14 &  0.741477 &  0.583415 &  0.554709 &   0.642094 &  0.001305 \\\\\n","            &        &         &    & 15 &  0.744602 &  0.580674 &  0.543406 &   0.653857 &  0.001008 \\\\\n","            &        &         &    & 16 &  0.744602 &  0.586832 &  0.554993 &   0.650027 &  0.002215 \\\\\n","            &        &         &    & 17 &  0.746307 &  0.595019 &  0.567811 &   0.648191 &  0.000754 \\\\\n","            &        &         &    & 18 &  0.745170 &  0.592597 &  0.565585 &   0.645604 &  0.000750 \\\\\n","            &        &         &    & 19 &  0.744602 &  0.590782 &  0.563713 &   0.643247 &  0.000647 \\\\\n","            &        &         &    & 20 &  0.745170 &  0.591869 &  0.565440 &   0.643742 &  0.000621 \\\\\n","            &        &         & 64 & 1  &  0.664773 &  0.035849 &  0.021209 &   0.140208 &  0.679878 \\\\\n","            &        &         &    & 2  &  0.692045 &  0.312199 &  0.273941 &   0.521237 &  0.625791 \\\\\n","            &        &         &    & 3  &  0.707955 &  0.540072 &  0.565571 &   0.653875 &  0.591565 \\\\\n","            &        &         &    & 4  &  0.742045 &  0.609879 &  0.625421 &   0.647693 &  0.443917 \\\\\n","            &        &         &    & 5  &  0.747727 &  0.580237 &  0.560246 &   0.677069 &  0.269539 \\\\\n","            &        &         &    & 6  &  0.751136 &  0.559176 &  0.517181 &   0.716983 &  0.173748 \\\\\n","            &        &         &    & 7  &  0.711648 &  0.567166 &  0.640138 &   0.604809 &  0.145845 \\\\\n","            &        &         &    & 8  &  0.733807 &  0.600794 &  0.640501 &   0.621411 &  0.248829 \\\\\n","            &        &         &    & 9  &  0.746023 &  0.600200 &  0.590368 &   0.665887 &  0.124602 \\\\\n","            &        &         &    & 10 &  0.749148 &  0.583294 &  0.553460 &   0.664628 &  0.090462 \\\\\n","            &        &         &    & 11 &  0.744602 &  0.608322 &  0.627023 &   0.620917 &  0.048064 \\\\\n","            &        &         &    & 12 &  0.756534 &  0.617324 &  0.599651 &   0.657511 &  0.020571 \\\\\n","            &        &         &    & 13 &  0.752273 &  0.612655 &  0.594598 &   0.654423 &  0.009342 \\\\\n","            &        &         &    & 14 &  0.755398 &  0.624468 &  0.620312 &   0.648528 &  0.006428 \\\\\n","            &        &         &    & 15 &  0.752841 &  0.616663 &  0.609650 &   0.645902 &  0.006102 \\\\\n","            &        &         &    & 16 &  0.753125 &  0.615895 &  0.607050 &   0.647149 &  0.004763 \\\\\n","            &        &         &    & 17 &  0.748864 &  0.608242 &  0.598901 &   0.641783 &  0.002519 \\\\\n","            &        &         &    & 18 &  0.749432 &  0.607221 &  0.594453 &   0.643004 &  0.002429 \\\\\n","            &        &         &    & 19 &  0.749432 &  0.607502 &  0.594756 &   0.643796 &  0.002171 \\\\\n","            &        &         &    & 20 &  0.749148 &  0.607561 &  0.595589 &   0.643114 &  0.002141 \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\end{table}\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pR_Zy_eIcl2i"},"source":["# Inference"]},{"cell_type":"markdown","metadata":{"id":"Hx8CXkIr3NuW"},"source":["##Train the model with Full Train dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"suGc2RmyuPzk","executionInfo":{"status":"ok","timestamp":1619786377671,"user_tz":180,"elapsed":576,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"db8f01ac-b4c3-4f51-ecd3-93c5cea9b79d"},"source":["## 10 Best resuts\n","MetricForBestResults = 'cem' if df_train['Label'].nunique() > 2 else 'f1'\n","DfResultsTask.nlargest(n=10, columns= MetricForBestResults )"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"10\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"6\" valign=\"top\">pooler</th>\n","      <th rowspan=\"3\" valign=\"top\">0.00001</th>\n","      <th>32</th>\n","      <th>4</th>\n","      <td>0.744602</td>\n","      <td>0.631424</td>\n","      <td>0.651356</td>\n","      <td>0.633806</td>\n","      <td>0.292655</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <th>7</th>\n","      <td>0.741477</td>\n","      <td>0.627647</td>\n","      <td>0.657827</td>\n","      <td>0.618375</td>\n","      <td>0.264552</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <th>7</th>\n","      <td>0.726705</td>\n","      <td>0.626452</td>\n","      <td>0.682886</td>\n","      <td>0.601620</td>\n","      <td>0.105680</td>\n","    </tr>\n","    <tr>\n","      <th>0.00005</th>\n","      <th>64</th>\n","      <th>14</th>\n","      <td>0.755398</td>\n","      <td>0.624468</td>\n","      <td>0.620312</td>\n","      <td>0.648528</td>\n","      <td>0.006428</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">0.00003</th>\n","      <th rowspan=\"2\" valign=\"top\">64</th>\n","      <th>16</th>\n","      <td>0.756534</td>\n","      <td>0.623729</td>\n","      <td>0.611697</td>\n","      <td>0.656825</td>\n","      <td>0.003672</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.755682</td>\n","      <td>0.621099</td>\n","      <td>0.608495</td>\n","      <td>0.654847</td>\n","      <td>0.003256</td>\n","    </tr>\n","    <tr>\n","      <th>hidden</th>\n","      <th>0.00001</th>\n","      <th>32</th>\n","      <th>5</th>\n","      <td>0.742614</td>\n","      <td>0.619629</td>\n","      <td>0.627808</td>\n","      <td>0.642867</td>\n","      <td>0.230715</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">pooler</th>\n","      <th>0.00003</th>\n","      <th>16</th>\n","      <th>5</th>\n","      <td>0.742898</td>\n","      <td>0.618628</td>\n","      <td>0.640485</td>\n","      <td>0.627488</td>\n","      <td>0.095520</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">0.00005</th>\n","      <th>16</th>\n","      <th>6</th>\n","      <td>0.738636</td>\n","      <td>0.618375</td>\n","      <td>0.628391</td>\n","      <td>0.633590</td>\n","      <td>0.043906</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <th>12</th>\n","      <td>0.756534</td>\n","      <td>0.617324</td>\n","      <td>0.599651</td>\n","      <td>0.657511</td>\n","      <td>0.020571</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  accuracy        f1  ...  precision      loss\n","SpanishBert pooler 0.00001 32 4   0.744602  0.631424  ...   0.633806  0.292655\n","                           64 7   0.741477  0.627647  ...   0.618375  0.264552\n","                           16 7   0.726705  0.626452  ...   0.601620  0.105680\n","                   0.00005 64 14  0.755398  0.624468  ...   0.648528  0.006428\n","                   0.00003 64 16  0.756534  0.623729  ...   0.656825  0.003672\n","                              17  0.755682  0.621099  ...   0.654847  0.003256\n","            hidden 0.00001 32 5   0.742614  0.619629  ...   0.642867  0.230715\n","            pooler 0.00003 16 5   0.742898  0.618628  ...   0.627488  0.095520\n","                   0.00005 16 6   0.738636  0.618375  ...   0.633590  0.043906\n","                           64 12  0.756534  0.617324  ...   0.657511  0.020571\n","\n","[10 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9Dh28hHvWw8","executionInfo":{"status":"ok","timestamp":1619785114532,"user_tz":180,"elapsed":666,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"01e6c98b-1cc6-4f86-a46f-dbc42725d214"},"source":["## Get best parameters from cross-validation DataFrame \n","BestResultParameters = DfResultsTask.sort_values(MetricForBestResults, ascending=False)[:1].index\n","print(f'Best parameters : {BestResultParameters}')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Best parameters : MultiIndex([('SpanishBert', 'pooler', 1e-05, 32, 4)],\n","           )\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-2gzpiLswJOL","executionInfo":{"status":"ok","timestamp":1619785117186,"user_tz":180,"elapsed":706,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Add best parameters to variables in the final train\n","BertPath = BertVersion[BestResultParameters[0][0]]\n","BertVersion = {BestResultParameters[0][0] : BertVersion[BestResultParameters[0][0]]}\n","OutputBert = [BestResultParameters[0][1]]\n","LearningRate = [float(BestResultParameters[0][2])]\n","BatchSize = [int(BestResultParameters[0][3])]\n","Epochs = int(BestResultParameters[0][4])"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNPAwRNtfrev","executionInfo":{"status":"ok","timestamp":1619785119000,"user_tz":180,"elapsed":552,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Criate dictinaril results\n","ResultsTaskBestParameters = { bert:{ output:{ lr:{ bat:{ epoc:{ metric:[] for metric in Metrics + ['loss']} for epoc in range(1, Epochs+1) } for bat in BatchSize} for lr in LearningRate} for output in OutputBert } for bert in BertVersion.keys() }\n","\n","## Create file to save results BEST Parameters\n","#### Create file name\n","FileResultsBestModel = FileResults + 'BestModel'\n","#### Save the file fro results BEST Parameters\n","with open(Path + FileResultsBestModel + \".pkl\",'wb') as f:\n","  pickle.dump(ResultsTaskBestParameters, f)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7e2YUAfxufW","executionInfo":{"status":"ok","timestamp":1619785199743,"user_tz":180,"elapsed":76509,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"6bed0f04-b365-4310-b3ab-415f5813aa59"},"source":["## Train with Best parameters\n","\n","## Best parameters\n","BertV = BestResultParameters[0][0]\n","BertPath = BertVersion[BestResultParameters[0][0]]\n","OutputB = OutputBert[0]\n","lr = LearningRate[0]\n","Batch = BatchSize[0]\n","Epochs = Epochs\n","\n","### Loading Bert trained weights\n","mx = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique())\n","\n","## Split train and test\n","X_train = df_train['Data']\n","y_train = df_train['Label']\n","_, X_test, _, y_test = train_test_split(df_train['Data'], df_train['Label'], test_size=0.33, random_state=42)\n","\n","print(f'parameters: Bertmodel: {BertV}, Output: {OutputB}, lr: {lr}, Batch: {Batch}, Totsl Num. Epochs: {Epochs}')\n","MoDeL = TrainModel(PathSaveFiles = Path,\n","                  BertVersion=BertV,\n","                  BertPath=BertPath,\n","                  OutputBert=OutputB,\n","                  LearningRate=lr,\n","                  BatchSize=Batch,\n","                  Epochs=Epochs,\n","                  FileName= FileResultsBestModel,\n","                  X_train=X_train, \n","                  X_valid=X_test,\n","                  y_train=y_train,\n","                  y_valid=y_test,\n","                  SaveModel=True)\n","\n","\n","def _mp_fn(rank, flags):\n","  torch.set_default_tensor_type('torch.FloatTensor')\n","  a = MoDeL._run()\n","\n","FLAGS={}\n","xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ were not used when initializing BertModel: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["parameters: Bertmodel: SpanishBert, Output: pooler, lr: 1e-05, Batch: 32, Totsl Num. Epochs: 4\n","num_train_steps = 54, world_size=8\n","Epoch: 1 of 4\n","bi=0, loss=0.6444810032844543\n","bi=10, loss=0.5622158646583557\n","Accuracy = 0.7062937062937062\n","Epoch: 2 of 4\n","bi=0, loss=0.4690134525299072\n","bi=10, loss=0.4536563754081726\n","Accuracy = 0.8006993006993006\n","Epoch: 3 of 4\n","bi=0, loss=0.3437179923057556\n","bi=10, loss=0.32595375180244446\n","Accuracy = 0.8496503496503496\n","Epoch: 4 of 4\n","bi=0, loss=0.248634472489357\n","bi=10, loss=0.32738834619522095\n","Accuracy = 0.8697552447552447\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kC7Qio7rfXpM","executionInfo":{"status":"ok","timestamp":1619785215850,"user_tz":180,"elapsed":785,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Average and Save Results\n","AverageResultsTaskBestModel = AveragResults(FileName=FileResultsBestModel, Path=Path)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"CPKruLgagBtS","executionInfo":{"status":"ok","timestamp":1619785216772,"user_tz":180,"elapsed":459,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"632a5374-3943-4d20-a18d-902da5f3277e"},"source":["## Create a Data Frame\n","DfResultsTaskBestModel = create_Data_Frame(all_resultas=AverageResultsTaskBestModel)\n","\n","### save results to a CSV file\n","DfResultsTaskBestModel.to_csv(Path + 'Average' + FileResultsBestModel + '_CSV_' + '.csv')\n","\n","### See the Avarage results in the Pandas data Frame\n","DfResultsTaskBestModel"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">SpanishBert</th>\n","      <th rowspan=\"4\" valign=\"top\">pooler</th>\n","      <th rowspan=\"4\" valign=\"top\">0.00001</th>\n","      <th rowspan=\"4\" valign=\"top\">32</th>\n","      <th>1</th>\n","      <td>0.706294</td>\n","      <td>0.233842</td>\n","      <td>0.137556</td>\n","      <td>0.833523</td>\n","      <td>0.614177</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.800699</td>\n","      <td>0.615392</td>\n","      <td>0.486321</td>\n","      <td>0.843831</td>\n","      <td>0.515099</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.849650</td>\n","      <td>0.738477</td>\n","      <td>0.647090</td>\n","      <td>0.864639</td>\n","      <td>0.398094</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.869755</td>\n","      <td>0.801625</td>\n","      <td>0.800512</td>\n","      <td>0.804803</td>\n","      <td>0.342449</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 accuracy        f1  ...  precision      loss\n","SpanishBert pooler 0.00001 32 1  0.706294  0.233842  ...   0.833523  0.614177\n","                              2  0.800699  0.615392  ...   0.843831  0.515099\n","                              3  0.849650  0.738477  ...   0.864639  0.398094\n","                              4  0.869755  0.801625  ...   0.804803  0.342449\n","\n","[4 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"Iei-0sLc3QRI"},"source":["## Inference on Test Dataset"]},{"cell_type":"markdown","metadata":{"id":"leSgnpUJ_UBq"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"AJb7MHXj4XbC","executionInfo":{"status":"ok","timestamp":1619785232899,"user_tz":180,"elapsed":587,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["PathDataSet = \"../content/drive/MyDrive/Code/DETOXIS/Data/test.csv\"\n","df_test = pd.read_csv(PathDataSet, usecols=[\"comment_id\",\"comment\"]).fillna(\"none\")\n","NewColumnsNames = {\"comment\":\"Data\"}\n","df_test = df_test.rename(columns=NewColumnsNames)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5h4GIZCiqEV","executionInfo":{"status":"ok","timestamp":1619785234041,"user_tz":180,"elapsed":551,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["class BERTDatasetTest:\n","    def __init__(self, comment_text, tokenizer, max_length):\n","        self.comment_text = comment_text\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, item):\n","        comment_text = str(self.comment_text[item])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            truncation=True,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_length = self.max_length - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n","        }"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVIuyw9PiqIC","executionInfo":{"status":"ok","timestamp":1619785244796,"user_tz":180,"elapsed":536,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Bert tozenizer\n","tokenizer = transformers.BertTokenizer.from_pretrained(BertPath, do_lower_case=True)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-wbaFWcitwr","executionInfo":{"status":"ok","timestamp":1619785315715,"user_tz":180,"elapsed":66124,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"22c9e550-9786-4913-ee4f-f6a3fb0f907c"},"source":["## Loading the best model\n","device = torch.device(\"xla\")\n","model = BERTBaseUncased(bert_path=BertPath, output_bert=OutputB, NumberOfClasses=df_train['Label'].nunique()).to(device)\n","FileBestModel = Path + FileResultsBestModel + '.bin'\n","model.load_state_dict(torch.load(FileBestModel))\n","model.eval()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ were not used when initializing BertModel: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ../content/bert-base-spanish-wwm-uncased/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BERTBaseUncased(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (bert_drop): Dropout(p=0.3, inplace=False)\n","  (out_last_hidden): Linear(in_features=1536, out_features=1, bias=True)\n","  (out_poller): Linear(in_features=768, out_features=1, bias=True)\n","  (out_last_hidden_MultiClasss): Linear(in_features=1536, out_features=2, bias=True)\n","  (out_poller_MultiClasss): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"fZwzG9CD3De_","executionInfo":{"status":"ok","timestamp":1619785322092,"user_tz":180,"elapsed":528,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## Prepresing the data\n","valid_dataset = BERTDatasetTest(\n","        comment_text=df_test['Data'].values,\n","        tokenizer=tokenizer,\n","        max_length=192\n",")\n","\n","valid_data_loader = torch.utils.data.DataLoader(\n","    valid_dataset,\n","    batch_size=Batch,\n","    drop_last=False,\n","    num_workers=4,\n","    shuffle=False\n",")"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r70_1m-3Dhh","executionInfo":{"status":"ok","timestamp":1619785429033,"user_tz":180,"elapsed":13553,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"9e3a3fcc-4585-46a6-bc48-2b3bbedb627a"},"source":["## Making the Inferences\n","with torch.no_grad():\n","    fin_outputs = []\n","    for bi, d in tqdm(enumerate(valid_data_loader)):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","\n","        outputs = model(\n","            ids=ids,\n","            mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","\n","        outputs_np = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n","        # outputs_np = outputs.detach().cpu().numpy().tolist()\n","        fin_outputs.extend(outputs_np) "],"execution_count":33,"outputs":[{"output_type":"stream","text":["28it [00:12,  2.21it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"BeS8ujZQApyS","executionInfo":{"status":"ok","timestamp":1619785455042,"user_tz":180,"elapsed":572,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"c8cfaf3a-bf3f-491f-d009-9e5785bdd2fc"},"source":["## List with Results\n","fin_outputs\n","\n","## create a Dataframe from List of Results\n","df_results = pd.DataFrame.from_records(fin_outputs)\n","\n","# ## get the model inference\n","# df_results['Inference'] = df_results.idxmax(axis=1)\n","df_results['Inference'] = df_results[0].apply(lambda x : 1 if x > 0.5 else 0)\n","\n","## Visualize results\n","df_results.head()"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.806787</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.115982</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.786314</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.851175</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.742455</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0  Inference\n","0  0.806787          1\n","1  0.115982          0\n","2  0.786314          1\n","3  0.851175          1\n","4  0.742455          1"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"LHcAnfzkMO-q","executionInfo":{"status":"ok","timestamp":1619785499560,"user_tz":180,"elapsed":585,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"9347adb5-ec68-4df5-de33-112e3c058bc3"},"source":["## Get rows index\n","df_idex = df_test.loc[:,\"comment_id\"].to_frame()\n","\n","## Add index to the Results dataframe\n","df_results = df_results.join(df_idex)\n","\n","### save results to a CSV file\n","df_results.to_csv(Path + 'ModelInfereneces' + FileResultsBestModel + '_CSV_' + '.csv')\n","\n","## ## Visualize results\n","df_results.head()"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>Inference</th>\n","      <th>comment_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.806787</td>\n","      <td>1</td>\n","      <td>10_001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.115982</td>\n","      <td>0</td>\n","      <td>10_002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.786314</td>\n","      <td>1</td>\n","      <td>10_003</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.851175</td>\n","      <td>1</td>\n","      <td>10_004</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.742455</td>\n","      <td>1</td>\n","      <td>10_005</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0  Inference comment_id\n","0  0.806787          1     10_001\n","1  0.115982          0     10_002\n","2  0.786314          1     10_003\n","3  0.851175          1     10_004\n","4  0.742455          1     10_005"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"rn24j0HBIUfD","executionInfo":{"status":"ok","timestamp":1619785511035,"user_tz":180,"elapsed":545,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"ed8ddf78-ab4c-43ee-80c9-a90d858164c2"},"source":["# Change the data to the DETOXIS format submition\n","### cerate a data frame only with the labels and ids\n","df_SubmationResults = df_results.loc[:, ['Inference', 'comment_id']]\n","df_SubmationResults.head()"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Inference</th>\n","      <th>comment_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>10_001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>10_002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>10_003</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>10_004</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>10_005</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Inference comment_id\n","0          1     10_001\n","1          0     10_002\n","2          1     10_003\n","3          1     10_004\n","4          1     10_005"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"VTgjyaYvJk_5","executionInfo":{"status":"ok","timestamp":1619785518745,"user_tz":180,"elapsed":561,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"fdc1ce63-212b-4c25-f191-f9d9581e572c"},"source":["## create a new id column\n","df_SubmationResults['id'] = np.arange(len(df_SubmationResults))\n","## removing olde id comment_id column\n","df_SubmationResults = df_SubmationResults.loc[:,['id', 'Inference']]\n","#submation format\n","df_SubmationResults.head()"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference\n","0   0          1\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          1"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"lJsKqLRYxE3j","executionInfo":{"status":"ok","timestamp":1619785555080,"user_tz":180,"elapsed":532,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["## saive inferences as .tsv\n","Path = 'drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/'\n","FileName = 'AI-UPV_subtask1_1'\n","df_SubmationResults.to_csv( Path + FileName + '.tsv', header=False, sep='\\t', index=False)"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anhhub8MwEj0"},"source":["# Reviewing results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"aCjcopNlE_1L","executionInfo":{"status":"ok","timestamp":1619785780272,"user_tz":180,"elapsed":695,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"41ef349d-a133-41a0-912f-d14bb1188d4f"},"source":["df = pd.read_csv(Path + 'Submited_' + FileName + '.tsv', header=None, sep='\\t')\n","df = df.rename({0:'id', 1:'Inference'}, axis=1)\n","df.head()"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference\n","0   0          0\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          1"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"KoRuAYAVJuVc","executionInfo":{"status":"ok","timestamp":1619785835762,"user_tz":180,"elapsed":544,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"ec5ceded-bdc6-430a-ce0a-6caf69c90821"},"source":["df['Submited_Inference'] = df_SubmationResults['Inference']\n","df.head()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","      <th>Submited_Inference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference  Submited_Inference\n","0   0          0                   1\n","1   1          0                   0\n","2   2          1                   1\n","3   3          1                   1\n","4   4          1                   1"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"ovXgMFHDLAHV","executionInfo":{"status":"ok","timestamp":1619785839727,"user_tz":180,"elapsed":903,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"23560022-b474-488d-d422-efe8c19296df"},"source":["df['check'] = df.apply(lambda x: 1 if x.Inference != x.Submited_Inference else 0, axis=1)\n","df.head()"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Inference</th>\n","      <th>Submited_Inference</th>\n","      <th>check</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  Inference  Submited_Inference  check\n","0   0          0                   1      1\n","1   1          0                   0      0\n","2   2          1                   1      0\n","3   3          1                   1      0\n","4   4          1                   1      0"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBda5EoLL6U3","executionInfo":{"status":"ok","timestamp":1619785842011,"user_tz":180,"elapsed":553,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}},"outputId":"edb89a98-11a9-43ac-99cd-012ae42bf77c"},"source":["df.check.sum()"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"ZexzKD0k8nN5"},"source":["# Util when the process stops sandly"]},{"cell_type":"code","metadata":{"id":"EyR_w29Jl71U","executionInfo":{"status":"ok","timestamp":1619780760167,"user_tz":180,"elapsed":949,"user":{"displayName":"Angel Felipe Magnossão de Paula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH_wFI1gQCBAL2qZw2jyZm5Oys0n0a_3m48vo=s64","userId":"01261253671233798051"}}},"source":["# import pickle\n","# path = \n","# File = \n","#  with open(path + File + \".pkl\", \"rb\") as f:\n","#    Re = pickle.load(f)\n","# Re"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-eBqL2fl1Ci"},"source":["# def CleanBrokeTrain(FileName, Path, NumberOfFoldes=10):\n","#   with open(Path + FileName + \".pkl\", \"rb\") as f:\n","#               Results = pickle.load(f)\n","\n","#   for BT, ModelBertType,  in Results.items():\n","#     for OP, OutPut in ModelBertType.items():\n","#       for LR, LearningRate in OutPut.items():\n","#         for BS, BatchSize in LearningRate.items():\n","#           for EP, Epoch in BatchSize.items():\n","#             for Metrics, ValuesCrossValidation in  Epoch.items():\n"," \n","#               if len(ValuesCrossValidation) != 0 and not len(ValuesCrossValidation) == NumberOfFoldes:\n","#                 Results[BT][OP][LR][BS][EP][Metrics] = []\n","            \n","#   with open(FileName + '.pkl','wb') as f:\n","#     pickle.dump(Results, f)\n","\n","#   with open(Path + FileName + '.pkl','wb') as f:\n","#     pickle.dump(Results, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mmbpB-nzzBhE"},"source":["# CleanBrokeTrain(FileName=FileResults, Path=Path, NumberOfFoldes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HbAOF5TGxklD"},"source":["# import pickle\n","# with open('drive/MyDrive/Code/DETOXIS/Machine-Learning-Tweets-Classification/Bert/Results/SpanishBertTask2Results' + \".pkl\", \"rb\") as f:\n","#   RE = pickle.load(f)\n","# RE"],"execution_count":null,"outputs":[]}]}